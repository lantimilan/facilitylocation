%% lyan doctoral dissertation
%% init @ 03/26/2013
%% TODO: make a better template

%% the preamble below is from the template
%%
%% uctest.tex 11/3/94
%% Copyright (C) 1988-2004 Daniel Gildea, BBF, Ethan Munson.
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License, either version 1.3
% of this license or (at your option) any later version.
% The latest version of this license is in
%   http://www.latex-project.org/lppl.txt
% and version 1.3 or later is part of all distributions of LaTeX
% version 2003/12/01 or later.
%
% This work has the LPPL maintenance status "maintained".
% 
% The Current Maintainer of this work is Daniel Gildea.
%
% 2007/08/01
% LaTeX Package "ucr" is modified from LaTeX package "ucthesis."
% This modification is therefore under to the conditions of 
% the LaTeX Project Public License.
% Its formality is suitable for the dissertation of Universty of
% California, Riverside.
% This test document is for the convenience of all students of
% Universty of California, Riverside.
% Contact Charles Yang at chcyang@yahoo.com if you like.
% Charles Yang has nothing to do with the original author's sarcasm.
%
% \documentclass[11pt]{ucthesis}
% \documentclass[11pt]{ucr}
\documentclass[oneside,final]{ucr}
\usepackage{amssymb}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{bm}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{caption,subcaption,enumerate}
\usepackage{flafter}
\usepackage{empheq}
\usepackage{sw20uctd}

\usepackage{tikz}
\usetikzlibrary{shapes}
%% pseudocode related
\usepackage[nothing]{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{array}

\floatname{algorithm}{Pseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}


\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
%%\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\def\dsp{\def\baselinestretch{2.0}\large\normalsize}
\dsp

%% The user must use \textheight and \topmargin to control to button margin.
\textheight = 8.25in
\topmargin = 0.750in

\input{macros.tex}
% for \cal definition
\makeatletter
\DeclareRobustCommand*\cal{\@fontswitch\relax\mathcal}
\makeatother
\setlength\headsep{-0.5in} 
\begin{document}

% Declarations for Front Matter

\title{Approximation Algorithms for the Fault-Tolerant Facility Placement Problem}
\author{Li Yan}
\degreemonth{June}
\degreeyear{2013}
\degree{Doctor of Philosophy}
\chair{Professor Marek Chrobak}
\othermembers{Professor Tao Jiang\\
Professor Stefano Lonardi\\
Professor Neal Young}
\numberofmembers{4}
\field{Computer Science}
\campus{Riverside}

\maketitle
\copyrightpage{}
\approvalpage{}

\degreesemester{Summer}

\begin{frontmatter}

\begin{acknowledgements}
  I would thank my advisor, Marek Chrobak, for bringing me
  into the PhD program at the University of California
  Riverside, and for his guidance and patience on my study
  and research during the past five years. I am also
  grateful for the committee, Tao Jiang, Stefano Lonardi,
  and Neal Young for helpful discussions and comments on my
  research and this dissertation.

  The supportive environment of the algorithm lab and
  computer science department has made PhD study here a
  pleasant experience and I am grateful for Claire Huang,
  Wei Li and the algorithm lab for helpful discussions and
  stimulation of ideas.

  Labmates Monik and Jonathan, despite their tight schedule,
  read a preliminary version of this dissertation with great
  patience and attention to details. Both offered many
  helpful comments and suggested numerous corrections. Their
  help is greatly appreciated.

  During this final stage of my PhD study, our family had a
  new member, little Terry. Being with a baby as laid-back
  as Terry helped relieve much of the stress. I would also
  like to take this opportunity to show appreaciation to my
  wife, Ying, for taking care of much the family
  responsibility and being considerate on my late hours
  working on the thesis and defense.
\end{acknowledgements}

\begin{dedication}
\null\vfil
{\large
\begin{center}
  To my parents, who always have faith in my endeavors.
\end{center}}
\vfil\null
\end{dedication}

\begin{abstract}
  In this thesis we study the Fault-Tolerant Facility
  Placement problem (FTFP). In the FTFP problem, we are
  given a set of sites at which we can open facilities, and
  a set of clients with demands. To satisfy demands, clients
  must be connected to open facilities. The goal is to
  satisfy all clients' demands while minimizing the combined
  cost of opening facilities and connecting clients to
  facilities. We shown that the problem is NP-hard and hence
  we study approximation algorithms and their performance
  guarantees. Approximation algorithms are algorithms that
  run in polynomial time with provable performance bounds
  relative to optimal solutions.

  We present two techniques with which we develop several
  LP-rounding algorithms with progressively improved
  approximation ratios. The best ratio we have is 1.575. We
  also study primal-dual approaches. In particular, we show
  that a natural greedy algorithm analyzed using the
  dual-fitting technique gives an approximation ratio of
  O(log n). On the negative side, under a natural
  assumption, we give an example showing that the
  dual-fitting analysis cannot give a ratio better than
  O(log n/log log n).
\end{abstract}

\tableofcontents
\listoffigures
\listoftables

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ch1 INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction} \label{ch: intro}

\section{The Problem and the Background}
Facility location problems are a class of problems of both
theoretical interest and practical significance. A
sufficiently general description is that facility location
problems are about selecting a set of sites to open
facilities and servicing clients by connecting them to
facilities. A classical application is to set up warehouses
to distribute commodities to retailers.  Having more
warehouses helps reducing shipping cost because every
retailer can be close to some warehouse. On the other hand,
setting up and maintaining a warehouse can be costly. In
this scenario there is a trade-off between having more
warehouses and cheaper shipping, and having fewer warehouses
at the expense of shipping. Another application is to
place content servers in a computer network. In the network
we have a number of client machines that need to access
files from one of the servers. Having more servers allows
faster access for all clients. However, keeping a large
number of servers around requires substantial hardware and
software investment, as well as committing operation
engineers to keep the servers up and running. A more recent
example concerns today's web giants like Google, Facebook
and Amazon. These web-based companies have geographically
distributed development offices. All these offices require
access of large amount of data from a handful of data
centers, located in a few carefully chosen
locations. However, building a data center is costly. The
electricity bill alone would discourage the plan of having
an excessive number of data centers. On the other hand,
demands from all offices need to be addressed, or engineers
would be idling while waiting for data transfer to
complete. We shall keep using the development office and
data center example to illustrate several aspects of
facility location problems.

First, setting up a data center incurs a cost and the cost
varies with location. The real estate rent in Kansas usually
does not compare to even a fraction of that in San Francisco
or New York. The electricity rate is also very different in
midwest areas from that in California. Other factors like
potential natural disasters such as tornado and earthquake
could also be factored into the cost estimate to build a
data center in a certain location. In short, different
locations have different costs to set up a data center.

Second, different offices may have different demands. A
larger office may require access to several data centers,
either for speedy data transfer, or for redundancy when one
of the data centers goes down.

Third, data centers may have capacities. It is thus
reasonable to put a cap on the number of offices that a
certain data center is able to serve. However, to keep the
problem simple, we may or may not have this constraint in
our problem definition.

Facility location problems have long been an active topic in
both Operations Research and Computer Science research
communities. Problems that have been considered include
\begin{itemize}
\item \emph{the Uncapacitated Facility Location problem}, where
  each site can open at most one facility and each client
  needs to be connected to one facility,
\item \emph{the Fault-Tolerant Facility Location problem},
  where each site can open at most one facility, and each
  client has a demand, which is the number of facilities
  that it needs to connect to,
\item \emph{the Capacitated Facility Location problem},
  where a site can open at most one facility and each client
  connects to one facility, but every facility has a
  capacity, which is the maximum number of clients it can
  accept,
\item \emph{the Prize-Collecting Facility Location problem},
  where a site can open at most one facility and each client
  can either connect to one facility, or stay unconnected,
  in which case a penalty is counted towards final cost.
\end{itemize}
In all the problems above, there is a cost to open a
facility at a site, and there is a cost to connect a client
to a facility as well. The goal is to minimize the total
cost of opening facilities and connecting clients to
facilities. In the Prize-Collecting Facility Location
problem we also pay a penalty for each client that is not
connected.

\paragraph{The UFL Problem.}
The simplest variant, known as the Uncapacitated Facility
Location problem ({\UFL}), is also the one that has been
studied most extensively. In the {\UFL} problem, we are
given a set of sites $\sitesset$ and a set of clients
$\clientset$. Each site $i$ can open one facility and each
client $j$ needs to connect to one facility at a site
$i$. Facilities are uncapacitated, meaning that a facility
can accept connections from any number of clients. We are
also given the facility opening cost $f_i$ for each site
$i$, and the distance $d_{ij}$ between a site $i$ and a
client $j$. The problem asks us to find a set of sites on
which to open facilities, and to connect clients to
facilities, in such a way that the total cost is
minimized. The total cost is the sum of the facility opening
cost and the client connection cost.

The UFL problem is known to be \NP-hard because the problem
contains the classic Set Cover problem as a special
case. The UFL problem with general distances has an
algorithm with an approximation ratio of $O(\log n)$, where
$n$ is the number of clients. This algorithm is due to
Hochbaum~\cite{Hochbaum82}. A matching lower bound of
$\Omega(\log n)$ is immediate, as the UFL problem contains
the well-known Set Cover problem as a special
case~\cite{Hochbaum82}. When the distances form a metric,
that is, they satisfy the triangle inequality, there are
algorithms with a constant approximation ratio. Shmoys,
Tardos and Aardal~\cite{ShmoysTA97} were the first ones to
obtain an $O(1)$-approximation algorithm for the {\UFL}
problem. Currently, the best known approximation ratio is
$1.488$ by Li~\cite{Li11}. On the other hand, Guha and
Khuller~\cite{GuhaK98} showed a lower bound of $1.463$ on
the approximation ratio, under the assumption that $\NP
\nsubseteq \DTIME(n^{O(\log\log
  n)})$. Sviridenko~\cite{Vygen05} weakened the underlying
assumption to $\PP \neq \NP$.

\section{The FTFP Problem}
The problem studied in this thesis is called the
Fault-Tolerant Facility Placement problem (\FTFP), which
generalizes the {\UFL} problem. The differences between the
{\UFL} problem and the {\FTFP} problem are: each client $j$
now has a demand $r_j$ that could be more than one; at each
site $i$ we can open one or more facilities. More formally,
we define the {\FTFP} problem as:
\begin{problem}[The FTFP Problem]
  \label{problem:ftfp}
  In the Fault-Tolerant Facility Placement problem
  ({\FTFP}), we are given a set of sites $\sitesset$ and a
  set of clients $\clientset$. Each client $j \in
  \clientset$ has a demand $r_j$. To open one facility at a
  site $i$ incurs a cost of $f_i$. To make one connection
  from a client $j$ to a facility at a site $i$ incurs a
  cost of the distance $d_{ij}$. The problem asks for a
  solution that every client $j$ is connected to $r_j$
  different facilities and the total cost of opening
  facilities and connecting clients to facilities is
  minimized.
\end{problem}
In this thesis we only consider the metric version of the
{\FTFP} problem. 

The {\FTFP} problem contains the {\UFL} problem as a special
case. Therefore any hardness results for {\UFL} also apply
to {\FTFP} automatically. Regarding metric {\UFL}, the
following results are known.
\begin{theorem}\label{thm:maxsnp} \cite{GuhaK98}
  The metric UFL problem is {\MaxSNP}-hard.
\end{theorem}
The proof is by a reduction from the B-Vertex Cover
problem. This theorem implies that there exists a constant
$c$ such that the metric UFL problem cannot be approximated
with a ratio less than $c$. The best known such constant
$c=1.463$ is given by Guha and Khuller~\cite{GuhaK98}.  On
the other hand, we do not know whether {\FTFP} is harder
than {\UFL} or not.


\section{Organization of the Thesis}
The rest of this thesis is organized as follows: In
Chapter~\ref{ch: related_work} we present approximation
results for FTFP, as well as those for related problems; in
Chapter~\ref{ch: lp} we give the LP formulation for FTFP and
describe structural properties of the optimal fractional
solution that we use for our algorithms; in Chapter~\ref{ch:
  techniques} we describe two main techniques, demand
reduction and adaptive partitioning, that allow us to obtain
a fractional solution with additional structural properties;
in Chapter~\ref{ch: lp-rounding} we show how to round the
obtained fractional solution to an integral solution with
bounded cost; in Chapter~\ref{ch: primal-dual} we present
preliminary results of the primal-dual approach; in
Chapter~\ref{ch: conclusion} we conclude this thesis with a
discussion on open problems.

%% end of ch1

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ch2 RELATED WORK AND RESULTS SUMMARY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work} \label{ch: related_work}

In this chapter we review the history of two problems
closely related to our FTFP problem, the Uncapacitated
Facility Location problem (UFL), and the Fault-Tolerant
Facility Location problem (FTFL).  We finish this chapter
with an overview of known results and our work for the FTFP
problem.

In all three problems, we are given a set of sites
$\sitesset$ and a set of clients $\clientset$, and $f_i$,
the facility opening cost for a site $i$, and $d_{ij}$, the
distance between a site $i$ and a client $j$. Each client
has a demand $r_j$. The differences between the problems
are:
\begin{itemize}
\item{UFL}: All demands are $1$; that is $r_j=1$ for
  all clients $j$. Then we need no more than $1$ facility at
  a site.
\item{FTFL}: Some demands may be more than $1$, but
  each site can open at most $1$ facility.
\item{FTFP}: Some demands may be more than $1$, and each site
  can open any number of facilities.
\end{itemize}
For all three problems above, we assume the metric
version. That is, the distances satisfy the triangle
inequality: for any two sites $i_1$ and $i_2$, and any two
clients $j_1$ and $j_2$, we have
\begin{equation*}
  d_{i_1 j_2} \leq d_{i_1 j_1} + d_{i_2 j_1} + d_{i_2 j_2}.
\end{equation*}

In designing algorithms for all three problems, {\UFL},
{\FTFL}, and {\FTFP}, we have two competing goals: on one
hand we want to open as few facilities as possible so that
our facility cost is small; on the other hand we need as
many facilities as possible so that every client can connect
to nearby facilities. The main challenge, therefore, is to
find a solution with a balanced facility cost and connection
cost. We shall see how this balance is achieved in several
known algorithms for UFL and FTFL. These two are
well-studied problems in literature, and are closely related
to our problem, FTFP. In particular, the LP-rounding
algorithms for UFL inspired our approach for the FTFP
problem. For this reason, we explain the LP-rounding
algorithms for {\UFL} in the coming section with full
detail, aiming at developing an intuition behind the
technical details.

\section{Related Work for UFL}
The Uncapacitated Facility Location problem (UFL) is the
simpliest variant of the Facility Location problems, and has
received the most attention. A surprising fact is that a
wide range of different techniques for designing
approximation algorithms have been found successful in
getting good ratios, as shown in
Table~\ref{tab:ufl_history}. One divertion of terminology:
to be consistent with the terminology in the UFL literature,
instead of saying opening facilities at sites, we simply say
opening or closing facilities without mentioning sites,
since in the {\UFL} problem we have no more than one
facility at each site.

%%%%%%%%%%%%%% begin table UFL history %%%%%%%%%%%%%%%%

\begin{table}
  \centering
  \begin{tabular}{l l l r}
    \toprule
    author & technique & ratio & year\\
    \midrule
    Shmoys, Tardos and Aardal & LP-rounding & 3.16 & 1997~\cite{ShmoysTA97}\\
    Chudak and Shmoys & LP-rounding & 1.736 & 1998~\cite{Chudak98}\\
    Sviridenko & LP-rounding & 1.582 & 2002~\cite{Svi02}\\
    \midrule
    Jain and Vazirani & primal-dual & 3 & 2001~\cite{JainV01}\\
    Jain {\etal} & dual-fitting & 1.61 & 2003~\cite{JainMMSV03}\\
    Arya {\etal} & local-search & 3 & 2001~\cite{AryaGKMMP01}\\
    \midrule
    Byrka and Aardal & hybrid & 1.50 & 2007~\cite{Byrka07}\\
    Li & hybrid & 1.488 (best result) & 2011~\cite{Li11}\\
    \bottomrule
    Guha and Khuller (*) & lower-bound & 1.463 &
    1998~\cite{GuhaK98} \\
    \bottomrule
  \end{tabular}
  \caption[A history of approximation algorithms for the UFL
  problem and their ratios.]
  {A history of approximation algorithms for the UFL problem and their ratios. The word \emph{hybrid} refers to
    using two independent algorithms and returning the better
    solution of the two. The last row with an asterisk gives
    a lower bound on approximation ratio.}
  \label{tab:ufl_history}
\end{table}
%%%%%%%%%% end table UFL history %%%%%%%%%%%%%%%%

\subsection{The LP Formulation}
The analysis of approximation algorithms for \NP-hard
problems requires an estimate on the optimal solution's
cost. For \NP-hard problems, the task to compute the optimal
solution's cost itself is also \NP-hard. One alternative is
to formulate an integer program for the problem and relax
the integral constraints to obtain a linear program
(LP). Solving the LP gives an optimal fractional
solution. The value of the fractional solution is then used
to estimate the optimal integral solution's cost. In
Appendix~\ref{sec: ILP} we give a primer for Integer
Program, Linear Program and their application for the {\UFL}
problem.

For the {\UFL} problem, the LP formulated by
Balinski~\cite{Bal66} is now standard. We start with an
integer program in which we use a variable $y_i\in \{0,1\}$
to indicate whether a facility $i\in\sitesset$ is open or
not, and a variable $x_{ij} \in \{0,1\}$ to indicate whether
a client $j$ is connected to a facility $i$. Relaxing the
integral constraints, we obtain the following
LP~(\ref{eqn:ufl_primal}) for the UFL problem. Observe that
we do not need an explicit constraint of $x_{ij} \leq 1$ or
$y_i \leq 1$, as any optimal solution to the
LP~(\ref{eqn:ufl_primal}) must satisfy these two constaints
automatically. The LP is:
%%%%%%%%%%%
\begin{empheq}[box=\fbox]{align}
  \textrm{minimize} \quad \sum_{i\in \sitesset}f_iy_i &\,+
  \sum_{i\in \sitesset, j\in \clientset} d_{ij}x_{ij}
  &\label{eqn:ufl_primal}
\\ \notag
\textrm{subject to} \quad y_i - x_{ij} &\geq 0 &\quad\quad & \forall i\in \sitesset, j\in \clientset 
\\ \notag
\sum_{i\in \sitesset} x_{ij} &\geq 1 & &\forall j\in \clientset
\\ \notag
x_{ij} \geq 0, y_i &\geq 0 & &\forall i\in \sitesset, j\in \clientset 
\end{empheq}

%%%%%%%%%%%%

\noindent
The dual program is:
\begin{empheq}[box=\fbox]{align}
  \textrm{maximize}\quad \sum_{j\in \clientset} \alpha_j&\label{eqn:ufl_dual}  
  \\ \notag
  \textrm{subject to} \quad 
  \sum_{j\in \clientset}\beta_{ij} &\leq f_i  &\quad\quad &\forall i \in \sitesset  
  \\ \notag
  \alpha_{j} - \beta_{ij} &\leq  d_{ij} &  & \forall i\in \sitesset, j\in \clientset 
  \\ \notag
  \alpha_j \geq 0, \beta_{ij} &\geq 0 &  & \forall i\in \sitesset, j\in \clientset
\end{empheq}


Two general schemes using LP to design approximation
algorithms are the LP-rounding scheme and the primal-dual
scheme. LP-rounding algorithms start with calling an
LP-solver to obtain an optimal fractional solution
$(\bfx^\ast,\bfy^\ast)$, and then round the fractional
solution in such a way that feasibility is preserved while
the solution's cost does not increase by much. However,
primal-dual algorithms do not require solving the LP and the
use of LP in the algorithms and their analysis is
implicit. Those algorithms work by constructing an integral
feasible primal solution and a feasible (fractional) dual
solution simultaneously, and the primal solution's cost is
related to the dual solution's cost. Note that the cost of
any feasible dual solution provides a lower bound on the
optimal value for the primal, assuming the primal is a
minimization program and the dual is a maximization program.

\subsection{Approximation Algorithms}
In this subsection we give an overview of known
approximation algorithms for the {\UFL} problem, including
LP-rounding algorithms, primal-dual algorithms, and
local-search algorithms. All of them, except local-search
algorithms, make use of the LP just decribed.

\subsubsection{LP-rounding Algorithms}
The first $O(1)$-approximation algorithm was obtained by
Shmoys, Tardos and Aardal~\cite{ShmoysTA97}, using
LP-rounding. The Shmoys {\etal}'s algorithm has also
established a general framework that underpins all
subsequent LP-rounding algorithms. In this framework the
clients are partitioning into clusters and each cluster has
a representative client. The rounding algorithm guarantees
that each representative has a nearby facility to connect
to, and the rest of the clients then connect to the
facilities via their representatives. This solution
structure is used in all known LP-rounding algorithms for
the UFL problem.

The Shmoys {\etal}'s algorithm achieved a ratio of
$3.16$. Since their algorithm has made several greedy
choices, the algorithm has left quite some room for
improvements. Chudak and Shmoys~\cite{ChudakS04} were the
first to use the idea of randomized rounding for
{\UFL}. Roughly speaking, in their algorithm, each facility
$i$ is opened with probabiliy $y_i^\ast$ where $y_i^\ast$ is
given by the optimal fractional solution
$(\bfx^\ast,\bfy^\ast)$ to the LP
(\ref{eqn:ufl_primal}). The expected connection cost is
estimated using a provably worse random process in which
each facility is opened indepedently, whose expected
connection cost is easier to analyze. They obtained a ratio
of $1+2/e = 1.736$. Sviridenko~\cite{Svi02} further improved
the ratio to $1.582$, by using a scaled version of the
optimal fractional solution $(\bfx^\ast,\bfy^\ast)$, and a
judicious choice of some distribution of the scaling
parameter. The rounding process is called pipage rounding, a
deterministic rounding process that takes advantage of the
concave property of some cost function. The analysis is
highly technical.

\subsubsection{Primal-dual Algorithms}
Primal-dual algorithms do not require solving the LP and
thus have lower time complexity. Two primal-dual algorithms
have achieved impressive approximation ratios: the first
algorithm is given by Jain and Vazirani~\cite{JainV01} and
their algorithm achieved a ratio of $3$. The approximation
ratio is obtained via a relaxed version of the complementary
slackness conditions. These conditions provide a bound on
the cost of the primal solution in terms of the cost of the
dual solution, which is a lower bound on the optimal primal
solution's cost. More on the use of complementary slackness
conditions for {\UFL} can be found in Appendix~\ref{subsec:
  LP_duality_CSC}.

A slightly different primal-dual based algorithm was
proposed by Jain, Markakis, Mahdian, Saberi and
Vazirani~\cite{JainMMSV03}. They analyzed a greedy algorithm
that repeatedly picks the most cost-effective star until all
clients are connected. A star consists of one facility and a
subset of clients. The cost-effectiveness is the cost of the
star divided by the number of clients in that star. The cost
of the star is the sum of the facility opening cost and
connection cost of the clients to that facility. In each
iteration, the algorithm picks the best star, connects all
member clients to the facility, and sets the facility cost
to zero. The clients that were in the star and just got
connected are removed from future consideration, but the
facility could be reused for future stars. Jain {\etal}
analyzed the greedy algorithm and its variant using the
dual-fitting technique. They first showed that the greedy
algorithm can be interpreted as a process of growing a dual
solution and updating a primal solution. Moreover, the cost
of the primal solution is equal to the cost of the dual
solution. It might appear that we have solved the {\UFL}
problem optimally, although we know that this cannot be the
case, as the {\UFL} problem is {\NP}-hard. The catch is that
the dual solution computed by the greedy algorithm is not
feasible. The next step is to find a common factor $\gamma$,
such that the dual solution, after shrinking by $\gamma$
(dividing by $\gamma$), becomes feasible. That common factor
$\gamma$ is then the desired approximation ratio. They
showed that their two algorithms have approximation ratio
$1.861$~\footnote{An improved analysis by Mahdian showed the
  actual ratio is $1.81$.}  and $1.61$ respectively.

\subsubsection{Other Algorithms}
A still different approach is local-search, in which we
start with some feasible integral solution and make local
moves to improve that solution, and stop when a local
optimum is achieved. The set of allowed local moves needs to
be chosen carefully: admitting more powerful moves allows
the local optimum to be closer to the global optimum, while
restricting to a few simple moves results in faster
algorithms and easier analysis. Arya
{\etal}~\cite{AryaGKMMP01} showed that their local-search
algorithm gave a ratio of $3$.

\paragraph{Best Result.} To date, the best-known
approximation algorithms for {\UFL} are due to
Byrka~\cite{Byrka07} with a ratio of $1.5$, and a follow-up
work by Li~\cite{Li11} with a ratio of $1.488$. Both
algorithms use a combination of two algorithms: one is an
LP-rounding algorithm and the other is the $1.61$ greedy
algorithm of Jain, Mahdian and Saberi~\cite{JainMS02}. To
explain the hybrid approach requires the notion of
\emph{bifactor} analysis, for this reason we postpone the
discussion of Byrka's and Li's work for now and introduce
bifactor analysis first.

\subsection{Bifactor Analysis} \label{subsec: bifactor}
Given that the cost of a UFL solution consists of two parts:
the facility cost and the connection cost, a notion of
bifactor approximation is appropriate. This notion was first
introduced by Jain {\etal} in~\cite{JainMMSV03}. An
algorithm with a facility cost of $F_{\smallALG}$ (sum of
$f_i$ for all open facility $i$) and a connection cost of
$C_{\smallALG}$ (sum of $d_{ij}$ for pairs of $(i,j)$
connected), is said to be a $(\gamma_f,\gamma_c)$-
approximation if, for every feasible solution {\SOL} with a
facility cost of $F_{\smallSOL}$ and a connection cost of
$C_{\smallSOL}$, we have
\begin{equation*}
  F_{\smallALG} + C_{\smallALG} \leq \gamma_f F_{\smallSOL} +
  \gamma_c C_{\smallSOL}.
\end{equation*}
In particular, the above holds if we substitute in an
optimal fractional solution $(\bfx^\ast,\bfy^\ast)$ for
{\SOL}. The solution $(\bfx^\ast,\bfy^\ast)$ has facility
cost $F^\ast = \sum_{i\in\sitesset} f_i y_i^\ast$ and
connection cost $C^\ast = \sum_{j\in\clientset} d_{ij}
x_{ij}^\ast$. Therefore, for a
$(\gamma_f,\gamma_c)$-approximation algorithm, we have
\begin{equation*}
  F_{\smallALG} + C_{\smallALG} \leq \gamma_f F^\ast +
  \gamma_c C^\ast.
\end{equation*}

The notion of bifactor approximation is helpful when an
algorithm has imbalanced factors $\gamma_f$ and
$\gamma_c$. It is easy to see that such an algorithm has an
approximation ratio of $\max\{\gamma_f,
\gamma_c\}$. However, more can be said, as there are
techniques like cost scaling and greedy augmentation to
balance these two factors, thus achieving a better overall
approximation ratio. The techniques of cost scaling and
greedy augmentation and their use for balancing the two
factors were introduced by Guha, Khuller and
Charikar~\cite{GuhaK98, CharikarG05}. For example, the
primal-dual algorithm by Jain and Vazirani~\cite{JainV03} is
a (1,3)-approximation algorithm; using cost scaling and
greedy augmentation, it is possible to show that the
algorithm can achieve a ratio of $1.85$~\cite{CharikarG05}.

Finally we return to the hybrid algorithms by Byrka and
Aardal~\cite{ByrkaA10}, and Li~\cite{Li11}. Byrka and Aardal
gave an LP-rounding algorithm with bifactor $(1.68,1.37)$,
and showed that this algorithm, when combined with the
$(1.11,1.78)$ algorithm by Jain {\etal}~\cite{JainMMSV03},
gave a ratio of $1.50$. Li showed that by choosing a
nontrivial distribution of the scaling factor of Byrka's
algorithm, the analysis can be refined to show an overall
ratio of $1.488$. The $1.488$ ratio is currently the best
known approximation result.

\subsection{LP-rounding for UFL}
We now present a more detailed description of the
LP-rounding algorithms, as our algorithms for {\FTFP} are
built on the LP-rounding algorithms for {\UFL}.

\subsubsection{The Motivation of Rounding}
Every LP-rounding algorithm for {\UFL} starts with solving
the LP (\ref{eqn:ufl_primal}) and obtaining an optimal
fraction solution $(\bfx^\ast,\bfy^\ast)$. Then we need to
round the fractional solution to an integral solution
$(\hat\bfx,\hat\bfy)$.

An integral solution with a small cost would have each
client connected to a nearby facility and few facilities
open. Consider a client $j$; to get a handle on the
connection cost, we would like the client $j$ to connect to
some neighboring facility $i\in N(j)$, where the
neighborhood $N(j) \defeq \{i\in\sitesset \suchthat
x_{ij}^\ast > 0\}$. For the sake of the connection cost, it
is desirable that every client has a neighboring facility
open, as those are facilities not too far away. However, it
is in general not possible, or we would have to open too
many facilities, and thus incur a high facility cost. An
alternative is to select a subset of clients, denoted by $C'
\subseteq \clientset$ and only require clients in $C'$ have
a neighboring facility open. Clients outside $C'$ are then
connected to a facility via some client in $C'$. The
connection cost for clients in $\clientset \setminus C'$ are
bounded using the triangle inequality. For this strategy to
work, every clients $j$ outside $C'$ needs to be able to
find some client $j'$ in $C'$ such that both $d_{jj'} \defeq
\min_{i\in \sitesset} d_{ij} + d_{ij'}$ and $d_{\phi(j')
  j'}$ are small. Here $\phi(j')$ is the facility that $j'$
connects to.

\subsubsection{The Clustering Structure}
The clustering structure produced by the Shmoys, Tardos and
Aardal's algorithm is depicted in Figure~\ref{fig:STA97}. It
has three interesting properties:
\begin{itemize}
\item First, each cluster of clients has a
  representative~\footnote{In the literature, the
    representative is called \emph{center}.}.
\item Second, the neighborhoods of the representatives are
  disjoint.
\item Third, each client shares a neighbor with its
  representative.
\end{itemize}

\subsubsection{A Simple $4$-approximation}
To see how this clustering structure helps rounding, we use
a simple $4$-approximation algorithm by
Chudak~\cite{Chudak98} as an example. In this algorithm
clusters are formed by repeatedly picking a non-clustered
client with minimum $\alpha_j^\ast$ value as a new
representative, where $(\bfalpha^\ast,\bfbeta^\ast)$ is the
optimal dual solution. Clients share a common neighboring
facility with the new representative then become members of
that cluster. Once all clients are clustered, the algorithm
opens the cheapest facility in each representative's
neighborhood. All clients in the same cluster connect to the
only facility open in their representative's neighborhood.

The facility cost of this solution is no more than $F^\ast =
\sum_{i\in\sitesset} f_i y_i^\ast$, because every facility
that is opened can have its cost bounded by the average
facility cost of the neighborhood. The connection cost of a
representative $j'$ is no more than $\alpha_{j'}^\ast$,
because the complentary slackness conditions imply $d_{ij'}
\leq \alpha_{j'}^\ast$ for any facility $i$ in $N(j')$. The
connection cost for a non-representative client $j$ can be
bounded by the triangle inequality; that is $d_{i'j} \leq
d_{i'j'} + d_{ij'} + d_{ij}$ where $j'$ is the
representative of $j$, and $i'$ is the facility opened in
$j'$'s neighborhood, and $i$ is a common neighbor of both
$j$ and $j'$. Using the complementary slackness conditions,
the distance of $d_{i'j}$ is no more than $\alpha_{j'}^\ast
+ \alpha_{j'}^\ast + \alpha_j^\ast$, which is no more than
$3\alpha_j^\ast$ because the representative $j'$ has the
minimum $\alpha_{j'}^\ast$ value among all clients in its
cluster. Summarizing, the solution has a facility cost at
most $F^\ast$ and a connection cost no more than
$3\sum_{j\in\clientset} \alpha_j^\ast = 3\,\LP^\ast$, where
$\LP^\ast = F^\ast + C^\ast$. Since $\LP^\ast \leq \OPT$
where $\OPT$ is the optimal integral solution's cost, the
algorithm is a $4$-approximation.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% UFL cluster figure %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
  \centering
  \begin{tikzpicture}[scale=0.6, every node/.style={scale=0.6}]
    %% neighborhood
    \node[draw,ellipse, minimum width=2cm, minimum height=4cm]
    at (0,12) {};
    \node[draw,ellipse, minimum width=2cm, minimum height=4cm]
    at (0,6) {};
    \node[draw,ellipse, minimum width=2cm, minimum height=4cm]
    at (0,0) {};

    %% facility
    \node[draw,minimum size=.6cm] (fac11) at (0,13) {};
    \node[draw,minimum size=.6cm] (fac12) at (0,12) {};
    \node[draw,minimum size=.6cm] (fac13) at (0,11) {};
                                   
    \node[draw,minimum size=.6cm] (fac21) at (0,8.5) {};
                                   
    \node[draw,minimum size=.6cm] (fac31) at (0,6.4) {};
    \node[draw,minimum size=.6cm] (fac32) at (0,5) {};
                                   
    \node[draw,minimum size=.6cm] (fac41) at (0,1) {};
    \node[draw,minimum size=.6cm] (fac42) at (0,-.6) {};

    %% client
    \node[circle,draw,minimum size=.6cm] (client11) at (8,11) {};
    \node[circle,fill,draw,minimum size=.6cm] (client12) at (8,10){};
    \node[circle,draw,minimum size=.6cm] (client13) at (8,9) {};

    \node[circle,fill,draw,minimum size=.6cm] (client21) at
    (8,6) {};
    \node[circle,draw,minimum size=.6cm] (client22) at (8,5) {};

    \node[circle,draw,minimum size=.6cm] (client31) at (8,2) {};
    \node[circle,draw,minimum size=.6cm] (client32) at (8,1) {};
    \node[circle,fill,draw,minimum size=.6cm] (client33) at
    (8,0) {};
    \node[circle,draw,minimum size=.6cm] (client34) at
    (8,-1) {};

    %% edges
    \foreach \fac in {fac11,fac12,fac13}
    \draw (\fac) -- (client12);

    \foreach \fac in {fac31,fac32}
    \draw (\fac) -- (client21);

    \foreach \fac in {fac41,fac42}
    \draw (\fac) -- (client33);

    \draw (fac11) -- (client11);
    \draw (fac13) -- (client13);
    \draw (fac21) -- (client13);

    \draw(fac32) -- (client22);
    
    \draw(fac41) -- (client31);
    \draw(fac41) -- (client32);
    \draw(fac42) -- (client34);
    
    \draw (fac41) -- (client22);

    \draw[dashed] (7,8) rectangle (9,12);
    \draw[dashed] (7,4) rectangle (9,7);
    \draw[dashed] (7,-2) rectangle (9,3);
  \end{tikzpicture}
  \caption[An illustration of the clustering structure of
  LP-rounding algorithms for UFL.]{An illustration of the
    clustering structure of LP-rounding algorithms for
    UFL. Rectangles are facilities and circles are
    clients. Dashed boxes indicate clusters. The solid
    circle in each box denotes the representative of that
    cluster. An edge is drawn from a client to a neighboring
    facility. An ellipse indicates the neighborhood of a
    representative.}
  \label{fig:STA97}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% end  UFL cluster figure %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Related Work for FTFL}
The Fault-Tolerant Facility Location problem (\FTFL) was
first introduced by Jain and Vazirani~\cite{JainV03}.  The
LP is
\begin{empheq}[box=\fbox]{align}
  \textrm{minimize} \quad \sum_{i\in \sitesset}f_iy_i &\,+
  \sum_{i\in \sitesset, j\in \clientset} d_{ij}x_{ij}
  &\label{eqn:ftfl_primal}
\\ \notag
\textrm{subject to} \quad y_i - x_{ij} &\geq 0 &\quad\quad & \forall i\in \sitesset, j\in \clientset 
\\ \notag
\sum_{i\in \sitesset} x_{ij} &\geq r_j & &\forall j\in \clientset
\\ \notag
y_i &\leq 1 & &\forall i \in \sitesset
\\ \notag
x_{ij} \geq 0, y_i &\geq 0 & &\forall i\in \sitesset, j\in
\clientset 
\end{empheq}
The constraint $\sum_{i\in\sitesset} x_{ij} \geq r_j$ is
there because, unlike in {\UFL}, a client $j$ now needs to
be connected to $r_j$ different facilities. The constraint
$y_i \leq 1$ is necessary as in the {\FTFL} problem a site
can open at most $1$ facility.

Jain and Vazirani adapted their primal-dual algorithm for
UFL to {\FTFL} and obtained a ratio of $3\ln R$ where
$R=\max_j r_j$ is the maximum demand among all clients. The
first $O(1)$-approximation algorithm was given by Guha,
Meyerson and Munagala~\cite{GuhaMM03}, using an LP-rounding
algorithm similar to the Shmoys, Tardos and
Aardal's~\cite{ShmoysTA97} algorithm for the {\UFL} problem.
Swamy and Shmoys~\cite{SwamyS08} gave an improved algorithm
with a ratio of $2.076$. Their algorithm uses the pipage
rounding technique. The current best known approximation
ratio is $1.7245$, due to Byrka, Srinivasan and
Swamy~\cite{ByrkaSS10}. Their algorithm uses dependent
rounding and a laminar clustering structure.

We note that all of the known $O(1)$-approximation
algorithms for {\FTFL} are LP-rounding algorithms and they
need to solve the LP as the first step, which can be
computationally expensive. Given the success of primal-dual
based approaches for {\UFL}, it is natural to ask whether
such algorithms could be adapted to {\FTFL} with a good
ratio. To the best of the author's knowledge, there has been
no success in obtaining a primal-dual algorithm for {\FTFL}
with a sub-logarithmic ratio. This is in stark contrast to
the fact that two different primal-dual
algorithms~\cite{JainV03,JainMMSV03} have achieved constant
ratio for {\UFL}.

\section{Related Work for FTFP}
Our problem, the Fault-Tolerant Facility Placement problem
(\FTFP), was introduced by Xu and
Shen~\cite{XuS09}~\footnote{In their paper they call the
  problem the Fault-Tolerant Facility Allocation problem, or
  FTFA.}. The study of {\FTFP} was partly motivated to
obtain a better understanding of the implication of the
fault-tolerant requirement on facility location
problems. The Xu and Shen's results, and a follow-up work by
Liao and Shen~\cite{LiaoS11}, seem to be valid only for a
special case of {\FTFP}.

Our work on the FTFP problem begins with a $4$-approximation
LP-rounding algorithm~\cite{YanC11}. In this thesis we
present significantly improved results for {\FTFP}. For
LP-rounding algorithms, we achieved a ratio of $1.575$,
which matches the best known LP-based ratio for
{\UFL}~\cite{ByrkaGS10}. The LP-rounding results are
explained in Chapter~\ref{ch: lp-rounding}. Our preliminary
results using primal-dual approaches are given in
Chapter~\ref{ch: primal-dual}, which includes an explanation
of the difficulty in obtaining $O(1)$-approximation using
such approaches.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ch3 LINEAR PROGRAM
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% what needs to be there?
%% - the LP
%% - special case: all r_j equal
%% - completeness
%%
%% figure?
%% table?
\chapter{Linear Program} \label{ch: lp} 

In this chapter we give the linear program (LP) for the
{\FTFP} problem. The structure of the linear program implies
a simple reduction for a special case of {\FTFP}. We
describe this reduction in
Section~\ref{sec:ftfp_uniform_demand}. For the optimal
fractional solution we use for our approximation algorithms,
we assume a structural property called
\emph{completeness}. The exact definition and a procedure to
obtain such a solution is given in
Section~\ref{sec:ftfp_complete}. The discussion in this
chapter prepares the reader for Chapter~\ref{ch:
  techniques}, where we introduce our main techniques:
demand reduction and adaptive partitioning.

\section{Notation and Definition}
For the reader's convenience we repeat the problem
definition and the notation before introducing the LP. In
the Fault-Tolerant Facility Placement problem ({\FTFP}), we
denote the set of sites as $\sitesset$ and the set of
clients as $\clientset$. Each client $j \in \clientset$ has
a demand $r_j$, meaning that the client $j$ needs to be
connected to $r_j$ different facilities. The distance
between a site $i$ and a client $j$ is denoted as
$d_{ij}$. To open one facility at a site $i$ incurs a cost
of $f_i$. To make one connection from a client $j$ to a
facility at a site $i$ incurs a cost of the distance
$d_{ij}$. Thus an {\FTFP} instance is fully specified by
$\sitesset, \clientset$, $r_j$ for every $j\in\clientset$,
and $d_{ij}$ for every $i\in\sitesset,j\in\clientset$. We
consider the metric version, that is the distances satisfy
the triangle inequality: for any two sites $i_1,i_2$ and any
two clients $j_1,j_2$, we have
\begin{equation*}
  d_{i_1 j_2} \leq d_{i_1 j_1} + d_{i_2 j_1} + d_{i_2 j_2}.
\end{equation*} 

A solution to the {\FTFP} problem is a vector $(\bfx, \bfy)$
where $x_{ij} \in \{0, 1, 2, \ldots\}$ denotes the number of
connections between a site $i$ and a client $j$, and $y_i
\in \{0, 1, 2\ldots\}$ denotes the number of facilities
opened at site $i$. We seek a solution such that $y_i \geq
x_{ij}$ for every $i \in \sitesset, j \in \clientset$ and
$\sum_{i\in\sitesset} x_{ij} = r_j$ for all clients $j \in
\clientset$. Our goal is to find such a solution with
minimum total cost, and the cost is $\sum_{i \in \sitesset}
f_i y_i + \sum_{i \in \sitesset, j \in \clientset} d_{ij}
x_{ij}$. We call the first term $\sum_{i \in \sitesset} f_i
y_i$ \emph{the facility cost} of the solution, and the
second term $\sum_{i \in \sitesset, j \in \clientset} d_{ij}
x_{ij}$ \emph{the connection cost} of the solution.

\section{The Linear Program of FTFP}
The FTFP problem has a natural Integer Programming (IP)
formulation. Let $y_i$ represent the number of facilities
opened at a site $i$, and let $x_{ij}$ represent the number
of connections from a client $j$ to facilities at a site
$i$. If we relax the integrality constraints, we obtain the
following LP:

%%%%%%%%%%%
\begin{empheq}[box=\fbox]{align}
  \textrm{minimize} \quad \cost(\bfx,\bfy) &= \textstyle{\sum_{i\in \sitesset}f_iy_i 
								+ \sum_{i\in \sitesset, j\in \clientset}d_{ij}x_{ij}}\label{eqn:fac_primal}\hspace{-1.5in}&&
									\\ \notag
  \textrm{subject to}\quad y_i - x_{ij} &\geq 0 			&\quad 		&\forall i\in \sitesset, j\in \clientset 
									\\ \notag
     \textstyle{\sum_{i\in \sitesset} x_{ij}} &\geq r_j  &			&\forall j\in \clientset
 									\\ \notag
  	  x_{ij} \geq 0, y_i &\geq 0 						& 			&\forall i\in \sitesset, j\in \clientset 
\end{empheq}

%%%%%%%%%%%%

\noindent
The dual program is,
\begin{empheq}[box=\fbox]{align}
  \textrm{maximize}\quad \textstyle{\sum_{j\in \clientset}} r_j\alpha_j&\label{eqn:fac_dual}  
     						\\ \notag
  \textrm{subject to} \quad \textstyle{
    \sum_{j\in \clientset}\beta_{ij}} &\leq f_i  &\quad\quad			&\forall i \in \sitesset  
							\\ \notag
  \alpha_{j} - \beta_{ij} 	&\leq  d_{ij}       &                 & \forall i\in \sitesset, j\in \clientset 
							\\ \notag
  \alpha_j \geq 0, \beta_{ij} &\geq 0           &            & \forall i\in \sitesset, j\in \clientset
\end{empheq}

In each of our algorithms we will fix some optimal solutions
of the LPs (\ref{eqn:fac_primal}) and (\ref{eqn:fac_dual})
that we will denote by $(\bfx^\ast, \bfy^\ast)$ and
$(\bfalpha^\ast,\bfbeta^\ast)$, respectively.

With $(\bfx^\ast, \bfy^\ast)$ fixed, we can define the
optimal facility cost as $F^\ast=\sum_{i\in\sitesset} f_i
y_i^\ast$ and the optimal connection cost as $C^\ast =
\sum_{i\in\sitesset,j\in\clientset} d_{ij}x_{ij}^\ast$.
Then $\LP^\ast = \cost(\bfx^\ast,\bfy^\ast) = F^\ast+C^\ast$
is the joint optimal value of (\ref{eqn:fac_primal}) and
(\ref{eqn:fac_dual}).  We can also associate with each
client $j$ its fractional connection cost $C^\ast_j =
\sum_{i\in\sitesset} d_{ij}x_{ij}^\ast$.  Clearly, $C^\ast =
\sum_{j\in\clientset} C^\ast_j$.  Throughout this thesis we
will use notation $\OPT$ for the optimal integral solution
of (\ref{eqn:fac_primal}).  $\OPT$ is the value we wish to
approximate, but, since $\OPT\ge\LP^\ast$, we can instead
use $\LP^\ast$ to estimate the approximation ratio of our
algorithms.

%%%%%%%%%

\section{LP Structure and Special Case: Uniform-demand}
\label{sec:ftfp_uniform_demand}

If we compare the the LP~(\ref{eqn:fac_primal}) and its
dual~(\ref{eqn:fac_dual}) for {\FTFP} to the
LP~(\ref{eqn:ufl_primal}) and its dual~(\ref{eqn:ufl_dual})
for {\UFL}, these two LPs are very similar. In fact, the
dual constraints of the two problems are identical. This
makes one wondering whether there is a simple reduction from
{\FTFP} to {\UFL}, so that we can take advantage of almost
all known approximation algorithms for {\UFL} and use them
to solve {\FTFP}. Unfortunately we are not aware of such a
reduction for general demands. For the special case when all
demands are equal, we observe that any fractional solution
$(\bfx,\bfy)$ to the LP~(\ref{eqn:fac_primal}), when scaled
down by $R = r_j$ (since all $r_j$'s are equal), is a
feasible solution to an {\UFL} instance with the same set of
sites and clients, and the same facility opening costs and
the same distances. If we solve that {\UFL} instance with an
approximation algorithm, and duplicate the solution $R$
times, we obtain an integral solution to the {\FTFP}
instance. It is not hard to see that the approximation ratio
is preserved. This shows a simple reduction from {\FTFP} to
{\UFL} for the uniform-demand case.

\section{Solution Structure: Completeness and Facility
  Splitting}
\label{sec:ftfp_complete}
In this section we describe a structural property we assume
of the optimal fractional solution $(\bfx^\ast, \bfy^\ast)$
that we use for designing and analyzing approximation
algorithms for {\FTFP}. This property is called
\emph{completeness}. Moreover, we give a procedure to obtain
such a complete solution.

Define $(\bfx^\ast, \bfy^\ast)$ to be \emph{complete} if
$x_{ij}^\ast>0$ implies that $x_{ij}^\ast=y_i^\ast$ for all
$i,j$. In other words, each connection either uses a site
fully or not at all.  As shown by Chudak and
Shmoys~\cite{ChudakS04}, we can modify the given instance by
adding at most $|\clientset|$ sites to obtain an equivalent
instance that has a complete optimal solution, where
``equivalent" means that the values of $F^\ast$, $C^\ast$
and $\LP^\ast$, as well as $\OPT$, are not
affected. Roughly, the argument is this: we notice that,
without loss of generality, for each client $k$ there exists
at most one site $i$ such that $0 < x_{ik}^\ast < y_i^\ast$.
We can then perform the following \emph{facility splitting}
operation on the site $i$: introduce a new site $i'$, let
$y^\ast_{i'} = y^\ast_i - x^\ast_{ik}$, redefine $y^\ast_i$
to be $x^\ast_{ik}$, and then for each client $j$
redistribute $x^\ast_{ij}$ so that $i$ retains as much
connection value as possible and $i'$ receives the
rest. Specifically, we set
%
\begin{empheq}[box=\fbox]{align*}
  &y^\ast_{i'} \;\assign\; y^\ast_i - x^\ast_{ik},\;   y^\ast_{i} \;\assign\; x^\ast_{ik}, \quad \text{ and }\\
  &x^\ast_{i'j} \;\assign\;\max( x^\ast_{ij} - x^\ast_{ik}, 0 ),\;	 x^\ast_{ij} \;\assign\; \min( x^\ast_{ij} , x^\ast_{ik}) 
			\quad	\textrm{for all}\ j \neq k.
\end{empheq}
%
This operation eliminates the partial connection between $k$
and $i$ and does not create any new partial
connections. Each client can split at most one site and
hence we shall have at most $|\clientset|$ more sites.

By the above paragraph, without loss of generality we can
assume that the optimal fractional solution $(\bfx^\ast,
\bfy^\ast)$ is complete. This assumption will in fact
greatly simplify some of the arguments in the
paper. Additionally, we will frequently use the facility
splitting operation described above in our algorithms to
obtain fractional solutions with desirable properties.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ch4 TECHNIQUES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Techniques} \label{ch: techniques} 

In this chapter we introduce two techniques: \emph{demand
  reduction} and \emph{adaptive partitioning}. These two
techniques together produce a structured fractional solution
to the LP~(\ref{eqn:fac_primal}). This structured solution
possesses a number of nice properties, which we then take
advantage in our LP-rounding algorithms (Chapter~\ref{ch:
  lp-rounding}) to obtain integral solutions with good
approximation ratios.

Our first technique, demand reduction, allows us to confine
our attention to a restricted version of the {\FTFP}
problem, in which all demands $r_j$'s are not too
large. This restriction then sets stage for the application
of our next technique, adaptive partitioning, with which we
obtain an FTFP instance with facilities created at sites
(not opened yet) and unit demand points derived from
clients. Now our job is to decide which facilities to open
and how to connect unit demands to open facilities. We would
like to point out that we still need to observe the
fault-tolerant requirement, that is, unit demands originated
from the same client must connect to different
facilities. We shall see that our adaptive partitioning step
deals with the fault-tolerant requirement smoothly.

\section{Demand Reduction}
\label{sec: polynomial demands}

This section presents the demand reduction technique that
reduces the {\FTFP} problem for arbitrary demands to a
special case where demands are bounded by $|\sitesset|$, the
number of sites.  (The formal statement is a little more
technical -- see Theorem~\ref{thm: reduction to
  polynomial}.)  The demand reduction step prepares us for
our second technique, adaptive
partitioning~(Section~\ref{sec: adaptive partitioning}), as
well as our LP-rounding algorithms (Chapter~\ref{ch:
  lp-rounding}), since those steps process individual
demands of each client one by one, and thus they critically
rely on the demands being bounded polynomially in terms of
$|\sitesset|$ and $|\clientset|$ to keep the overall running
time polynomial.

The reduction is based on an optimal fractional solution
$(\bfx^\ast,\bfy^\ast)$ to the
LP~(\ref{eqn:fac_primal}). From the optimality of this
solution, we can also assume that $\sum_{i\in\sitesset}
x^\ast_{ij} = r_j$ for all $j\in\clientset$.  As explained
in Chapter~\ref{ch: lp}, we can assume that
$(\bfx^\ast,\bfy^\ast)$ is complete, that is $x^\ast_{ij} >
0$ implies $x^\ast_{ij} = y^\ast_i$ for all $i,j$.  We split
this solution into two parts, namely $(\bfx^\ast,\bfy^\ast)
= (\hatbfx,\hatbfy)+ (\dotbfx,\dotbfy)$, where
%
\begin{empheq}[box=\fbox]{align*}
\haty_i &\;\assign\; \floor{y_i^\ast}, \quad
			\hatx_{ij} \;\assign\; \floor{x_{ij}^\ast} \quad\textrm{and}
			\\
\doty_i &\;\assign\; y_i^\ast - \floor{y_i^\ast}, \quad
 	\dotx_{ij} \;\assign\; x_{ij}^\ast -  \floor{x_{ij}^\ast}
\end{empheq}
%
for all $i,j$. Now we construct two
FTFP instances $\hatcalI$ and $\dotcalI$ with the same
parameters as the original instance, except that the demand of each client $j$ is
$\hatr_j = \sum_{i\in\sitesset} \hatx_{ij}$ in instance $\hatcalI$ and
$\dotr_j = \sum_{i\in\sitesset} \dotx_{ij} = r_j - \hatr_j$ in instance $\dotcalI$. 
It is obvious that if we have integral solutions to both $\hatcalI$
and $\dotcalI$ then, when added together, they form an integral
solution to the original instance.  Moreover, we have the
following lemma.

%%%%%%%%%%

\begin{lemma}\label{lem: polynomial demands partition}
{\rm (i)}
  $(\hatbfx, \hatbfy)$ is a feasible integral solution to
  instance $\hatcalI$.

\noindent
{\rm (ii)}
  $(\dotbfx, \dotbfy)$ is a feasible fractional
  solution to instance $\dotcalI$.

\noindent
{\rm (iii)}
$\dotr_j\leq |\sitesset|$ for every client $j$.

\end{lemma}
%%%%%%%%%%%%%%%%%%%%
\begin{proof}
  (i) For feasibility, we need to verify that the
  constraints of the LP~(\ref{eqn:fac_primal}) are
  satisfied. Directly from the definition, we have $\hatr_j
  = \sum_{i\in\sitesset} \hatx_{ij}$.  For any $i$ and $j$,
  by the feasibility of $(\bfx^\ast,\bfy^\ast)$ we have
  $\hatx_{ij} = \floor{x_{ij}^\ast} \le \floor{y^\ast_i} =
  \haty_i$.

  (ii) From the definition, we have $\dotr_j =
  \sum_{i\in\sitesset} \dotx_{ij}$.  It remains to show that
  $\doty_i \geq \dotx_{ij}$ for all $i,j$.  If
  $x_{ij}^\ast=0$, then $\dotx_{ij}=0$ and we are done.
  Otherwise, by completeness, we have
  $x_{ij}^\ast=y_i^\ast$.  Then $\doty_i = y_i^\ast -
  \floor{y_i^\ast} = x_{ij}^\ast - \floor{x_{ij}^\ast}
  =\dotx_{ij}$.

  (iii) From the definition of $\dotx_{ij}$ we have
  $\dotx_{ij} < 1$.  Then the bound follows from the
  definition of $\dotr_j$.
\end{proof}

Notice that our construction relies on the completeness
assumption; in fact, it is easy to give an example where
$(\dotbfx, \dotbfy)$ would not be feasible if we used a
non-complete optimal solution $(\bfx^\ast,\bfy^\ast)$.  Note
also that the solutions $(\hatbfx,\hatbfy)$ and $(\dotbfx,
\dotbfy)$ are in fact optimal for their corresponding
instances, for if a better solution to $\hatcalI$ or
$\dotcalI$ existed, it could give us a solution to $\calI$
with a smaller objective value. We would also like to
comment that completeness, although simplifies our argument
here and afterwards, is not essential. For our demand
reduction, we can deal with non-complete fractional
solutions by taking $\hat y_i = (y_i^\ast -
1)_+$~\footnote{The notation $(\cdot)_+$ means taking
  maximum of the term or 0.} and $\hat x_{ij} =
\min\{\floor{x_{ij}^\ast}, \hat y_i\}$, and $\dot y_i =
y_i^\ast - \hat y_i$, $\dot x_{ij} = x_{ij}^\ast - \hat
x_{ij}$. For this set of fractional values, item (i) and
(ii) of Lemma~\ref{lem: polynomial demands partition} remain
valid, and (iii) now reads: $\dot r_j < 2|\sitesset|$ for
every client $j$.

%%%%%%%%%%%%%%%

\begin{theorem}\label{thm: reduction to polynomial}
  Suppose that there is a polynomial-time algorithm $\calA$
  that, for any instance of {\FTFP} with maximum demand
  bounded by $|\sitesset|$, computes an integral solution
  that approximates the fractional optimum of this instance
  within factor $\rho\geq 1$.  Then there is a
  $\rho$-approximation algorithm $\calA'$ for {\FTFP}.
\end{theorem}
%%%%%%%%%%%%%%%
\begin{proof}
  Given an {\FTFP} instance with arbitrary demands,
  Algorithm~$\calA'$ works as follows: it solves the
  LP~(\ref{eqn:fac_primal}) to obtain a fractional optimal
  solution $(\bfx^\ast,\bfy^\ast)$, which we may assume to
  be complete, then it constructs instances $\hatcalI$ and
  $\dotcalI$ described above, applies algorithm~$\calA$ to
  $\dotcalI$, and finally combines (by adding the values)
  the integral solution $(\hatbfx, \hatbfy)$ of $\hatcalI$
  and the integral solution of $\dotcalI$ produced by
  $\calA$. This clearly produces a feasible integral
  solution for the original instance $\calI$.  The solution
  produced by $\calA$ has a cost at most
  $\rho\cdot\cost(\dotbfx,\dotbfy)$, because
  $(\dotbfx,\dotbfy)$ is feasible for $\dotcalI$. Thus the
  cost of $\calA'$ is at most
% 
\begin{equation*}
 \cost(\hatbfx, \hatbfy) + \rho\cdot\cost(\dotbfx,\dotbfy)
	\le
 \rho(\cost(\hatbfx, \hatbfy) + \cost(\dotbfx,\dotbfy))
		= \rho\cdot\LP^\ast \le \rho\cdot\OPT,
\end{equation*}
%
where the first inequality follows from $\rho\geq 1$. This completes
the proof.
\end{proof}

The demand reduction step has two nice consequences which we
describe in the next two sections. In Section~\ref{sec:
  reduction_to_ftfl} we give a reduction from {\FTFP} to
{\FTFL}. In Section~\ref{sec: large_demands}, we give a
precise statement that confirms an intuitively appealing
result. That is, when all demands $r_j$ are large, then the
fractional optimal solution is very close to an integral
solution. In other words, we can round the fractional
solution to an integral solution with almost the same cost.


\section{Reduction from FTFP to FTFL}
\label{sec: reduction_to_ftfl}
Given the demand reduction technique, we may assume that we
are working with a restricted version of FTFP where every
demand $r_j$ is no more than $|\sitesset|$. In this case we
can reduce this version of FTFP to FTFL. For the reduction
we simply creates $|\sitesset|$ facilities at each site, and
every such facility may be opened or closed later. Thus we
have an FTFL instance where every client have a demand $r_j$
and every facility could be opened or closed. It follows
that any FTFL rounding algorithm can be applied to solve
this FTFL instance, and the solution trivially maps to a
solution for the corresponding FTFP instance. Moreover, the
approximation ratio for FTFL is preserved for FTFP. Given
that FTFL has an LP-rounding $1.7245$-approximation
algorithm by Byrka, Srinivasan and Swamy~\cite{ByrkaSS10},
it is immediate that FTFP has an approximation algorithm
with the same ratio. On the other hand, as we show in
Chapter~\ref{ch: lp-rounding}, {\FTFP} can be approximated
with a ratio of $1.575$. Thus from the standpoint of
approximation, {\FTFP} is more amenable than {\FTFL}.

\section{Asymptotic Approximation Ratio for Large
  Demands}
\label{sec: large_demands}
When all demands are large, one would expect that the
fractional optimal solution to LP (\ref{eqn:fac_primal}) is
very close to an integral solution, and it is reasonable to
expect that in this case the fractional solution can be
rounded to an integral solution with almost the same
cost. We turn this intuition into a concrete statement,
Theorem~\ref{thm:largeR}.

\begin{theorem}
  \label{thm:largeR}
  Given an {\FTFP} instance $(\sitesset, \clientset, \{r_j\}
  , \{f_i\}, \{d_{ij}\})$, let $m = |\sitesset|$ be the
  number of sites, and let $Q = \min_{j\in\clientset} r_j$
  be the minimum demand, then there is an approximation
  algorithm with ratio $1 + O(m/Q)$. In particular, when $Q$
  is large compared to $m$, this ratio approaches $1$.
\end{theorem}
%%%%%%%%%%%%%%%%%%%
\begin{proof}
  We first solve the LP~(\ref{eqn:fac_primal}) for this
  instance and obtain an optimal frational solution
  $(\bfx^\ast, \bfy^\ast)$. Then we apply demand reduction
  to obtain the two instances $\hat \calI$ and $\dot
  \calI$. For the $\hat \calI$ instance we already have an
  optimal integral solution, namely
  $(\hat\bfx,\hat\bfy)$. We now deal with the $\dot\calI$
  instance.

  Lemma~\ref{lem: polynomial demands partition} tells us
  that $\dot r_j$ is no more than $|\sitesset|$ for every
  client $j$. So we can solve the $\dot \calI$ instance by
  creating $m$ independent {\UFL} instances with the same
  parameters $\sitesset, \clientset, \{f_i\},
  \{d_{ij}\}$. Clearly combining the integral solutions to
  the $m$ copies of {\UFL} instances would give us an
  integral solution to the $\dot \calI$ instance (We might
  have to remove some redundant facilities and connections,
  but this only reduces the total cost.). Using a
  $c$-approximation algorithm for {\UFL}~\footnote{We
    actually need more than that. What we need is that the
    integral solution for the {\UFL} instance needs to have
    cost no more than $c$ times the cost of an optimal
    fractional solution. However, almost all LP-rounding
    algorithms for {\UFL} have this property.}, we can
  obtain a solution with cost no more than $c m \cdot
  \LP_{\smallUFL}^\ast$, where $\LP_{\smallUFL}^\ast$ is the
  cost of the optimal fractional solution for the {\UFL}
  instance. On the other hand, it is easy to see that
  $(\bfx^\ast/Q,\bfy^\ast/Q)$ constitutes a feasible
  solution to the {\UFL} instance as we have $r_j \geq Q$
  for every client $j$. As a result we have
  \begin{equation*}
    \LP_{\smallUFL}^\ast \leq \LP^\ast / Q,
  \end{equation*}
  where $\LP^\ast$ is the optimal fractional solution's cost
  to the original {\FTFP} instance $\calI$. Therefore, we
  have an integral solution to the instance $\dot\calI$ with
  cost at most $(cm/Q) \LP^\ast$. Let $S_1$ be the integral
  solution to the instance $\dot\calI$, obtained by
  combining solutions to the $m$ copies of {\UFL} instances,
  and let $S_2 = (\hat\bfx,\hat\bfy)$ be the solution to the
  instance $\hat\calI$. The two solutions, $S_1$ and $S_2$,
  when combined, give a feasible integral solution to the
  instance $\calI$, and the total cost is no more than
  \begin{equation*}
    \cost(S_1) + \cost(S_2) \leq (cm/Q) \LP^\ast + \OPT \leq (1 +
    cm/Q) \OPT = 1 + O(m/Q) \OPT.
  \end{equation*}
  As usual $\OPT$ is the optimal integral solution's cost
  for the original {\FTFP} instance and $\LP^\ast \leq \OPT$
  because an integral solution's cost is lower bounded by
  the optimal fractional solution's cost.
\end{proof}


\section{Adaptive Partition}
\label{sec: adaptive partitioning}

In this section we develop our second technique, which we
call \emph{adaptive partitioning}. Given an FTFP instance
and an optimal fractional solution $(\bfx^\ast, \bfy^\ast)$
to the LP~(\ref{eqn:fac_primal}), we split each client $j$
into $r_j$ individual \emph{unit demand points} (or just
\emph{demands}), and we split each site $i$ into no more
than $|\sitesset|+2R|\clientset|^2$ \emph{facility points}
(or \emph{facilities}), where
$R=\max_{j\in\clientset}r_j$. We denote the demand set by
$\demandset$ and the facility set by $\facilityset$,
respectively.  We will also partition
$(\bfx^\ast,\bfy^\ast)$ into a fractional solution
$(\barbfx,\barbfy)$ for the split instance.  We will
typically use symbols $\nu$ and $\mu$ to index demands and
facilities respectively, that is $\barbfx =
(\barx_{\mu\nu})$ and $\barbfy = (\bary_{\mu})$.  As before,
the \emph{neighborhood of a demand} $\nu$ is
$\wbarN(\nu)=\braced{\mu\in\facilityset \suchthat
  \barx_{\mu\nu}>0}$.  We will use notation $\nu\in j$ to
mean that $\nu$ is a demand of client $j$; similarly,
$\mu\in i$ means that facility $\mu$ is on site
$i$. Different demands of the same client (that is,
$\nu,\nu'\in j$) are called \emph{siblings}.  Further, we
use the convention that $f_\mu = f_i$ for $\mu\in i$,
$\alpha_\nu^\ast = \alpha_j^\ast$ for $\nu\in j$ and
$d_{\mu\nu} = d_{\mu j} = d_{ij}$ for $\mu\in i$ and $\nu\in
j$.  We define $\concost_{\nu}
=\sum_{\mu\in\wbarN(\nu)}d_{\mu\nu}\barx_{\mu\nu} =
\sum_{\mu\in\facilityset}d_{\mu\nu}\barx_{\mu\nu}$.  One can
think of $\concost_{\nu}$ as the average connection cost of
demand $\nu$, if we chose a connection to facility $\mu$
with probability $\barx_{\mu\nu}$. In our partitioned
fractional solution we guarantee for every $\nu$ that
$\sum_{\mu\in\facilityset} \barx_{\mu\nu}=1$.

Some demands in $\demandset$ will be designated as
\emph{primary demands} and the set of primary demands will
be denoted by $P$. By definition we have $P\subseteq \demandset$.
 In addition, we will use the overlap
structure between demand neighborhoods to define a mapping
that assigns each demand $\nu\in\demandset$ to some primary
demand $\kappa\in P$. As shown in the rounding algorithms in
later sections, for each primary demand we guarantee exactly
one open facility in its neighborhood, while for a
non-primary demand, there is constant probability that none
of its neighbors open. In this case we estimate its
connection cost by the distance to the facility opened in
its assigned primary demand's neighborhood. For this reason
the connection cost of a primary demand must be ``small''
compared to the non-primary demands assigned to it. We also
need sibling demands assigned to different primary demands to satisfy
the fault-tolerance requirement. Specifically, this
partitioning will be constructed to satisfy a number of
properties that are detailed below.
%
\begin{description}
	
      \renewcommand{\theenumii}{(\alph{enumii})}
      \renewcommand{\labelenumii}{\theenumii}

\item{(PS)} \emph{Partitioned solution}.
Vector $(\barbfx,\barbfy)$ is a partition of $(\bfx^\ast,\bfy^\ast)$, with unit-value
  demands, that is,

	\begin{enumerate}
		%
	\item \label{PS:one} 
          $\sum_{\mu\in\facilityset} \barx_{\mu\nu} = 1$ for each demand $\nu\in\demandset$. 
		%
	\item \label{PS:xij} $\sum_{\mu\in i, \nu\in j} \barx_{\mu\nu}
          = x^\ast_{ij}$ for each site $i\in\sitesset$ and client $j\in\clientset$.
		%
	\item \label{PS:yi}
          $\sum_{\mu\in i} \bary_{\mu} = y^\ast_i$ for each site $i\in\sitesset$.
		%
	\end{enumerate}
		
\item{(CO)} \emph{Completeness.}
	Solution   $(\barbfx,\barbfy)$ is complete, that is $\barx_{\mu\nu}\neq 0$ implies
				$\barx_{\mu\nu} = \bary_{\mu}$, for all $\mu\in\facilityset, \nu\in\demandset$.

\item{(PD)} \emph{Primary demands.}
	Primary demands satisfy the following conditions:

	\begin{enumerate}
		
	\item\label{PD:disjoint}  For any two different primary demands $\kappa,\kappa'\in P$ we have
				$\wbarN(\kappa)\cap \wbarN(\kappa') = \emptyset$.

	\item \label{PD:yi} For each site $i\in\sitesset$, 
		$ \sum_{\mu\in i}\sum_{\kappa\in P}\barx_{\mu\kappa} \leq y_i^\ast$.
		
	\item \label{PD:assign} Each demand $\nu\in\demandset$ is assigned
        to one primary demand $\kappa\in P$ such that

  			\begin{enumerate}
	
				\item \label{PD:assign:overlap} $\wbarN(\nu) \cap \wbarN(\kappa) \neq \emptyset$, and
				%
				\item \label{PD:assign:cost} $\concost_{\nu}+\alpha_{\nu}^\ast \geq
        			\concost_{\kappa}+\alpha_{\kappa}^\ast$.

			\end{enumerate}

	\end{enumerate}
	
\item{(SI)} \emph{Siblings}. For any pair $\nu,\nu'$ of different siblings we have
  \begin{enumerate}

	\item \label{SI:siblings disjoint}
		  $\wbarN(\nu)\cap \wbarN(\nu') = \emptyset$.
		
	\item \label{SI:primary disjoint} If $\nu$ is assigned to a primary demand $\kappa$ then
 		$\wbarN(\nu')\cap \wbarN(\kappa) = \emptyset$. In particular, by Property~(PD.\ref{PD:assign:overlap}),
		this implies that different sibling demands are assigned to different primary demands.

	\end{enumerate}
	
\end{description}

As we shall demonstrate in later sections, these properties allow us
to extend known UFL rounding algorithms to obtain an integral solution
to our FTFP problem with a matching approximation ratio. Our
partitioning is ``adaptive" in the sense that it is constructed one
demand at a time, and the connection values for the demands of a
client depend on the choice of earlier demands, of this or other
clients, and their connection values. We would like to point out that
the adaptive partitioning process for the $1.575$-approximation
algorithm (Section~\ref{sec: 1.575-approximation}) is more subtle than that for 
the $3$-apprximation (Section~\ref{sec: 3-approximation}) and the
$1.736$-approximation algorithms (Section~\ref{sec:
  1.736-approximation}), due to the introduction of close and far
neighborhood.

%%%%%%%%%%%%%%%%

\paragraph{Implementation of Adaptive Partitioning.}
We now describe an algorithm for partitioning the instance
and the fractional solution so that the properties (PS),
(CO), (PD), and (SI) are satisfied.  Recall that
$\facilityset$ and $\demandset$, respectively, denote the
sets of facilities and demands that will be created in this
stage, and $(\barbfx,\barbfy)$ is the partitioned solution
to be computed. 

The adaptive partitioning algorithm consists of two phases:
Phase 1 is called the partitioning phase and Phase 2 is called
the augmenting phase. Phase 1 is done in iterations, where
in each iteration we find the ``best'' client $j$ and create a
new demand $\nu$ out of it. This demand either becomes a
primary demand itself, or it is assigned to some existing
primary demand. We call a client $j$ \emph{exhausted} when
all its $r_j$ demands have been created and assigned to some
primary demands. Phase 1 completes when all clients are
exhausted. In Phase 2 we ensure that every demand has a
total connection values $\barx_{\mu\nu}$ equal to $1$, that is condition (PS.\ref{PS:one}).

For each site $i$ we will initially create one ``big" facility $\mu$
with initial value $\bary_\mu = y^\ast_i$.  While we partition the
instance, creating new demands and connections, this facility may end
up being split into more facilities to preserve completeness of the
fractional solution. Also, we will gradually decrease the fractional
connection vector for each client $j$, to account for the demands
already created for $j$ and their connection values.  These decreased
connection values will be stored in an auxiliary vector
$\tildebfx$. The intuition is that $\tildebfx$ represents the part of
$\bfx^\ast$ that still has not been allocated to existing demands and
future demands can use $\tildebfx$ for their connections. For
technical reasons, $\tildebfx$ will be indexed by facilities (rather
than sites) and clients, that is $\tildebfx = (\tildex_{\mu j})$.  At
the beginning, we set $\tildex_{\mu j}\assign x_{ij}^\ast$ for each
$j\in\clientset$, where $\mu\in i$ is the single facility created
initially at site $i$.  At each step, whenever we create a new demand
$\nu$ for a client $j$, we will define its values $\barx_{\mu\nu}$ and
appropriately reduce the values $\tildex_{\mu j}$, for all facilities
$\mu$. We will deal with two types of neighborhoods, with respect to
$\tildebfx$ and $\barbfx$, that is $\wtildeN(j)=\{\mu\in\facilityset
\suchthat\tildex_{\mu j} > 0\}$ for $j\in\clientset$ and
$\wbarN(\nu)=\{\mu\in\facilityset \suchthat \barx_{\mu\nu} >0\}$ for
$\nu\in\demandset$.  During this process we preserve the completeness
(CO) of the fractional solutions $\tildebfx$ and $\barbfx$. More
precisely, the following properties will hold for every facility $\mu$
after every iteration,
%
\begin{description}
	
	\item{(c1)} For each demand $\nu$ either $\barx_{\mu\nu}=0$ or
			$\barx_{\mu\nu}=\bary_{\mu}$. This is the same
      condition as condition (CO), yet we repeat it here as
      (c1) needs to hold after every iteration, while
      condition (CO) only applies to the final partitioned
      fractional solution $(\barbfx, \barbfy)$.

	\item{(c2)} For each client $j$,
			either $\tildex_{\mu j}=0$ or $\tildex_{\mu j}=\bary_{\mu}$.
			
\end{description}

A full description of the algorithm is given in
Pseudocode~\ref{alg:lpr2}.  Initially, the set $U$ of
non-exhausted clients contains all clients, the set
$\demandset$ of demands is empty, the set $\facilityset$ of
facilities consists of one facility $\mu$ on each site $i$
with $\bary_\mu = y^\ast_i$, and the set $P$ of primary
demands is empty (Lines 1--4).  In one iteration of the
while loop (Lines 5--8), for each client $j$ we
compute a quantity called $\tcc(j)$ (tentative connection
cost), that represents the average distance from $j$ to the
set $\wtildeN_1(j)$ of the nearest facilities $\mu$ whose
total connection value to $j$ (the sum of $\tildex_{\mu
  j}$'s) equals $1$.  This set is computed by Procedure
$\NearestUnitChunk()$ (see Pseudocode~\ref{alg:helper},
Lines~1--9), which adds facilities to $\wtildeN_1(j)$ in
order of nondecreasing distance, until the total connection
value is exactly $1$. (The procedure actually uses the
$\bary_\mu$ values, which are equal to the connection values,
by the completeness condition (c2).)  This may require splitting the last added
facility and adjusting the connection values so that
conditions (c1) and (c2) are preserved.

%%%%%%%%%%%

\begin{algorithm}[ht]
  \caption{Algorithm: Adaptive Partitioning}
  \label{alg:lpr2}
  \begin{algorithmic}[1]
    \Require $\sitesset$, $\clientset$, $(\bfx^\ast,\bfy^\ast)$
    \Ensure  $\facilityset$,  $\demandset$, $(\barbfx, \barbfy)$ 
    \Comment Unspecified $\barx_{\mu \nu}$'s and $\tildex_{\mu j}$'s are assumed to be $0$

    \State $\tildebfr \assign \bfr, U\assign \clientset, \facilityset\assign \emptyset,
    \demandset\assign \emptyset, P\assign \emptyset$
    \Comment{Phase 1}

    \For{each site $i\in\sitesset$} 
    \State create a facility $\mu$ at $i$ and add $\mu$ to $\facilityset$
    \State $\bary_\mu \assign y_i^\ast$ and $\tildex_{\mu j}\assign
    x_{ij}^\ast$ for each $j\in\clientset$ 
    \EndFor

    \While{$U\neq \emptyset$}
    \For{each $j\in U$}
    \State $\wtildeN_1(j) \assign {\NearestUnitChunk}(j, \facilityset, \tildebfx, \barbfx, \barbfy)$ \Comment see Pseudocode~\ref{alg:helper}
    \State $\tcc(j)\assign \sum_{\mu\in \wtildeN_1(j)} d_{{\mu}j}\cdot \tildex_{\mu j}$
    \EndFor
 
    \State $p \assign {\argmin}_{j\in U}\{ \tcc(j)+\alpha_j^\ast \}$
    \State create a new demand $\nu$ for client $p$

    \If{$\wtildeN_1 (p)\cap \wbarN(\kappa) \neq \emptyset$
      for some primary demand $\kappa\in P$}
    \State assign $\nu$ to $\kappa$
    \State $\barx_{\mu \nu}\assign \tildex_{\mu p}$ and $\tildex_{\mu p}\assign 0$ for each $\mu \in \wtildeN(p) \cap \wbarN(\kappa)$
    \Else 
    \State make $\nu$ primary, $P \assign P \cup \{\nu\}$, assign $\nu$ to itself
    \State set $\barx_{\mu\nu} \assign \tildex_{\mu p}$ and $\tildex_{\mu p}\assign 0$ for each $\mu\in \wtildeN_1(p)$

    \EndIf
    \State $\demandset\assign \demandset\cup \{\nu\},
    \tilder_p \assign \tilder_p -1$
	\State \textbf{if} {$\tilder_p=0$} \textbf{then} $U\assign U \setminus \{p\}$
    \EndWhile

    \For{each client $j\in\clientset$} \Comment{Phase 2}
    \For{each demand $\nu\in j$}    \Comment{each client $j$ has $r_j$ demands}
    \State \textbf{if} $\sum_{\mu\in \wbarN(\nu)}\barx_{\mu\nu}<1$
    \textbf{then} $\AugmentToUnit(\nu, j, \facilityset, \tildebfx, \barbfx, \barbfy)$ \Comment see Pseudocode~\ref{alg:helper}
    \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% subroutine: NearestUnitChunk and AugmentToUnit
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[ht]
  \caption{Helper functions used in Pseudocode~\ref{alg:lpr2}}
  \label{alg:helper}
  \begin{algorithmic}[1]
    \Function{\NearestUnitChunk}{$j, \facilityset, \tildebfx, \barbfx,\barbfy$}		
						\Comment upon return, $\sum_{\mu\in\wtildeN_1(j)} \tildex_{\mu j} = 1$
    \State Let $\wtildeN(j) = \{\mu_1,...,\mu_{q}\}$ where $d_{\mu_1 j} \leq d_{\mu_2 j} \leq \ldots \leq d_{\mu_{q j}}$
    \State Let $l$ be such that $\sum_{k=1}^{l} \bary_{\mu_k} \geq 1$ and $\sum_{k=1}^{l -1} \bary_{\mu_{k}} < 1$
    \State Create a new facility $\sigma$ at the same site as $\mu_l$ and add it to $\facilityset$
			\Comment split $\mu_l$
    \State Set $\bary_{\sigma}\assign \sum_{k=1}^{l} \bary_{\mu_{k}}-1$
					and $\bary_{\mu_l} \assign \bary_{\mu_l} - \bary_{\sigma}$
    \State For each $\nu\in\demandset$ with $\barx_{\mu_{l}\nu}>0$
 			set $\barx_{\mu_{l}\nu} \assign \bary_{\mu_l}$ and $\barx_{\sigma \nu} \assign \bary_{\sigma}$
    \State For each $j'\in\clientset$ with $\tildex_{\mu_{l} j'}>0$ (including $j$)
			set $\tildex_{\mu_l j'} \assign \bary_{\mu_l}$ and $\tildex_{\sigma j'} \assign \bary_\sigma$
	\State (All other new connection values are set to $0$)
    \State \Return $\wtildeN_1(j) = \{\mu_{1},\ldots,\mu_{l-1}, \mu_{l}\}$    				
    \EndFunction

    \Function{\AugmentToUnit}{$\nu, j, \facilityset, \tildebfx, \barbfx, \barbfy$}
    					\Comment $\nu$ is a demand of client $j$
    \While{$\sum_{\mu\in \facilityset} \barx_{\mu\nu} <1$}
    					\Comment upon return, $\sum_{\mu\in\wbarN(\nu)} \barx_{\mu\nu} = 1$
    \State Let $\eta$ be any facility such that $\tildex_{\eta j} > 0$
    \If{$1-\sum_{\mu\in \facilityset} \barx_{\mu\nu} \geq \tildex_{\eta j}$}
    \State $\barx_{\eta\nu} \assign \tildex_{\eta j}, \tildex_{\eta j} \assign 0$
    \Else
    \State Create a new facility $\sigma$ at the same site as $\eta$ and add it to $\facilityset$
    					\Comment split $\eta$
    \State Let $\bary_\sigma \assign 1-\sum_{\mu\in \facilityset} \barx_{\mu\nu}, \bary_{\eta} \assign \bary_{\eta} - \bary_{\sigma}$
    \State Set $\barx_{\sigma\nu}\assign \bary_{\sigma},\; \barx_{\eta \nu} \assign  0,\; \tildex_{\eta j} \assign \bary_{\eta}, \; \tildex_{\sigma j} \assign 0$
    \State For each $\nu' \neq \nu$ with $\barx_{\eta \nu'}>0$, set $\barx_{\eta \nu'} \assign \bary_{\eta},\; \barx_{\sigma \nu'} \assign \bary_{\sigma}$
    \State For each $j' \neq j$ with $\tildex_{\eta j'}>0$, set $\tildex_{\eta j'} \assign \bary_{\eta}, \tildex_{\sigma j'} \assign \bary_{\sigma}$
	\State  (All other new connection values are set to $0$)
    \EndIf
    \EndWhile
    \EndFunction
  \end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%


The next step is to pick a client $p$ with minimum
$\tcc(p)+\alpha_p^\ast$ and create a demand $\nu$ for $p$
(Lines~9--10). If $\wtildeN_1(p)$ overlaps the neighborhood
of some existing primary demand $\kappa$ (if there are
multiple such $\kappa$'s, pick any of them), we assign $\nu$
to $\kappa$, and $\nu$ acquires all the connection values
$\tildex_{\mu p}$ between client $p$ and facility $\mu$ in
$\wtildeN(p)\cap \wbarN(\kappa)$ (Lines~11--13). Note that
although we check for overlap with $\wtildeN_1(p)$, we then
move all facilities in the intersection with $\wtildeN(p)$,
a bigger set, into $\wbarN(\nu)$.  The other case is when
$\wtildeN_1(p)$ is disjoint from the neighborhoods of all
existing primary demands. Then, in Lines~15--16, $\nu$
becomes itself a primary demand and we assign $\nu$ to
itself. It also inherits the connection values to all
facilities $\mu\in\wtildeN_1(p)$ from $p$ (recall that
$\tildex_{\mu p} = \bary_{\mu}$), with all other
$\barx_{\mu\nu}$ values set to $0$.

At this point all primary demands satisfy
Property~(PS.\ref{PS:one}), but this may not be true for
non-primary demands. For those demands we still may need to
adjust the $\barx_{\mu\nu}$ values so that the total
connection value for $\nu$, that is $\connsum(\nu) \stackrel{\mathrm{def}}{=}
\sum_{\mu\in\facilityset}\barx_{\mu \nu}$, is equal $1$. This
is accomplished by Procedure $\AugmentToUnit()$ (definition
in Pseudocode~\ref{alg:helper}, Lines~10--21) that allocates
to $\nu\in j$ some of the remaining connection values
$\tildex_{\mu j}$ of client $j$ (Lines 19--21).
$\AugmentToUnit()$ will repeatedly pick any facility $\eta$ with
$\tildex_{\eta j} >0$.  If $\tildex_{\eta j} \leq
1-\connsum(\nu)$, then the connection value $\tildex_{\eta
  j}$ is reassigned to $\nu$. 
Otherwise, $\tildex_{\eta j} >
1-\connsum(\nu)$, in which case we split $\eta$ so that
connecting $\nu$ to one of the created copies of $\eta$ will
make $\connsum(\nu)$ equal $1$, and we'll be done.


\smallskip

Notice that we start with $|\sitesset|$ facilities and in
each iteration of the while loop in Line~5 (Pseudocode~\ref{alg:lpr2}) each client causes at most one split.
 We have a total of no more than $R|\clientset|$ iterations as in
each iteration we create one demand. (Recall that $R =
\max_jr_j$.) In Phase 2 we do an augment step for each
demand $\nu$ and this creates no more than $R|\clientset|$
new facilities.  So the total number of facilities we
created will be at most $|\sitesset|+ R|\clientset|^2 +
R|\clientset| \leq |\sitesset| + 2R|\clientset|^2$, which is
polynomial in $|\sitesset|+|\clientset|$ due to our earlier
bound on $R$.

%%% example for adaptive partition
\paragraph{Example.}
We now illustrate our partitioning algorithm with an example, where the FTFP instance
has four sites and four clients. The demands are $r_1=1$ and $r_2=r_3=r_4=2$.
The facility costs are $f_i = 1$ for all $i$. The distances are defined as follows: 
$d_{ii} = 3$ for $i=1,2,3,4$ and $d_{ij} = 1$ for all $i\neq j$. 
Solving the LP(\ref{eqn:fac_primal}), we obtain the fractional solution given in
Table~\ref{tbl:example_opt}.
%
{
\small
\begin{table}[ht]
%
\hfill
\setlength{\extrarowheight}{4pt}
\begin{subtable}{0.2\textwidth}
  \centering
  \begin{tabular}{c | c c c c | c }
    $x_{ij}^\ast$ & $1$ & $2$ & $3$ & $4$ & $y_{i}^\ast$\\
    \hline
    $1$ & 0 & $\fourthirds$ & $\fourthirds$ & $\fourthirds$ & $\fourthirds$ \\
    $2$ & $\onethird$ & 0 & $\onethird$ & $\onethird$ & $\onethird$ \\
    $3$ & $\onethird$ & $\onethird$ & 0 & $\onethird$ & $\onethird$ \\
    $4$ & $\onethird$ & $\onethird$ & $\onethird$ & 0 & $\onethird$ \\
  \end{tabular}
  \subcaption{}
  \label{tbl:example_opt}
\end{subtable}
%
\hspace{0.8in}
%
\begin{subtable}{0.4\textwidth}
  \centering
  \begin{tabular}{c | c c c c c c c | c} % seven demands, five facilities
    $\barx_{\mu\nu}$ & $1'$ & $2'$ & $2''$ & $3'$ & $3''$ & $4'$ & $4''$ & $\bary_{\mu}$ \\
    \hline
    $\dot{1}$ & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1\\
    $\ddot{1}$ & 0 & 0 & $\onethird$ & 0 & $\onethird$ & 0 & $\onethird$ & $\onethird$ \\
    $\dot{2}$ & $\onethird$ & 0 & 0 & 0 & $\onethird$ & 0 & $\onethird$  & $\onethird$ \\
    $\dot{3}$ & $\onethird$ & 0 & $\onethird$ & 0 & 0 & 0 & $\onethird$  & $\onethird$ \\
    $\dot{4}$ & $\onethird$ & 0 & $\onethird$ & 0 & $\onethird$ & 0 & 0 & $\onethird$ \\
  \end{tabular}
  \subcaption{}
  \label{tbl:example_part}
\end{subtable}
\hfill{\ }
%
\caption[An example of an execution of the partitioning algorithm.]{
  An example of an execution of the partitioning algorithm.
  (a) An optimal fractional solution $x^\ast,y^\ast$.
  (b) The partitioned solution. $j'$ and $j''$ denote the first and second demand of a client $j$, 
	and $\dot{\imath}$ and $\ddot{\imath}$ denote the first and second facility at site $i$.}
%
\end{table}
}

It is easily seen that the fractional solution in
Table~\ref{tbl:example_opt} is optimal and complete ($x_{ij}^\ast > 0$
implies $x_{ij}^\ast = y_i^\ast$). The dual optimal solution has all
$\alpha_j^\ast = 4/3$ for $j=1,2,3,4$.

Now we perform Phase 1, the adaptive partitioning, following the
description in Pseudocode~\ref{alg:lpr2}. To streamline the
presentation, we assume that all ties are broken in favor of
lower-numbered clients, demands or facilities.  First we create one
facility at each of the four sites, denoted as $\dot{1}$, $\dot{2}$,
$\dot{3}$ and $\dot{4}$ (Line~2--4, Pseudocode~\ref{alg:lpr2}).  We
then execute the ``while'' loop in Line 5
Pseudocode~\ref{alg:lpr2}. This loop will have seven iterations.
Consider the first iteration. In Line 7--8 we compute $\tcc(j)$ for
each client $j=1,2,3,4$ in $U$. When computing $\wtildeN_1(2)$,
facility $\dot{1}$ will get split into $\dot{1}$ and $\ddot{1}$ with
$\bary_{\dot{1}}=1$ and $\bary_{\ddot{1}} = 1/3$. (This will happen in
Line~4--7 of Pseudocode~\ref{alg:helper}.)  Then, in Line~9 we will
pick client $p=1$ and create a demand denoted as $1'$ (see
Table~\ref{tbl:example_part}). Since there are no primary demands yet,
we make $1'$ a primary demand with $\wbarN(1') = \wtildeN_1(1) =
\{\dot{2}, \dot{3}, \dot{4}\}$. Notice that client $1$ is exhausted
after this iteration and $U$ becomes $\{2,3,4\}$.

In the second iteration we compute $\tcc(j)$ for $j=2,3,4$ and pick
client $p=2$, from which we create a new demand $2'$. We have
$\wtildeN_1(2) = \{\dot{1}\}$, which is disjoint from $\wbarN(1')$. So
we create a demand $2'$ and make it primary, and set $\wbarN(2') =
\{\dot{1}\}$. In the third iteration we compute $\tcc(j)$ for
$j=2,3,4$ and again we pick client $p=2$. Since $\wtildeN_1(2) =
\{\ddot{1}, \dot{3}, \dot{4}\}$ overlaps with $\wbarN(1')$, we create
a demand $2''$ and assign it to $1'$. We also set $\wbarN(2'') =
\wbarN(1') \cap \wtildeN(2) = \{\dot{3}, \dot{4}\}$. After this
iteration client $2$ is exhausted and we have $U = \{3,4\}$.

In the fourth iteration we compute $\tcc(j)$ for client $j=3,4$. We
pick $p=3$ and create demand $3'$. Since $\wtildeN_1(3) = \{\dot{1}\}$
overlaps $\wbarN(2')$, we assign $3'$ to $2'$ and set
$\wbarN(3') = \{\dot{1}\}$. In the fifth iteration we compute
$\tcc(j)$ for client $j=3,4$ and pick $p=3$ again. At this time
$\wtildeN_1(3) = \{\ddot{1},\dot{2},\dot{4}\}$, which overlaps with
$\wbarN(1')$. So we create a demand $3''$ and assign it to $1'$, as
well as set $\wbarN(3'') = \{\dot{2}, \dot{4}\}$.

In the last two iterations we will pick client $p=4$ twice and
create demands $4'$ and $4''$. For $4'$ we have $\wtildeN_1(4) =
\{\dot{1}\}$ so we assign $4'$ to $2'$ and set $\wbarN(4') =
\{\dot{1}\}$. For $4''$ we have $\wtildeN_1(4) = \{\ddot{1}, \dot{2},
\dot{3}\}$ and we assign it to $1'$, as well as set $\wbarN(4'') =
\{\dot{2}, \dot{3}\}$.

Now that all clients are exhausted we perform Phase 2, the augmenting
phase, to construct a fractional solution in which all demands have
total connection value equal to $1$.  We iterate through each of the
seven demands created, that is $1',2',2'',3',3'',4',4''$.  $1'$ and $2'$
already have neighborhoods with total connection value of $1$, so
nothing will change in the first two iterations.
$2''$ has $\dot{3},\dot{4}$ in its neighborhood, with total connection value of
$2/3$, and $\wtildeN(2) = \{\ddot{1}\}$ at this time, so we add
$\ddot{1}$ into $\wbarN(2'')$ to make $\wbarN(2'') = \{\ddot{1},
\dot{3}, \dot{4}\}$ and now $2''$ has total connection value of
$1$. Similarly, $3''$ and $4''$ each get $\ddot{1}$ added to their
neighborhood and end up with total connection value of $1$. The other
two demands, namely $3'$ and $4'$, each have $\dot{1}$ in its
neighborhood so each of them has already its total connection value
equal $1$. This completes Phase 2.

The final partitioned fractional solution is given in
Table~\ref{tbl:example_part}. We have created a total of five
facilities $\dot{1}, \ddot{1}, \dot{2}, \dot{3}, \dot{4}$, and seven
demands, $1',2',2'',3',3'',4',4''$. It can be verified that all the
stated properties are satisfied.

%%%% end example %%%

%%%%%%

\medskip

\emparagraph{Correctness.}  We now show that all the
required properties (PS), (CO), (PD) and (SI) are satisfied
by the above construction.

Properties~(PS) and (CO) follow directly from the
algorithm. (CO) is implied by the completeness condition
(c1) that the algorithm maintains after each
iteration. Condition~(PS.\ref{PS:one}) is a result of
calling Procedure~$\AugmentToUnit()$ in Line~21. To see that
(PS.\ref{PS:xij}) holds, note that
at each step the algorithm maintains the
invariant that, for every $i\in\sitesset$ and
$j\in\clientset$, we have $\sum_{\mu\in i}\sum_{\nu \in j}
\barx_{\mu \nu} + \sum_{\mu\in i} \tildex_{\mu j} =
x_{ij}^\ast$. In the end, we will create $r_j$ demands for
each client $j$, with each demand $\nu\in j$ satisfying
(PS.\ref{PS:one}), and thus $\sum_{\nu\in
  j}\sum_{\mu\in\facilityset}\barx_{\mu\nu}=r_j$.  This
implies that $\tildex_{\mu j}=0$ for every facility
$\mu\in\facilityset$, and (PS.\ref{PS:xij}) follows.
(PS.\ref{PS:yi}) holds because every time we split a
facility $\mu$ into $\mu'$ and $\mu''$, the sum of
$\bary_{\mu'}$ and $\bary_{\mu''}$ is equal to the old value of
$\bary_{\mu}$.

Now we deal with properties in group (PD).  First,
(PD.\ref{PD:disjoint}) follows directly from the algorithm,
Pseudocode~\ref{alg:lpr2} (Lines 14--16), since every
primary demand has its neighborhood fixed when created, and
that neighborhood is disjoint from those of the existing primary
demands.

Property (PD.\ref{PD:yi}) follows from (PD.\ref{PD:disjoint}), (CO) and
(PS.\ref{PS:yi}). In more detail, it can be justified as
follows. By (PD.\ref{PD:disjoint}), for each $\mu\in i$ there
is at most one $\kappa\in P$ with $\barx_{\mu\kappa} > 0$
and we have $\barx_{\mu\kappa} = \bary_{\mu}$ due do (CO).
Let $K\subseteq i$ be the set of those $\mu$'s for which
such $\kappa\in P$ exists, and denote this $\kappa$ by
$\kappa_\mu$. Then, using conditions (CO) and
(PS.\ref{PS:yi}), we have $ \sum_{\mu\in i}\sum_{\kappa\in
  P}\barx_{\mu\kappa} = \sum_{\mu\in K}\barx_{\mu\kappa_\mu}
= \sum_{\mu\in K}\bary_{\mu} \leq \sum_{\mu\in i}
\bary_{\mu} = y_i^\ast$.

Property (PD.\ref{PD:assign:overlap}) follows from the way the algorithm
assigns primary demands.  When demand $\nu$ of
client $p$ is assigned to a primary demand $\kappa$ in
Lines~11--13 of Pseudocode~\ref{alg:lpr2}, we move all
facilities in $\wtildeN(p)\cap \wbarN(\kappa)$ (the
intersection is nonempty) into $\wbarN(\nu)$, and we never
remove a facility from $\wbarN(\nu)$.  We postpone the proof 
for (PD.\ref{PD:assign:cost}) to Lemma~\ref{lem: PD:assign:cost holds}.

Finally we argue that the properties in group (SI)
hold. (SI.\ref{SI:siblings disjoint}) is easy, since for any client
$j$, each facility $\mu$ is added to the neighborhood of at most one
demand $\nu\in j$, by setting $\barx_{\mu\nu}$ to $\bary_\mu$, while
other siblings $\nu'$ of $\nu$ have $\barx_{\mu\nu'}=0$. Note that
right after a demand $\nu\in p$ is created, its neighborhood is
disjoint from the neighborhood of $p$, that is $\wbarN(\nu)\cap
\wtildeN(p) = \emptyset$, by Lines~11--13 of the algorithm. Thus all
demands of $p$ created later will have neighborhoods disjoint from the
set $\wbarN(\nu)$ before the augmenting phase 2. Furthermore,
Procedure~$\AugmentToUnit()$ preserves this property, because when it
adds a facility to $\wbarN(\nu)$ then it removes it from
$\wtildeN(p)$, and in case of splitting, one resulting facility is
added to $\wbarN(\nu)$ and the other to $\wtildeN(p)$. Property
(SI.\ref{SI:primary disjoint}) is shown below in Lemma~\ref{lem:
  property SI:primary disjoint holds}.

It remains to show Properties~(PD.\ref{PD:assign:cost}) and
(SI.\ref{SI:primary disjoint}). We show them in the lemmas
below, thus completing the description of our adaptive
partition process.

%%%%%%%

\begin{lemma}\label{lem: property SI:primary disjoint holds}
  Property~(SI.\ref{SI:primary disjoint}) holds after the
  Adaptive Partitioning stage.
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
  Let $\nu_1,\ldots,\nu_{r_j}$ be the demands of a client
  $j\in\clientset$, listed in the order of creation, and, for each
  $q=1,2,\ldots,r_j$, denote by $\kappa_q$ the primary demand that
  $\nu_q$ is assigned to. After the completion of Phase~1 of
  Pseudocode~\ref{alg:lpr2} (Lines 5--18), we have
  $\wbarN(\nu_s)\subseteq \wbarN(\kappa_s)$ for  $s=1,\ldots,r_j$. 
Since any two primary demands have disjoint
  neighborhoods, we have $\wbarN(\nu_s) \cap \wbarN(\kappa_q) =
  \emptyset$ for any $s\neq q$, that is
	Property~(SI.\ref{SI:primary disjoint}) holds right after Phase~1.

        After Phase~1 all neighborhoods $\wbarN(\kappa_s),
        s=1,\ldots,r_j$ have already been fixed and they do not change
        in Phase~2.  None of the facilities in $\wtildeN(j)$ appear in
        any of $\wbarN(\kappa_s)$ for $s=1,\ldots,r_j$, by the way we
        allocate facilities in Lines~13 and 16.  Therefore during the
        augmentation process in Phase~2, when we add facilities from
        $\wtildeN(j)$ to $\wbarN(\nu)$, for some $\nu\in j$
        (Line~19--21 of Pseudocode~\ref{alg:lpr2}), all the required
        disjointness conditions will be preserved.
\end{proof}

%%%%%%%

We need one more lemma before proving our last property
(PD.\ref{PD:assign:cost}).  For a client $j$ and a demand
$\nu$, we use notation $\tcc^{\nu}(j)$ for the value of
$\tcc(j)$ at the time when $\nu$ was created. (It is not
necessary that $\nu\in j$ but we assume that $j$ is not
exhausted at that time.)


\begin{lemma}\label{lem: tcc optimal}
  Let $\eta$ and $\nu$ be two demands, with $\eta$ created
  no later than $\nu$, and let $j\in\clientset$ be a client
  that is not exhausted when $\nu$ is created. Then we have
\begin{description}
	\item{(a)} $\mbox{\tcc}^\eta(j) \le \mbox{\tcc}^{\nu}(j)$, and 
	\item{(b)} if $\nu\in j$ then $\mbox{\tcc}^\eta(j) \le \concost_{\nu}$.
\end{description}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
  We focus first on the time when demand $\eta$ is about to be created,
  right after the call to $\NearestUnitChunk()$ in
  Pseudocode~\ref{alg:lpr2}, Line~7.  Let $\wtildeN(j) =
  \{\mu_1,...,\mu_q\}$ with all facilities $\mu_s$ ordered
  according to nondecreasing distance from $j$.  Consider
  the following linear program,
%
\begin{empheq}[box=\fbox]{align*}
	\textrm{minimize} \quad & \sum_s d_{\mu_s j}z_s
			\\
	\textrm{subject to} \quad & \sum_s z_s  \ge 1
			\\
 	0 &\le z_s \le \tildex_{\mu_s j} \quad \textrm{for all}\ s
\end{empheq}
%
  This is a fractional
  minimum knapsack covering problem (with knapsack size equal $1$) and its optimal fractional
  solution is the greedy solution, whose value is exactly
  $\tcc^\eta(j)$.  

On the other hand, we claim that
  $\tcc^{\nu}(j)$ can be thought of as the value of some feasible
  solution to this linear program, and that the same is true for $\concost_{\nu}$ if $\nu\in j$.
  Indeed, each of these
  quantities involves some later values $\tildex_{\mu j}$,
  where $\mu$ could be one of the facilities $\mu_s$ or a
  new facility obtained from splitting. For each $s$,
  however, the sum of all values $\tildex_{\mu j}$,
  over the facilities $\mu$ that were split from $\mu_s$, cannot exceed
 the value $\tildex_{\mu_s j}$ at the time when
  $\eta$ was created, because splitting facilities preserves this sum and
 creating new demands for $j$ can only decrease it.
Therefore both quantities
  $\tcc^{\nu}(j)$ and $\concost_{\nu}$ (for $\nu\in j$) correspond to some
  choice of the $z_s$ variables (adding up to $1$), and the
  lemma follows.
\end{proof}

%%%%%%%

\begin{lemma}\label{lem: PD:assign:cost holds}
Property~(PD.\ref{PD:assign:cost}) holds after the Adaptive Partitioning stage.
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
Suppose that demand $\nu\in j$ is assigned to some primary demand $\kappa\in p$.
Then
%
\begin{eqnarray*}
 \concost_{\kappa} + \alpha_{\kappa}^\ast \;=\; \tcc^{\kappa}(p) + \alpha^\ast_p
 					\;\le\; \tcc^{\kappa}(j) + \alpha^\ast_j   
					\;\le\; \concost_{\nu} + \alpha^\ast_\nu.
\end{eqnarray*}
%
We now justify this derivation. By definition we have
$\alpha_{\kappa}^\ast = \alpha^\ast_p$.  Further, by the
algorithm, if $\kappa$ is a primary demand of client $p$,
then $\concost_{\kappa}$ is equal to $\tcc(p)$ computed when
$\kappa$ is created, which is exactly $\tcc^{\kappa}(p)$. Thus
the first equation is true. The first inequality follows
from the choice of $p$ in Line~9 in
Pseudocode~\ref{alg:lpr2}. The last inequality holds
because $\alpha^\ast_j = \alpha^\ast_\nu$ (due to $\nu\in
j$), and because $\tcc^{\kappa}(j) \le \concost_{\nu}$, which
follows from Lemma~\ref{lem: tcc optimal}.
\end{proof}

We have thus proved that all properties (PS), (CO), (PD) and (SI) hold
for our partitioned fractional solution $(\barbfx,\barbfy)$. In the
following sections we show how to use these properties to round the
fractional solution to an approximate integral solution. For the
$3$-approximation algorithm (Section~\ref{sec: 3-approximation}) and
the $1.736$-approximation algorithm (Section~\ref{sec:
  1.736-approximation}), the first phase of the algorithm is exactly
the same partition process as described above. However, the
$1.575$-approximation algorithm (Section~\ref{sec:
  1.575-approximation}) demands a more sophisticated partitioning
process as the interplay between close and far neighborhood of sibling
demands result in more delicate properties that our partitioned
fractional solution must satisfy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ch5 LP-ROUNDING RESULTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{LP-rounding Algorithms} \label{ch: lp-rounding}

In Section~\ref{sec: adaptive partitioning} of
Chapter~\ref{ch: techniques}, we have seen that the adaptive
partitioning technique produces a fractional solution for
individual facilities and unit demand points with a number
of structural properties. In this chapter we show how those
properties help in designing LP-rounding algorithms with
good approximation ratios. We start with a simple algorithm
with ratio $3$ that illustrates the main steps of a rounding
algorithm and the use of the structural properties to derive
an approximation ratio. A more refined rounding algorithm
with ratio $1.736$ is presented next, using the same
partitioned fractional solution as a starting point. Our
best approximation algorithm with ratio $1.575$ is presented
last, and the algorithm also requires a more sophisticated
structure of the fractional solution, compared to the one
used by both the $3$-approximation and the
$1.736$-approximation algorithms.

%% EGUP 3
\section{Algorithm~{\EGUP} with Ratio $3$}
\label{sec: 3-approximation}

The algorithm we describe in this section achieves ratio
$3$. Although this is still quite far from our best ratio
$1.575$ that we derive later, we include this algorithm in
the paper to illustrate, in a relatively simple setting, how
the properties of our partitioned fractional solution are
used in rounding it to an integral solution with cost not
too far away from an optimal solution.  The rounding
approach we use here is an extension of the corresponding
method for UFL described in~\cite{gupta08}.

\paragraph{Algorithm~{\EGUP.}}
At a high level, we would open exactly one facility for each
primary demand $\kappa$, and each non-primary demand is
connected to the facility opened for the primary demand it
was assigned to.

More precisely, we apply a rounding process, guided by the
fractional values $(\bary_{\mu})$ and $(\barx_{\mu\nu})$,
that produces an integral solution. This integral solution
is obtained by choosing a subset of facilities in
$\facilityset$ to open, and for each demand in $\demandset$,
specifying an open facility that this demand will be
connected to.  For each primary demand $\kappa\in P$, we
want to open one facility $\phi(\kappa) \in
\wbarN(\kappa)$. To this end, we use randomization: for each
$\mu\in\wbarN(\kappa)$, we choose $\phi(\kappa) = \mu$ with
probability $\barx_{\mu\kappa}$, ensuring that exactly one
$\mu \in \wbarN(\kappa)$ is chosen. Note that
$\sum_{\mu\in\wbarN(\kappa)}\barx_{\mu\kappa}=1$, so this
distribution is well-defined.  We open this facility
$\phi(\kappa)$ and connect to $\phi(\kappa)$ all demands
that are assigned to $\kappa$.

In our description above, the algorithm is presented as a
randomized algorithm. It can be de-randomized using the
method of conditional expectations, which is commonly used
in approximation algorithms for facility location problems
and standard enough that presenting it here would be
redundant. Readers less familiar with this field are
recommended to consult \cite{ChudakS04}, where the method of
conditional expectations is applied in a context very
similar to ours.

%%%%%%%%%

\paragraph{Analysis.}
We now bound the expected facility cost and connection cost
by establishing the two lemmas below.

%%%%%

\begin{lemma}\label{lemma:3fac}
The expectation of facility cost $F_{\smallEGUP}$ of our solution is
  at most $F^\ast$.
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
  By Property~(PD.\ref{PD:disjoint}), the neighborhoods of
  primary demands are disjoint. Also, for any primary demand
  $\kappa\in P$, the probability that a facility
  $\mu\in\wbarN(\kappa)$ is chosen as the open facility
  $\phi(\kappa)$ is $\barx_{\mu\kappa}$. Hence the expected
  total facility cost is
%
\begin{empheq}[box=\fbox]{align*}
    \Exp[F_{\smallEGUP}]
	&= \textstyle{\sum_{\kappa\in P}\sum_{\mu\in\wbarN(\kappa)}} f_{\mu} \barx_{\mu\kappa}
	\\
	&= \textstyle{\sum_{\kappa\in P}\sum_{\mu\in\facilityset}} f_{\mu} \barx_{\mu\kappa} 
	\\
	&= \textstyle{\sum_{i\in\sitesset}} f_i \textstyle{\sum_{\mu\in i}\sum_{\kappa\in P}} \barx_{\mu\kappa} 
	\\
	&\leq \textstyle{\sum_{i\in\sitesset}} f_i y_i^\ast 
	= F^\ast,
\end{empheq}
%
where the inequality follows from Property~(PD.\ref{PD:yi}).
\end{proof}

%%%%%%%

\begin{lemma}\label{lemma:3dist}
The expectation of connection cost $C_{\smallEGUP}$ of our solution
is at most  $C^\ast+2\cdot\LP^\ast$.
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
  For a primary demand $\kappa$, its expected connection cost is
  $C_{\kappa}^{\avg}$ because we choose facility $\mu$ with
  probability $\barx_{\mu\kappa}$.

  Consider a non-primary demand $\nu$ assigned to a primary demand
  $\kappa\in P$. Let $\mu$ be any facility in $\wbarN(\nu) \cap
  \wbarN(\kappa)$.  Since $\mu$ is in both $\wbarN(\nu)$ and
  $\wbarN(\kappa)$, we have $d_{\mu\nu} \leq \alpha_{\nu}^\ast$ and
  $d_{\mu\kappa} \leq \alpha_{\kappa}^\ast$ (This follows from the
  complementary slackness conditions since
  $\alpha_{\nu}^\ast=\beta_{\mu\nu}^\ast + d_{\mu\nu}$ for each
  $\mu\in \wbarN(\nu)$.). Thus, applying the triangle inequality, for
  any fixed choice of facility $\phi(\kappa)$ we have
%
\begin{equation*}
    d_{\phi(\kappa)\nu} \leq d_{\phi(\kappa)\kappa}+d_{\mu\kappa}+d_{\mu\nu}
    \leq d_{\phi(\kappa)\kappa} + \alpha_{\kappa}^\ast + \alpha_{\nu}^\ast.
\end{equation*}
%
Therefore the expected distance from $\nu$ to its facility $\phi(\kappa)$ is 
%
\begin{align*}
  \Exp[  d_{\phi(\kappa)\nu}   ] &\le \concost_{\kappa} + \alpha_{\kappa}^\ast + \alpha_{\nu}^\ast 
\\
  &\leq \concost_{\nu} + \alpha_{\nu}^\ast + \alpha_{\nu}^\ast
   = \concost_{\nu} + 2\alpha_{\nu}^\ast,
  \end{align*}
%
  where the second inequality follows from Property~(PD.\ref{PD:assign:cost}).  
From the definition of $\concost_{\nu}$ and Property~(PS.\ref{PS:xij}), for any $j\in \clientset$ 
we have
%
\begin{align*}
\sum_{\nu\in j} \concost_{\nu} &= \sum_{\nu\in j}\sum_{\mu\in\facilityset}d_{\mu\nu}\barx_{\mu\nu}
			\\
 			&= \sum_{i\in\sitesset} d_{ij}\sum_{\nu\in j}\sum_{\mu\in i}\barx_{\mu\nu}
			\\
			&= \sum_{i\in\sitesset} d_{ij}x^\ast_{ij} 
			= C^\ast_j.
\end{align*}
% 
Thus, summing over all demands, the expected total connection cost is
%
\begin{align*}
    \Exp[C_{\smallEGUP}] &\le 
			\textstyle{\sum_{j\in\clientset} \sum_{\nu\in j}} (\concost_{\nu} + 2\alpha_{\nu}^\ast) 
			\\
    	& = \textstyle{\sum_{j\in\clientset}} (C_j^\ast + 2r_j\alpha_j^\ast)
 		= C^\ast + 2\cdot\LP^\ast,
\end{align*}
%
completing the proof of the lemma.
\end{proof}

%%%%%%%%

\begin{theorem}
Algorithm~{\EGUP} is a $3$-approximation algorithm.
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
  By Property~(SI.\ref{SI:primary disjoint}), different
  demands from the same client are assigned to different
  primary demands, and by (PD.\ref{PD:disjoint}) each primary
  demand opens a different facility. This ensures that our
  solution is feasible, namely each client $j$ is connected
  to $r_j$ different facilities (some possibly located on
  the same site).  As for the total cost,
  Lemma~\ref{lemma:3fac} and Lemma~\ref{lemma:3dist} imply
  that the total cost is at most
  $F^\ast+C^\ast+2\cdot\LP^\ast = 3\cdot\LP^\ast \leq
  3\cdot\OPT$.
\end{proof}

%%%%%%%%%

%% ECHS 1.736
\section{Algorithm~{\ECHS} with Ratio $1.736$}
\label{sec: 1.736-approximation}

In this section we improve the approximation ratio to $1+2/e \approx
1.736$. The improvement comes from a slightly modified rounding
process and refined analysis.  Note that the facility opening cost of
Algorithm~{\EGUP} does not exceed that of the fractional optimum
solution, while the connection cost could be far from the optimum,
since we connect a non-primary demand to a facility in the neighborhood of
its assigned primary demand and then estimate the distance using the
triangle inequality. The basic idea to improve the estimate of the connection cost,
following the approach of Chudak and Shmoys~\cite{ChudakS04}, 
is to connect each non-primary demand to its
nearest neighbor when one is available, and to only use the facility opened by
its assigned primary demand when none of its neighbors is open.

%%%%%%%%%%

\paragraph{Algorithm~{\ECHS}.}
As before,
the algorithm starts by solving the linear program and applying the
adaptive partitioning algorithm  described in 
Section~\ref{sec: adaptive partitioning} to obtain a partitioned
solution $(\barbfx, \barbfy)$. Then we apply the rounding
process to compute an integral solution (see Pseudocode~\ref{alg:lpr3}).  

We start, as before, by opening exactly one facility $\phi(\kappa)$ in the 
neighborhood of each primary demand $\kappa$ (Line 2).  For any
non-primary demand $\nu$ assigned to $\kappa$, we refer to
$\phi(\kappa)$ as the \emph{target} facility of $\nu$.  In
Algorithm~{\EGUP}, $\nu$ was connected to $\phi(\kappa)$,
but in Algorithm~{\ECHS} we may be able to find an open
facility in $\nu$'s neighborhood and connect $\nu$ to this
facility.  Specifically, the two changes in the
algorithm are as follows:
%
\begin{description}
\item{(1)} Each facility $\mu$ that is not in the neighborhood of any
  primary demand is opened, independently, with probability
  $\bary_{\mu}$ (Lines 4--5). Notice that if $\bary_\mu>0$ then, due
  to completeness of the partitioned fractional solution, we have
  $\bary_{\mu}= \barx_{\mu\nu}$ for some demand $\nu$. This implies
  that $\bary_{\mu}\leq 1$, because $\barx_{\mu\nu}\le 1$, by
  (PS.\ref{PS:one}).
%
\item{(2)} When connecting demands to facilities, a primary demand
  $\kappa$ is connected to the only facility $\phi(\kappa)$ opened in
  its neighborhood, as before (Line 3).  For a non-primary demand
  $\nu$, if its neighborhood $\wbarN(\nu)$ has an open facility, we
  connect $\nu$ to the closest open facility in $\wbarN(\nu)$ (Line
  8). Otherwise, we connect $\nu$ to its target facility (Line 10).
%
\end{description}

%%%%%%%%%%%%%

\begin{algorithm}
  \caption{Algorithm~{\ECHS}:
    Constructing Integral Solution}
  \label{alg:lpr3}
  \begin{algorithmic}[1]
    \For{each $\kappa\in P$} 
    \State choose one $\phi(\kappa)\in \wbarN(\kappa)$,
    with each $\mu\in\wbarN(\kappa)$ chosen as $\phi(\kappa)$
    with probability $\bary_\mu$ 
    \State open $\phi(\kappa)$ and connect $\kappa$ to $\phi(\kappa)$
    \EndFor
    \For{each $\mu\in\facilityset - \bigcup_{\kappa\in P}\wbarN(\kappa)$} 
    \State open $\mu$ with probability $\bary_\mu$ (independently)
    \EndFor
    \For{each non-primary demand $\nu\in\demandset$}
    \If{any facility in $\wbarN(\nu)$ is open}
    \State{connect $\nu$ to the nearest open facility in $\wbarN(\nu)$}
    \Else
    \State connect $\nu$ to $\phi(\kappa)$ where $\kappa$ is $\nu$'s
     assigned primary demand
    \EndIf
    \EndFor
  \end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%

\paragraph{Analysis.}
We shall first argue that the integral solution thus
constructed is feasible, and then we bound the total cost of
the solution. Regarding feasibility, the only constraint
that is not explicitly enforced by the algorithm is the
fault-tolerance requirement; namely that each client $j$ is
connected to $r_j$ different facilities. Let $\nu$ and
$\nu'$ be two different sibling demands of client $j$ and let
their assigned primary demands be $\kappa$ and $\kappa'$
respectively. Due to (SI.\ref{SI:primary
  disjoint}) we know $\kappa \neq \kappa'$. From
(SI.\ref{SI:siblings disjoint}) we have $\wbarN(\nu) \cap
\wbarN(\nu') = \emptyset$. From (SI.\ref{SI:primary
  disjoint}), we have $\wbarN(\nu) \cap \wbarN(\kappa') =
\emptyset$ and $\wbarN(\nu') \cap \wbarN(\kappa) =
\emptyset$. From (PD.\ref{PD:disjoint}) we have
$\wbarN(\kappa)\cap \wbarN(\kappa') = \emptyset$. It follows
that $(\wbarN(\nu) \cup \wbarN(\kappa)) \cap (\wbarN(\nu')
\cup \wbarN(\kappa')) = \emptyset$. Since the algorithm
connects $\nu$ to some facility in $\wbarN(\nu) \cup
\wbarN(\kappa)$ and $\nu'$ to some facility in $\wbarN(\nu')
\cup \wbarN(\kappa')$, $\nu$ and $\nu'$ will be connected to
different facilities.


%%%%%%%%%

\smallskip
We now show that the expected cost of the computed solution is bounded by
$(1+2/e) \cdot \LP^\ast$. By
(PD.\ref{PD:disjoint}), every facility may appear in at
most one primary demand's neighborhood, and the facilities
open in Line~4--5 of Pseudocode~\ref{alg:lpr3} do not appear
in any primary demand's neighborhood. Therefore, by
linearity of expectation, the expected facility cost of
Algorithm~{\ECHS} is 
%
\begin{equation*}
\Exp[F_{\smallECHS}] 
	= \sum_{\mu\in\facilityset} f_\mu \bary_{\mu} 
	= \sum_{i\in\sitesset} f_i\sum_{\mu\in i} \bary_{\mu} 
	= \sum_{i\in\sitesset} f_i y_i^\ast = F^\ast,
\end{equation*}
%
where the third equality follows from (PS.\ref{PS:yi}).

\smallskip

To bound the connection cost, we adapt an argument of Chudak
and Shmoys~\cite{ChudakS04}. Consider a demand $\nu$ and denote by $C_\nu$ the
random variable representing the connection cost for $\nu$.
Our goal now is to estimate $\Exp[C_\nu]$, the expected value of $C_\nu$.
Demand $\nu$ can either get connected directly to some facility in
$\wbarN(\nu)$ or indirectly to its target facility $\phi(\kappa)\in
\wbarN(\kappa)$, where $\kappa$ is the primary demand to
which $\nu$ is assigned. We will analyze these two cases separately.

In our analysis, in this section and the next one, we will use notation
%
\begin{equation*}
D(A,\sigma) {=} \sum_{\mu\in A}
d_{\mu\sigma}\bary_{\mu}/\sum_{\mu\in A} \bary_{\mu}
\end{equation*}
%
for the average distance between a demand $\sigma$ and a set $A$ of facilities.
Note that, in particular, we have $\concost_\nu = D(\wbarN(\nu),\nu)$.

We first estimate the expected cost $d_{\phi(\kappa)\nu}$ of the indirect
connection. Let $\Lambda^\nu$ denote the event that some 
facility in $\wbarN(\nu)$ is opened. Then
%
\begin{equation}
	\Exp[C_\nu \mid\neg\Lambda^\nu] 
	=   \Exp[ d_{\phi(\kappa)\nu} \mid \neg\Lambda^\nu] 
	= 	D(\wbarN(\kappa) \setminus \wbarN(\nu), \nu).
			\label{eqn: expected indirect connection}
\end{equation}
%
Note that $\neg\Lambda^\nu$ implies that $\wbarN(\kappa) \setminus
\wbarN(\nu)\neq\emptyset$, since $\wbarN(\kappa)$ contains
exactly one open facility, namely $\phi(\kappa)$.

%%%%%%%%%%

\begin{lemma}
  \label{lem:echu indirect}
  Let $\nu$ be a demand assigned to a primary demand $\kappa$, and
assume that $\wbarN(\kappa) \setminus \wbarN(\nu)\neq\emptyset$.
Then 
%
\begin{equation*}
	\Exp[ C_\nu \mid\neg\Lambda^\nu]  \leq
  		\concost_\nu+2\alpha_{\nu}^\ast.
\end{equation*}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
By (\ref{eqn: expected indirect connection}), we need to show that $D(\wbarN(\kappa)
  \setminus \wbarN(\nu), \nu) \leq \concost_\nu +
  2\alpha_{\nu}^\ast$. There are two cases to consider.

\begin{description}
%	
\item{\mycase{1}}
	 There exists some $\mu'\in \wbarN(\kappa) \cap
  \wbarN(\nu)$ such that $d_{\mu' \kappa} \leq \concost_\kappa$.
In this case, for every $\mu\in \wbarN(\kappa)\setminus \wbarN(\nu)$, we have
%
\begin{equation*}
d_{\mu \nu} \leq d_{\mu \kappa} + d_{\mu' \kappa} + d_{\mu' \nu}  
 	\le  \alpha^\ast_\kappa + \concost_\kappa + \alpha^\ast_{\nu}
  \leq \concost_\nu + 2\alpha_{\nu}^\ast,
\end{equation*}
%
using the triangle inequality, complementary slackness, and (PD.\ref{PD:assign:cost}).
By summing over all $\mu\in \wbarN(\kappa) \setminus \wbarN(\nu)$, it
follows that $D(\wbarN(\kappa) \setminus \wbarN(\nu), \nu) \leq
\concost_\nu + 2\alpha_{\nu}^\ast$.

\item{\mycase{2}}
 Every $\mu'\in \wbarN(\kappa)\cap \wbarN(\nu)$
has $d_{\mu'\kappa} > \concost_\kappa$. Since $\concost_{\kappa} = D(\wbarN(\kappa),\kappa)$,
this implies that
$D(\wbarN(\kappa) \setminus \wbarN(\nu),\kappa)\leq \concost_{\kappa}$. Therefore,
choosing an arbitrary $\mu'\in \wbarN(\kappa)\cap \wbarN(\nu)$,
we obtain
%
\begin{equation*}
  D(\wbarN(\kappa) \setminus \wbarN(\nu), \nu) 
	\leq  D(\wbarN(\kappa) \setminus \wbarN(\nu), \kappa) 
			+ d_{\mu' \kappa} + d_{\mu' \nu} 
	\leq  \concost_{\kappa} +
  \alpha_{\kappa}^\ast + \alpha_{\nu}^\ast
	\leq \concost_\nu + 2\alpha_{\nu}^\ast,
\end{equation*}
%
where we again use the triangle inequality,
complementary slackness, and  (PD.\ref{PD:assign:cost}).
%
\end{description}
%
Since the lemma holds in both cases, the proof is now complete.
\end{proof}

We now continue our estimation of the connection cost.  The next step
of our analysis is to show that 
%
\begin{equation}
	\Exp[C_\nu]\le \concost_{\nu} + \frac{2}{e}\alpha^\ast_\nu.
	\label{eqn: echs bound for connection cost}
\end{equation}
%
The argument is divided into three cases. The first, easy case is when
$\nu$ is a primary demand $\kappa$. According to the algorithm
(see Pseudocode~\ref{alg:lpr3}, Line~2), we have $C_\kappa = d_{\mu\kappa}$ with probability $\bary_{\mu}$, 
for $\mu\in \wbarN(\kappa)$. Therefore $\Exp[C_\kappa] = \concost_{\kappa}$, so
(\ref{eqn: echs bound for connection cost}) holds.

Next, we consider a non-primary demand $\nu$. Let $\kappa$
be the primary demand that $\nu$ is assigned to. We first
deal with the sub-case when $\wbarN(\kappa)\setminus
\wbarN(\nu) = \emptyset$, which is the same as
$\wbarN(\kappa) \subseteq \wbarN(\nu)$. Property (CO)
implies that $\barx_{\mu\nu} = \bary_{\mu} =
\barx_{\mu\kappa}$ for every $\mu \in \wbarN(\kappa)$, so we
have $\sum_{\mu\in\wbarN(\kappa)} \barx_{\mu\nu} =
\sum_{\mu\in\wbarN(\kappa)} \barx_{\mu\kappa} = 1$, due to
(PS.\ref{PS:one}). On the other hand, we have
$\sum_{\mu\in\wbarN(\nu)} \barx_{\mu\nu} = 1$, and
$\barx_{\mu\nu} > 0$ for all $\mu\in \wbarN(\nu)$. Therefore
$\wbarN(\kappa) = \wbarN(\nu)$ and $C_\nu$ has exactly the
same distribution as $C_\kappa$.  So this case reduces to
the first case, namely we have $\Exp[C_{\nu}] =
\concost_{\nu}$, and (\ref{eqn: echs bound for connection
  cost}) holds.

The last, and only non-trivial case is when $\wbarN(\kappa)\setminus
\wbarN(\nu)\neq\emptyset$. We handle this case in the following lemma.

%%%%%%

\begin{lemma}\label{lem: echs expected C_nu}
Assume that $\wbarN(\kappa) \setminus \wbarN(\nu) \neq \emptyset$.
Then the expected connection cost of $\nu$, conditioned on the event that at least one of 
its neighbor opens, satisfies
%
\begin{equation*}
  \Exp[C_\nu \mid \Lambda^\nu] \leq \concost_{\nu}.
\end{equation*}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
The proof is similar to an analogous result in~\cite{ChudakS04,ByrkaA10}. 
For the sake of completeness we sketch here a simplified argument, adapted to our
terminology and notation.
The idea is to consider a different random process that is
easier to analyze and whose expected connection cost is not better than that in
the algorithm.

We partition $\wbarN(\nu)$ into groups $G_1,...,G_k$, where two
different facilities $\mu$ and $\mu'$ are put in the same $G_s$, where
$s\in \{1,\ldots,k\}$, if they both belong to the same set
$\wbarN(\kappa)$ for some primary demand $\kappa$. If some $\mu$ is
not a neighbor of any primary demand, then it constitutes a singleton
group.  For each $s$, let $\bard_s = D(G_s,\nu)$ be the average
distance from $\nu$ to $G_s$.  Assume that $G_1,...,G_k$ are ordered
by nondecreasing average distance to $\nu$, that is $\bard_1 \le
\bard_2 \le ... \le \bard_k$.  For each group $G_s$, we select it,
independently, with probability $g_s = \sum_{\mu\in G_s}\bary_{\mu}$.
For each selected group $G_s$,  we
open exactly one facility in $G_s$, where each $\mu\in G_s$
is opened with probability $\bary_{\mu}/\sum_{\eta\in G_s}
\bary_{\eta}$.

So far, this process is the same as that in the algorithm (if restricted to $\wbarN(\nu)$).
However, we connect $\nu$ in a slightly different way, by choosing the smallest
$s$ for which $G_s$ was selected and connecting $\nu$ to the open facility in $G_s$.
This can only increase our expected connection cost, assuming that at least one
facility in $\wbarN(\nu)$ opens, so
%
\begin{align}
  \Exp[C_\nu \mid \Lambda^\nu] &\leq \frac{1}{\Prob[\Lambda^\nu]}
  \left( \bard_1 g_1 + \bard_2 g_2 (1-g_1) + \ldots + \bard_k g_k
    (1-g_1) (1-g_2) \ldots (1-g_k) \right)
			\notag
  \\
  &\leq \frac{1}{\Prob[\Lambda^\nu]}
	\cdot \sum_{s=1}^k \bard_s g_s
	\cdot
		\left(\sum_{t=1}^k g_t \prod_{z=1}^{t-1} (1-g_z)\right)
			\label{eqn: echs ineq direct cost, step 1}
  \\
  &= \sum_{s=1}^k \bard_s g_s
			\label{eqn: echs ineq direct cost, step 2}
	\\
			&= \concost_{\nu}.
				\label{eqn: echs ineq direct cost, step 3}
\end{align}
%
The proof for inequality (\ref{eqn: echs ineq direct cost, step 1}) 
is given in \ref{sec: ECHSinequality} (note that $\sum_{s=1}^k g_s = 1$),
equality (\ref{eqn: echs ineq direct cost, step 2}) follows from
$\Prob[\Lambda^\nu] = 1 - \prod_{t=1}^k (1-g_t)
					= \sum_{t=1}^k g_t
                                        \prod_{z=1}^{t-1} (1 - g_z)$,
and (\ref{eqn: echs ineq direct cost, step 3}) follows from the definition
of the distances $\bard_s$, probabilities $g_s$, and simple algebra.
\end{proof}

Next, we show an estimate on the probability that none of $\nu$'s
neighbors is opened by the algorithm.

\begin{lemma}\label{lem: probability of not Lambda^nu}
The probability that none of $\nu$'s neighbors is opened satisfies
$\Prob[\neg\Lambda^\nu] \le 1/e$.
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
We use the same partition of $\wbarN(\nu)$ into groups $G_1,...,G_k$ as
in the proof of Lemma~\ref{lem: echs expected C_nu}. Denoting by
$g_s$ the probability that a group $G_s$ is selected (and thus that it
has an open facility), we have
%
\begin{equation*}
\Prob[\neg\Lambda^\nu] = \prod_{s=1}^k (1 - g_s)
			\le e^{- \sum_{s=1}^k g_s}
			= e^{-\sum_{\mu \in \wbarN(\nu)} \bary_{\mu}}
			= \frac{1}{e}.
\end{equation*}
%
In this derivation, we first use that $1-x\le e^{-x}$ holds for all $x$,
the second equality follows from $\sum_{s=1}^k g_s = \sum_{\mu \in \wbarN(\nu)} \bary_{\mu}$
and the last equality follows from 
$\sum_{\mu \in \wbarN(\nu)} \bary_{\mu} = 1$.
\end{proof}

We are now ready to estimate the unconditional expected connection cost of $\nu$
(in the case when $\wbarN(\kappa)\setminus \wbarN(\nu)\neq\emptyset$)
as follows,
%
\begin{align}
  \notag
  \Exp[C_\nu] &= \Exp[C_{\nu} \mid \Lambda^\nu] \cdot \Prob[\Lambda^\nu] 
	+ \Exp[C_{\nu} \mid \neg \Lambda^\nu] \cdot	\Prob[\neg \Lambda^\nu]
  \\
  &\leq \concost_{\nu} \cdot \Prob[\Lambda^\nu] 
		+ (\concost_{\nu} + 2\alpha_{\nu}^\ast)  \cdot \Prob[\neg \Lambda^\nu]
  \label{eqn: Cnu estimate 0}
  \\
  &= \concost_{\nu} 
	+  2\alpha_{\nu}^\ast \cdot \Prob[\neg \Lambda^\nu]
		\notag
	\\
	&\le \concost_{\nu} + \frac{2}{e}\cdot\alpha_{\nu}^\ast.
	  \label{eqn: Cnu estimate last}
\end{align}
%
In the above derivation, inequality (\ref{eqn: Cnu estimate 0})
follows from Lemmas~\ref{lem:echu indirect} and \ref{lem: echs expected C_nu}, 
and inequality (\ref{eqn: Cnu estimate last}) follows from
Lemma~\ref{lem: probability of not Lambda^nu}.

\medskip

We have thus shown that the bound (\ref{eqn: echs bound for connection cost})
holds in all three cases.
Summing over all demands $\nu$ of a client $j$, we can now bound
the expected connection cost of client $j$:
%
\begin{equation*}
  \Exp[C_j] = \textstyle\sum_{\nu\in j} \Exp[C_\nu] 
\leq {\textstyle\sum_{\nu\in j} (\concost_{\nu} + \frac{2}{e}\cdot\alpha_{\nu}^\ast) }
  = { C_j^\ast + \frac{2}{e}\cdot r_j\alpha_j^\ast}.
\end{equation*}
%
Finally, summing over all clients $j$, we obtain our bound on
the expected connection cost,
%
\begin{equation*}
 \Exp[ C_{\smallECHS}] \le C^\ast + \frac{2}{e}\cdot\LP^\ast.
\end{equation*}
% 
Therefore we have established that
our algorithm constructs a feasible integral solution with
an overall expected cost 
%
\begin{equation*}
  \label{eq:chudakall}
	 \Exp[ F_{\smallECHS} + C_{\smallECHS}]
	\le
  	F^\ast + C^\ast + \frac{2}{e}\cdot \LP^\ast = (1+2/e)\cdot \LP^\ast
  \leq (1+2/e)\cdot \OPT.
\end{equation*}
%
Summarizing, we obtain the main result of this section.

\begin{theorem}\label{thm:1736}
  Algorithm~{\ECHS} is a $(1+2/e)$-approximation algorithm for \FTFP.
\end{theorem}

%% EBGS 1.575
\section{Algorithm~{\EBGS} with Ratio $1.575$}
\label{sec: 1.575-approximation}

In this section we give our main result, a $1.575$-approximation
algorithm for $\FTFP$, where $1.575$ is the value of $\min_{\gamma\geq
  1}\max\{\gamma, 1+2/e^\gamma, \frac{1/e+1/e^\gamma}{1-1/\gamma}\}$,
rounded to three decimal digits. This matches the ratio of the best
known LP-rounding algorithm for UFL by
Byrka~{\etal}~\cite{ByrkaGS10}. 

Recall that in Section~\ref{sec: 1.736-approximation} we showed how to
compute an integral solution with facility cost bounded by $F^\ast$
and connection cost bounded by $C^\ast + 2/e\cdot\LP^\ast$. Thus,
while our facility cost does not exceed the optimal fractional
facility cost, our connection cost is significantly larger than the
connection cost in the optimal fractional solution.  A natural idea is
to balance these two ratios by reducing the connection cost at the
expense of the facility cost. One way to do this would be to increase
the probability of opening facilities, from $\bary_{\mu}$ (used in
Algorithm~{\ECHS}) to, say, $\gamma\bary_{\mu}$, for some $\gamma >
1$. This increases the expected facility cost by a factor of $\gamma$
but, as it turns out, it also reduces the probability that an indirect
connection occurs for a non-primary demand to $1/e^\gamma$ (from the
previous value $1/e$ in {\ECHS}). As a consequence, for each primary
demand $\kappa$, the new algorithm will select a facility to open from
the nearest facilities $\mu$ in $\wbarN(\kappa)$ such that the
connection values $\barx_{\mu\nu}$ sum up to $1/\gamma$, instead of
$1$ as in Algorithm {\ECHS}. It is easily seen that this will improve
the estimate on connection cost for primary demands.  These two
changes, along with a more refined analysis, are the essence of the
approach in~\cite{ByrkaGS10}, expressed in our terminology.

Our approach can be thought of as a combination of the above ideas
with the techniques of demand reduction and
adaptive partitioning that we introduced earlier. However, our
adaptive partitioning technique needs to be carefully modified,
because now we will be using a more intricate neighborhood structure,
with the neighborhood of each demand divided into two disjoint parts,
and with restrictions on how parts from different demands can overlap.

We begin by describing properties that our partitioned fractional
solution $(\barbfx,\barbfy)$ needs to satisfy. Assume that $\gamma$ is
some constant such that $1 < \gamma < 2$. As mentioned earlier,
the neighborhood $\wbarN(\nu)$ of each demand $\nu$ will be divided
into two disjoint parts.  The first part, called the \emph{close
  neighborhood} and denoted $\wbarclsnb(\nu)$, contains the facilities
in $\wbarN(\nu)$ nearest to $\nu$ with the total connection value
equal $1/\gamma$, that is $\sum_{\mu\in\wbarclsnb(\nu)} \barx_{\mu\nu}
= 1/\gamma$.  The second part, called the \emph{far neighborhood} and
denoted $\wbarfarnb(\nu)$, contains the remaining facilities in
$\wbarN(\nu)$ (so $\sum_{\mu\in\wbarfarnb(\nu)} \barx_{\mu\nu} = 1-1/\gamma$).  We
restate these definitions formally below in Property~(NB).  Recall
that for any set $A$ of facilities and a demand $\nu$, by
$D(A,\nu)$ we denote the average distance between $\nu$ and the
facilities in $A$, that is $D(A,\nu) =\sum_{\mu\in A}
d_{\mu\nu}\bary_{\mu}/\sum_{\mu\in A} \bary_{\mu}$.  We will use
notations $\clsdist(\nu)=D(\wbarclsnb(\nu),\nu)$ and
$\fardist(\nu)=D(\wbarfarnb(\nu),\nu)$ for the average distances from
$\nu$ to its close and far neighborhoods, respectively.  By the
definition of these sets and the completeness property (CO), these
distances can be expressed as
%
\begin{equation*}
\clsdist(\nu)=\gamma\sum_{\mu\in\wbarclsnb(\nu)}
			d_{\mu\nu}\barx_{\mu\nu} \quad\text{and}\quad
\fardist(\nu)=\frac{\gamma}{\gamma-1}\sum_{\mu\in\wbarfarnb(\nu)}
d_{\mu\nu}\barx_{\mu\nu}. 
\end{equation*}
%
We will also use notation $\clsmax(\nu)=\max_{\mu\in\wbarclsnb(\nu)}
d_{\mu\nu}$ for the maximum distance from $\nu$ to its close
neighborhood. The average distance from a demand $\nu$ to its overall
neighborhood $\wbarN(\nu)$ is denoted as $\concost(\nu) =
D(\wbarN(\nu), \nu) = \sum_{\mu \in \wbarN(\nu)} d_{\mu\nu}
\barx_{\mu\nu}$. It is easy to see that
\begin{equation}
  \concost(\nu) = \frac{1}{\gamma} \clsdist(\nu) + \frac{\gamma -
    1}{\gamma} \fardist(\nu).
  \label{eqn:avg dist cls dist far dist}
\end{equation}

Our partitioned solution $(\barbfx,\barbfy)$ must satisfy the same
partitioning and completeness properties as before, namely properties
(PS) and (CO) in Section~\ref{sec: adaptive partitioning}.  In
addition, it must satisfy a new neighborhood property (NB) and modified
properties (PD') and (SI'), listed below.

\begin{description}
	
      \renewcommand{\theenumii}{(\alph{enumii})}
      \renewcommand{\labelenumii}{\theenumii}

\item{(NB)} \label{NB}
	\emph{Neighborhoods.}
	For each demand $\nu \in \demandset$, its neighborhood is divided into \emph{close} and
	\emph{far} neighborhood, that is $\wbarN(\nu) = \wbarclsnb(\nu) \cup \wbarfarnb(\nu)$, where
	%
	\begin{itemize}
	\item $\wbarclsnb(\nu) \cap \wbarfarnb(\nu) = \emptyset$,
	\item $\sum_{\mu\in\wbarclsnb(\nu)} \barx_{\mu\nu} =1/\gamma$, and 
	\item if $\mu\in \wbarclsnb(\nu)$ and $\mu'\in \wbarfarnb(\nu)$ 
				then $d_{\mu\nu}\le d_{\mu'\nu}$.   
	\end{itemize}
	%
	Note that the first two conditions, together with
        (PS.\ref{PS:one}), imply that $\sum_{\mu\in\wbarfarnb(\nu)}
        \barx_{\mu\nu} =1-1/\gamma$. When defining $\wbarclsnb(\nu)$,
        in case of ties, which can occur when some facilities in
        $\wbarN(\nu)$ are at the same distance from $\nu$, we use a
        tie-breaking rule that is explained in the proof of
        Lemma~\ref{lem: PD1: primary overlap} (the only place where
        the rule is needed).

\item{(PD')} \emph{Primary demands.}
	Primary demands satisfy the following conditions:

	\begin{enumerate}
		
	\item\label{PD1:disjoint}  For any two different primary demands $\kappa,\kappa'\in P$ we have
				$\wbarclsnb(\kappa)\cap \wbarclsnb(\kappa') = \emptyset$.

	\item \label{PD1:yi} For each site $i\in\sitesset$, 
		$ \sum_{\kappa\in P}\sum_{\mu\in
                  i\cap\wbarclsnb(\kappa)}\barx_{\mu\kappa} \leq
                y_i^\ast$. In the summation, as before, we overload notation $i$ to stand for the set of
						facilities created on site $i$.
		
	\item \label{PD1:assign} Each demand $\nu\in\demandset$ is assigned
        to one primary demand $\kappa\in P$ such that

  			\begin{enumerate}
	
				\item \label{PD1:assign:overlap} $\wbarclsnb(\nu) \cap \wbarclsnb(\kappa) \neq \emptyset$, and
				%
				\item \label{PD1:assign:cost}
          $\clsdist(\nu)+\clsmax(\nu) \geq
          \clsdist(\kappa)+\clsmax(\kappa)$.
          %
			\end{enumerate}

	\end{enumerate}
	
\item{(SI')} \emph{Siblings}. For any pair $\nu,\nu'\in\demandset$ of different siblings we have
  \begin{enumerate}

	\item \label{SI1:siblings disjoint}
		  $\wbarN(\nu)\cap \wbarN(\nu') = \emptyset$.
		
	\item \label{SI1:primary disjoint} If $\nu$ is assigned to a primary demand $\kappa$ then
 		$\wbarN(\nu')\cap \wbarclsnb(\kappa) = \emptyset$. In particular, by Property~(PD'.\ref{PD1:assign:overlap}),
		this implies that different sibling demands are assigned to different primary demands, since $\wbarclsnb(\nu')$ is a subset of $\wbarN(\nu')$.

	\end{enumerate}
	
\end{description}

%%%%%%%%%%%%%%%%%

\paragraph{Modified adaptive partitioning.}
To obtain a fractional solution with the above properties, we employ a
modified adaptive partitioning algorithm. As in Section~\ref{sec:
  adaptive partitioning}, we have two phases.  In Phase~1 we split
clients into demands and create facilities on sites, while in Phase~2
we augment each demand's connection values $\barx_{\mu\nu}$ so that the total connection
value of each demand $\nu$ is $1$. As the partitioning algorithm proceeds, for any demand $\nu$,
$\wbarN(\nu)$ denotes the set of facilities with $\barx_{\mu\nu} > 0$;
hence the notation $\wbarN(\nu)$ actually represents a dynamic set which gets fixed 
once the partitioning algorithm concludes both Phase 2. On the
other hand, $\wbarclsnb(\nu)$ and $\wbarfarnb(\nu)$ refer to the close
and far neighborhoods at the time when $\wbarN(\nu)$ is fixed.

Similar to the algorithm in Section~\ref{sec: adaptive partitioning},
Phase~1 runs in iterations. Fix some iteration and consider any client
$j$.  As before, $\wtildeN(j)$ is the neighborhood of $j$ with respect
to the yet unpartitioned solution, namely the set of facilities $\mu$
such that $\tildex_{\mu j}>0$. Order the facilities in this set as
$\wtildeN(j) = \braced{\mu_1,...,\mu_q}$ with non-decreasing distance
from $j$, that is $d_{\mu_1 j} \leq d_{\mu_2 j} \leq \ldots \leq
d_{\mu_q j}$. Without loss of generality,
there is an index $l$ for which $\sum_{s=1}^l \tildex_{\mu_s j} =
1/\gamma$, since we can always split one facility to achieve
this. Then we define $\wtildeclsnb(j) = \braced{\mu_1,...,\mu_l}$. 
(Unlike close neighborhoods of demands, $\wtildeclsnb(j)$ can vary over time.)
We also use notation
%
\begin{equation*}
\tcccls(j) =  D(\wtildeclsnb(j), j) = \gamma\sum_{\mu\in\wtildeclsnb(j)} d_{\mu j} \tildex_{\mu j}
			\quad\textrm{ and }\quad
 \dmaxcls(j) = \max_{\mu \in \wtildeclsnb(j)} d_{\mu j}. 
\end{equation*}
%

When the iteration starts, we first find a not-yet-exhausted client
$p$ that minimizes the value of $\tcccls(p) + \dmaxcls(p)$ and create
a new demand $\nu$ for $p$.  Now we have two cases:
%
\begin{description}
%
\item{\mycase{1}} $\wtildeclsnb(p) \cap \wbarN(\kappa)\neq\emptyset$
  for some existing primary demand $\kappa\in P$.  In this case we
  assign $\nu$ to $\kappa$. As before, if there are multiple such
  $\kappa$, we pick any of them. We also fix $\barx_{\mu \nu} \assign
  \tildex_{\mu p}$ and $\tildex_{\mu p}\assign 0$ for each $\mu \in
  \wtildeN(p)\cap \wbarN(\kappa)$. Note that although we
  check for overlap between $\wtildeclsnb(p)$ and $\wbarN(\kappa)$,
  the facilities we actually move into $\wbarN(\nu)$ include all
  facilities in the intersection of $\wtildeN(p)$, a bigger set, with
  $\wbarN(\kappa)$.

  At this time, the total connection value 
	between $\nu$ and $\mu\in \wbarN(\nu)$ is at most $1/\gamma$,
	 since $\sum_{\mu \in \wbarN(\kappa)}\bary_{\mu} = 1/\gamma$ 
	(this follows from the definition of neighborhoods for new primary demands in Case~2 below) 
	and  we have $\wbarN(\nu) \subseteq \wbarN(\kappa)$ at this point. Later
  in Phase 2 we will add additional facilities from $\wtildeN(p)$ to
  $\wbarN(\nu)$ to make $\nu$'s total connection value equal to $1$.

%
\item{\mycase{2}} $\wtildeclsnb(p) \cap \wbarN(\kappa) = \emptyset$
  for all existing primary demands $\kappa\in P$.  In this case we
  make $\nu$ a primary demand (that is, add it to $P$) and assign it
  to itself.  We then move the facilities from $\wtildeclsnb(p)$ to
  $\wbarN(\nu)$, that is for $\mu \in \wtildeclsnb(p)$ we set
  $\barx_{\mu \nu}\assign \tildex_{\mu p}$ and $\tildex_{\mu p}\set
  0$.

  It is easy to see that the total connection value of $\nu$ to
  $\wbarN(\nu)$ is now exactly $1/\gamma$, that is
	$\sum_{\mu \in \wbarN(\nu)}\bary_{\mu} = 1/\gamma$.
Moreover, facilities
  remaining in $\wtildeN(p)$ are all farther away from $\nu$ than
  those in $\wbarN(\nu)$. As we add only facilities from $\wtildeN(p)$
  to $\wbarN(\nu)$ in Phase~2, the final $\wbarclsnb(\nu)$ contains
  the same set of facilities as the current set $\wbarN(\nu)$.
  (More precisely, $\wbarclsnb(\nu)$ consists of the facilities that
	either are currently in $\wbarN(\nu)$ or were obtained from splitting
	the facilities currently in $\wbarN(\nu)$.)
%
\end{description}
%
Once all clients are exhausted, that is, each client $j$ has $r_j$
demands created, Phase~1 concludes. We then run Phase~2, the
augmenting phase, following the same steps as in Section~\ref{sec:
  adaptive partitioning}.  For each client $j$ and each demand $\nu\in
j$ with total connection value to $\wbarN(\nu)$ less than $1$
(that is, $\sum_{\mu\in\wbarN(\nu)} \barx_{\mu\nu} < 1$),
we use our $\AugmentToUnit()$
procedure to add additional facilities (possibly split, if necessary)
from $\wtildeN(j)$ to $\wbarN(\nu)$ to make the total connection value
between $\nu$ and $\wbarN(\nu)$ equal $1$.

\medskip

This completes the description of the partitioning
algorithm. Summarizing, for each client $j\in\clientset$ we 
created $r_j$ demands on the same point as $j$, and we created a number
of facilities at each site $i\in\sitesset$. Thus computed sets of
demands and facilities are denoted $\demandset$ and $\facilityset$,
respectively.  For each facility $\mu\in i$ we defined its fractional
opening value $\bary_\mu$, $0\le \bary_\mu\le 1$, and for each demand
$\nu\in j$ we defined its fractional connection value
$\barx_{\mu\nu}\in \braced{0,\bary_\mu}$.  The connections with
$\barx_{\mu\nu} > 0$ define the neighborhood $\wbarN(\nu)$. The facilities in
$\wbarN(\nu)$ that are closest to $\nu$ and have total connection value from $\nu$ equal
$1/\gamma$ form the close neighborhood $\wbarclsnb(\nu)$, while the remaining facilities
in $\wbarN(\nu)$ form the far neighborhood
$\wbarfarnb(\nu)$. It remains to show that this partitioning satisfies all the desired
properties.

%%%%%%%

\medskip
\paragraph{Correctness of partitioning.}
We now argue that our partitioned fractional solution $(\barbfx,\barbfy)$
satisfies all the stated properties. Properties~(PS), (CO) and (NB) are
directly enforced by the algorithm.

(PD'.\ref{PD1:disjoint}) holds because for each primary demand
$\kappa\in p$, $\wbarclsnb(\kappa)$ is the same set as
$\wtildeclsnb(p)$ at the time when $\kappa$ was created, and
$\wtildeclsnb(p)$ is removed from $\wtildeN(p)$ right after this
step. Further, the partitioning algorithm makes $\kappa$ a primary
demand only if $\wtildeclsnb(p)$ is disjoint from the set
$\wbarN(\kappa')$ of all existing primary demands $\kappa'$ at that
iteration, but these neighborhoods are the same as the final close
neighborhoods $\wbarclsnb(\kappa')$.

The justification of (PD'.\ref{PD1:yi}) is similar to that for
(PD.\ref{PD:yi}) from Section~\ref{sec: adaptive partitioning}. All
close neighborhoods of primary demands are disjoint, due to
(PD'.\ref{PD1:disjoint}), so each facility $\mu \in i$ can appear in
at most one $\wbarclsnb(\kappa)$, for some $\kappa\in P$. Condition
(CO) implies that $\bary_{\mu} = \barx_{\mu\kappa}$ for $\mu \in \wbarclsnb(\kappa)$.
As a result, the summation on
the left-hand side is not larger than $\sum_{\mu\in i}\bary_{\mu} = y_i^\ast$.

Regarding (PD'.\ref{PD1:assign:overlap}), at first glance this
property seems to follow directly from the algorithm, as we only
assign a demand $\nu$ to a primary demand $\kappa$ when $\wbarN(\nu)$
at that iteration overlaps with $\wbarN(\kappa)$ (which is equal to
the final value of $\wbarclsnb(\kappa)$).  However, it is a little
more subtle, as the final $\wbarclsnb(\nu)$ may contain facilities
added to $\wbarN(\nu)$ in Phase 2. Those facilities may turn out to be
closer to $\nu$ than some facilities in $\wbarN(\kappa) \cap
\wtildeN(j) $ (not $\wtildeN_{\cls}(j)$) that we added to
$\wbarN(\nu)$ in Phase 1. If the final $\wbarclsnb(\nu)$ consists only of
facilities added in Phase 2, we no longer have the desired overlap of
$\wbarclsnb(\kappa)$ and $\wbarclsnb(\nu)$. Luckily this bad scenario
never occurs. We postpone the proof of this property to
Lemma~\ref{lem: PD1: primary overlap}.  The proof of
(PD'.\ref{PD1:assign:cost}) is similar to that of Lemma~\ref{lem:
  PD:assign:cost holds}, and we defer it to Lemma~\ref{lem: PD1:
  primary optimal}.

(SI'.\ref{SI1:siblings disjoint}) follows directly from the algorithm
because for each demand $\nu\in j$, all facilities added to
$\wbarN(\nu)$ are immediately removed from $\wtildeN(j)$ and each
facility is added to $\wbarN(\nu)$ of exactly one demand $\nu \in j$.
Splitting facilities obviously preserves (SI'.\ref{SI1:siblings disjoint}).

The proof of (SI'.\ref{SI1:primary disjoint}) is similar to that of
Lemma~\ref{lem: property SI:primary disjoint holds}. If $\kappa=\nu$
then (SI'.\ref{SI1:primary disjoint}) follows from
(SI'.\ref{SI1:siblings disjoint}), so we can assume that
$\kappa\neq\nu$.  Suppose that $\nu'\in j$ is assigned to $\kappa'\in
P$ and consider the situation after Phase~1. By the way we reassign
facilities in Case~1, at this time we have $\wbarN(\nu)\subseteq
\wbarN(\kappa) = \wbarclsnb(\kappa)$ and $\wbarN(\nu')\subseteq
\wbarN(\kappa') =\wbarclsnb(\kappa')$, so $\wbarN(\nu')\cap
\wbarclsnb(\kappa) = \emptyset$, by (PD'.\ref{PD1:disjoint}).
Moreover, we have $\wtildeN(j) \cap \wbarclsnb(\kappa) = \emptyset$
after this iteration, because any facilities that were also in
$\wbarclsnb(\kappa)$ were removed from $\wtildeN(j)$ when $\nu$ was
created. In Phase~2, augmentation does not change $\wbarclsnb(\kappa)$
and all facilities added to $\wbarN(\nu')$ are from the set
$\wtildeN(j)$ at the end of Phase 1, which is a subset of the set
$\wtildeN(j)$ after this iteration, since $\wtildeN(j)$ can only shrink. 
So the condition (SI'.\ref{SI1:primary disjoint}) will
remain true.

%%%%%%%%%%%%%%%

\begin{lemma} \label{lem: PD1: primary overlap}
  Property (PD'.\ref{PD1:assign:overlap}) holds.
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
  Let $j$ be the client for which $\nu\in j$. We consider an iteration
  when we create $\nu$ from $j$ and assign it to $\kappa$, and
  within this proof, notation $\wtildeclsnb(j)$ and $\wtildeN(j)$
  will refer to the value of the sets at this particular time.  
% if reviewers complain, we can introduce superscript v to indicate that
% as we do elsewhere
At this time, $\wbarN(\nu)$ is initialized to $\wtildeN(j)\cap
  \wbarN(\kappa)$.  Recall that $\wbarN(\kappa)$ is now equal to the
  final $\wbarclsnb(\kappa)$ (taking into account facility splitting). We
  would like to show that the set $\wtildeclsnb(j)\cap
  \wbarclsnb(\kappa)$ (which is not empty) will be included in
  $\wbarclsnb(\nu)$ at the end. Technically speaking, this will not be
  true due to facility splitting, so we need to rephrase this claim
  and the proof in terms of the set of facilities obtained after the
  algorithm completes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht]
\begin{center}
\includegraphics[width=3.2in]{proof_of_lemma_PD'3a.pdf}
\caption[Illustration of the sets in the proof of
Lemma~\ref{lem: PD1: primary overlap}]{Illustration of the
  sets $\wbarN(\nu)$, $A$, $B$, $E^-$ and $E^+$ in the proof
  of Lemma~\ref{lem: PD1: primary overlap}. Let $X \Subset
  Y$ mean that the facility sets $X$ is obtained from $Y$ by
  splitting facilities.  We then have $A \Subset
  \wtildeN(j)$, $B \Subset \wtildeclsnb(j) \cap
  \wbarclsnb(\kappa)$, $E^- \Subset \wtildeclsnb(j) -
  \wbarclsnb(\kappa)$, $E^+ \Subset \wtildeN(j) -
  \wtildeclsnb(j)$.}
\label{fig: sets lemma PD'3a}
\end{center}
\end{figure}

  We define the sets $A$, $B$, $E^-$ and $E^+$ as the subsets of
  $\facilityset$ (the final set of facilities) that were obtained from
  splitting facilities in the sets $\wtildeN(j)$, $\wtildeclsnb(j)\cap
  \wbarclsnb(\kappa)$, $\wtildeclsnb(j) - \wbarclsnb(\kappa)$ and
  $\wtildeN(j) - \wtildeclsnb(j)$, respectively.  (See
  Figure~\ref{fig: sets lemma PD'3a}.)  We claim that at the end
  $B\subseteq \wbarclsnb(\nu)$, with the caveat that the ties in the
  definition of $\wbarclsnb(\nu)$ are broken in favor of the
  facilities in $B$.  (This is the tie-breaking rule that we mentioned
  in the definition of $\wbarclsnb(\nu)$.)  This will be sufficient to
  prove the lemma because $B\neq\emptyset$, by the algorithm.

  We now prove this claim. In this paragraph $\wbarN(\nu)$ denotes the
  final set $\wbarN(\nu)$ after both phases are completed. Thus the total
connection value of $\wbarN(\nu)$ to $\nu$ is $1$.
	Note first that
  $B\subseteq \wbarN(\nu) \subseteq A$, because we never remove
  facilities from $\wbarN(\nu)$ and we only add facilities from
  $\wtildeN(j)$.  Also, $B\cup E^-$ represents the facilities obtained
  from $\wtildeclsnb(j)$, so $\sum_{\mu\in B\cup E^-} \bary_{\mu} =
  1/\gamma$.  This and $B\subseteq \wbarN(\nu)$ implies that the total
  connection value of $B\cup (\wbarN(\nu)\cap E^-)$ to $\nu$ is at
  most $1/\gamma$. But all facilities in $B\cup (\wbarN(\nu)\cap E^-)$
  are closer to $\nu$ (taking into account our tie breaking in property (NB))
 	than those in $E^+\cap \wbarN(\nu)$. It follows
  that $B\subseteq \wbarclsnb(\nu)$, completing the proof.
\end{proof}

%%%%%%%%%%%%%%

\begin{lemma}\label{lem: PD1: primary optimal}
  Property (PD'.\ref{PD:assign:cost}) holds.
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
This proof is similar to that for Lemma~\ref{lem: PD:assign:cost holds}.
For a client $j$ and demand $\eta$, we will write
$\tcccls^\eta(j)$ and $\dmaxcls^\eta(j)$ to denote the values of
$\tcccls(j)$ and $\dmaxcls(j)$ at the time when $\eta$
was created. (Here $\eta$ may or may not be a demand of client $j$).

Suppose $\nu \in j$ is assigned to a primary demand $\kappa \in p$.
By the way primary demands are constructed in the partitioning
algorithm, $\wtildeclsnb(p)$ becomes $\wbarN(\kappa)$, which is equal
to the final value of $\wbarclsnb(\kappa)$. So we have
$\clsdist(\kappa) = \tcccls^\kappa (p)$ and $\clsmax(\kappa) =
\dmaxcls^\kappa(p)$. Further, since we choose $p$ to minimize
$\tcccls(p) + \dmaxcls(p)$, we have that $\tcccls^\kappa(p) +
\dmaxcls^\kappa(p) \leq \tcccls^\kappa(j) + \dmaxcls^\kappa(j)$.

Using an argument analogous to that in the proof of Lemma~\ref{lem: tcc optimal}, 
our modified partitioning algorithm guarantees that
  $\tcccls^{\kappa}(j) \leq \tcccls^{\nu}(j) \leq \clsdist(\nu)$ and
  $\dmaxcls^{\kappa}(j) \leq \dmaxcls^{\nu}(j) \leq \clsmax(\nu)$ since $\nu$ was
  created later.
  Therefore, we have
%
  \begin{align*}
    \clsdist(\kappa) + \clsmax(\kappa) &= \tcccls^{\kappa}(p) +	\dmaxcls^{\kappa}(p) 
					\\
					&\leq \tcccls^{\kappa}(j) + \dmaxcls^{\kappa}(j) 
					\leq \tcccls^{\nu}(j) + \dmaxcls^{\nu}(j) 
					\leq \clsdist(\nu) + \clsmax(\nu),
  \end{align*}
%
completing the proof.
\end{proof}

%%%%%%%%

Now we have completed the proof that the computed partitioning satisfies
all the required properties. 


\paragraph{Algorithm~{\EBGS}.}
The complete algorithm starts with solving the LP(\ref{eqn:fac_primal}) and
computing the partitioning described earlier in this section.  Given
the partitioned fractional solution $(\barbfx, \barbfy)$ with the
desired properties, we start the process of opening facilities and
making connections to obtain an integral solution. To this end, for
each primary demand $\kappa\in P$, we open exactly one facility
$\phi(\kappa)$ in $\wbarclsnb(\kappa)$, where each
$\mu\in\wbarclsnb(\kappa)$ is chosen as $\phi(\kappa)$ with
probability $\gamma\bary_{\mu}$. For all facilities
$\mu\in\facilityset - \bigcup_{\kappa\in P}\wbarclsnb(\kappa)$, we
open them independently, each with probability
$\gamma\bary_{\mu}$. 

We claim that all probabilities are well-defined, that is
$\gamma\bary_{\mu} \le 1$ for all $\mu$. Indeed, if $\bary_{\mu}>0$ then
$\bary_{\mu} = \barx_{\mu\nu}$ for some $\nu$, by Property~(CO).
If $\mu\in \wbarclsnb(\nu)$ then the definition of close
neighborhoods implies that $\barx_{\mu\nu} \le 1/\gamma$.
If $\mu\in \wbarfarnb(\nu)$ then
$\barx_{\mu\nu} \le 1-1/\gamma \le 1/\gamma$, because $\gamma < 2$.
Thus $\gamma\bary_{\mu} \le 1$, as claimed.

Next, we connect demands to facilities.  Each primary demand
$\kappa\in P$ will connect to the only open facility $\phi(\kappa)$ in
$\wbarclsnb(\kappa)$.  For each non-primary demand $\nu\in \demandset
- P$, if there is an open facility in $\wbarclsnb(\nu)$ then we
connect $\nu$ to the nearest such facility. Otherwise, we connect
$\nu$ to the nearest far facility in $\wbarfarnb(\nu)$ if one is
open. Otherwise, we connect $\nu$ to its \emph{target facility}
$\phi(\kappa)$, where $\kappa$ is the primary demand that $\nu$ is
assigned to.

%%%%%%%%%%%

\paragraph{Analysis.}
By the algorithm, for each client $j$, all its $r_j$ demands are connected to
open facilities. If two different siblings $\nu,\nu'\in j$ are assigned, respectively,
to primary demands $\kappa$, $\kappa'$ then, by
Properties~(SI'.\ref{SI1:siblings disjoint}), (SI'.\ref{SI1:primary
  disjoint}), and (PD'.\ref{PD1:disjoint}) we have
%
\begin{equation*}
( \wbarN(\nu) \cup \wbarclsnb(\kappa)) \cap (\wbarN(\nu')\cup \wbarclsnb(\kappa')) = \emptyset.
\end{equation*}
%
This condition guarantees that $\nu$ and $\nu'$ are assigned to different facilities,
regardless whether they are connected to a neighbor facility or to its target facility.
Therefore the computed solution is feasible.

\medskip

We now estimate the cost of the solution computed by Algorithm {\EBGS}. The lemma
below bounds the expected facility cost.

%%%%%%%%%%

\begin{lemma} \label{lem: EBGS facility cost}
The expectation of facility cost $F_{\smallEBGS}$ of Algorithm~{\EBGS} is at most $\gamma F^\ast$.
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
By the algorithm, each facility $\mu\in \facilityset$ is opened with
probability $\gamma \bary_{\mu}$, independently of whether it belongs to the
close neighborhood of a primary demand or not. Therefore, by
  linearity of expectation, we have that the expected facility cost is
%
\begin{equation*}
	\Exp[F_{\smallEBGS}] = \sum_{\mu \in \facilityset} f_\mu \gamma \bary_{\mu} 
			= \gamma \sum_{i\in \sitesset} f_i \sum_{\mu\in i} \bary_{\mu} 
			= \gamma \sum_{i \in \sitesset} f_i y_i^\ast = \gamma F^\ast,
\end{equation*}
%
where the third equality follows from (PS.\ref{PS:yi}).
\end{proof}

%%%%%%%%%%%

\medskip

In the remainder of this section we focus on the connection cost. Let $C_{\nu}$ be the
random variable representing the connection cost of a demand $\nu$. Our objective is
to show that the expectation of $\nu$ satisfies
%
\begin{equation}
\Exp[C_\nu]	\leq \concost(\nu) \cdot \max\left\{\frac{1/e+1/e^\gamma}{1-1/\gamma}, 1 + \frac{2}{e^\gamma}\right\}.
		\label{eqn: expectation of C_nu for EBGS}
\end{equation}
%
If $\nu$ is a primary demand then, due to the algorithm, we have $\Exp[C_{\nu}] =
\clsdist(\nu) \le \concost(\nu)$, so (\ref{eqn: expectation of C_nu for EBGS}) is
easily satisfied.

Thus for the rest of the argument we will focus on the case when $\nu$
is a non-primary demand.  Recall that the
algorithm connects $\nu$ to the nearest open facility in
$\wbarclsnb(\nu)$ if at least one facility in $\wbarclsnb(\nu)$ is
open. Otherwise the algorithm connects $\nu$ to the nearest open
facility in $\wbarfarnb(\nu)$, if any. In the event that no facility in
$\wbarN(\nu)$ opens, the algorithm will connect $\nu$ to its target
facility $\phi(\kappa)$, where $\kappa$ is the primary demand that
$\nu$ was assigned to, and $\phi(\kappa)$ is the only facility open in
$\wbarclsnb(\kappa)$. Let $\Lambda^\nu$ denote the event that at least
one facility in $\wbarN(\nu)$ is open and $\Lambda^\nu_{\cls}$ be the
event that at least one facility in $\wbarclsnb(\nu)$ is open.
$\neg \Lambda^\nu$ denotes the complement event of $\Lambda^\nu$, that is,
the event that none of $\nu$'s neighbors opens. 
We want to estimate the following three conditional expectations: 
%
\begin{equation*}
  \Exp[C_{\nu} \mid
  \Lambda^\nu_{\cls}],\quad \Exp[C_{\nu} \mid \Lambda^\nu \wedge \neg
  \Lambda^\nu_{\cls}], \quad\text{and}\quad \Exp[C_{\nu} \mid \neg \Lambda^\nu], 
\end{equation*}
%
and their associated probabilities.

We start with a lemma dealing with the third expectation,
$\Exp[C_\nu\mid\neg \Lambda^{\nu}] = \Exp[d_{\phi(\kappa)\nu} \mid
\Lambda^{\nu}]$. The proof of this lemma relies on
Properties~(PD'.\ref{PD1:assign:overlap}) and
(PD'.\ref{PD1:assign:cost}) of modified partitioning and follows the
reasoning in the proof of a similar lemma
in~\cite{ByrkaGS10,ByrkaA10}.

%%%%%%%

\begin{lemma}\label{lem: EBGS target connection cost}
Assuming that no facility in $\wbarN(\nu)$ opens, the expected connection
cost of $\nu$ is
%
\begin{equation}
  \Exp[C_{\nu} \mid \neg \Lambda^{\nu}] \leq
  \clsdist(\nu) + 2\fardist(\nu).
  \label{eqn: expected connection cost target facility}
\end{equation}
%
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
It suffices to show a stronger inequality
\begin{equation}
  \Exp[C_{\nu} \mid \neg \Lambda^{\nu}] \leq
  \clsdist(\nu) + \clsmax(\nu) + \fardist(\nu)
			\label{eqn: lemma ebgs indirect connection cost},
\end{equation}
which then implies (\ref{eqn: expected connection cost
  target facility}) because $\clsmax(\nu) \leq
\fardist(\nu)$.  The proof of (\ref{eqn: lemma ebgs indirect
  connection cost}) is similar to that in
\cite{ByrkaA10}. For the sake of completeness, we provide it
here, formulated in our terminology and notation.

Assume that the event $\neg \Lambda^{\nu}$ is true, that is Algorithm~{\EBGS}
does not open any facility in $\wbarN(\nu)$.
Let $\kappa$ be the primary demand that $\nu$ was assigned to. Also let
%K
\begin{equation*}
K = \wbarclsnb(\kappa) \setminus \wbarN(\nu), \quad
V_{\cls} = \wbarclsnb(\kappa) \cap \wbarclsnb(\nu) \quad \textrm{and}\quad 
V_{\far} = \wbarclsnb(\kappa) \cap \wbarfarnb(\nu).
\end{equation*}
% 
Then $K, V_{\cls}, V_{\far}$ form a partition of
$\wbarclsnb(\kappa)$, that is, they are disjoint and their union is $\wbarclsnb(\kappa)$.
Moreover, we have that $K$ is not empty, because Algorithm~{\EBGS}
opens some facility in $\wbarclsnb(\kappa)$ and this facility cannot be in $V_{\cls}\cup V_{\far}$,
by our assumption. 
We also have that $V_{\cls}$ is not empty due to (PD'.\ref{PD1:assign:overlap}). 

Recall that $D(A,\eta) = \sum_{\mu\in A}d_{\mu\eta}\bary_{\mu}/\sum_{\mu\in A}\bary_{\mu}$
is the average distance between a demand $\eta$ and the facilities in a set $A$. We shall show that
%
\begin{equation}
	 D(K, \nu) \leq \clsdist(\kappa)+\clsmax(\kappa) + \fardist(\nu).
				\label{eqn: bound on D(K,nu)}
\end{equation}
%
This is sufficient, because, by the algorithm, $D(K,\nu)$ is exactly 
the expected connection cost for demand $\nu$ conditioned on
the event that none of $\nu$'s neighbors 
opens, that is the left-hand side of (\ref{eqn: lemma ebgs indirect connection cost}).
Further, (PD'.\ref{PD1:assign:cost}) states that 
$\clsdist(\kappa)+\clsmax(\kappa) \le \clsdist(\nu) + \clsmax(\nu)$, and thus
(\ref{eqn: bound on D(K,nu)})  implies (\ref{eqn: lemma ebgs indirect connection cost}).

\medskip

The proof of (\ref{eqn: bound on D(K,nu)}) is by analysis of several cases.
%

\medskip
\noindent
{\mycase{1}} $D(K, \kappa) \leq \clsdist(\kappa)$. For any
facility $\mu \in V_{\cls}$ (recall that $V_{\cls}\neq\emptyset$), 
we have $d_{\mu\kappa} \leq \clsmax(\kappa)$ 
and $d_{\mu\nu} \leq \clsmax(\nu) \leq \fardist(\nu)$. Therefore, using the
case assumption, we get
	$D(K,\nu) \leq D(K,\kappa) + d_{\mu\kappa} + d_{\mu\nu} 
				\leq \clsdist(\kappa) + \clsmax(\kappa) + \fardist(\nu)$.

\medskip
\noindent
{\mycase{2}} There exists a facility $\mu\in V_{\cls}$ such that
  $d_{\mu\kappa} \leq \clsdist(\kappa)$. Since $\mu\in V_{\cls}$, we infer
  that $d_{\mu\nu} \leq \clsmax(\nu) \leq \fardist(\nu)$.  Using
  $\clsmax(\kappa)$ to bound $D(K, \kappa)$, we have $D(K, \nu)
  \leq D(K, \kappa) + d_{\mu\kappa} + d_{\mu\nu} \leq
  \clsmax(\kappa) + \clsdist(\kappa) + \fardist(\nu)$.

\medskip
\noindent
{\mycase{3}} In this case we assume that neither of Cases~1 and 2 applies, that is
 $D(K, \kappa) > \clsdist(\kappa)$ and every $\mu \in V_{\cls}$ satisfies
 $d_{\mu\kappa} >  \clsdist(\kappa)$. This implies that
$D(K\cup V_{\cls}, \kappa) > \clsdist(\kappa) = D(\wbarclsnb(\kappa), \kappa)$.
Since sets $K$, $V_{\cls}$ and $V_{\far}$ form a partition of $\wbarclsnb(\kappa)$,
we obtain that in this case $V_{\far}$ is not
empty and $D(V_{\far}, \kappa) < \clsdist(\kappa)$. 
Let $\delta = \clsdist(\kappa) - D(V_{\far}, \kappa) > 0$. 
We now have two sub-cases:
%
\begin{description}
	
\item{\mycase{3.1}} {$D(V_{\far}, \nu) \leq \fardist(\nu) + \delta$}.
  Substituting $\delta$, this implies that $D(V_{\far}, \nu) +
  D(V_{\far},\kappa) \le \clsdist(\kappa) + \fardist(\nu)$.  From the
  definition of the average distance $D(V_{\far},\kappa)$ and
  $D(V_{\far}, \nu)$, we obtain that there exists some $\mu \in
  V_{\far}$ such that $d_{\mu\kappa} + d_{\mu\nu} \leq
  \clsdist(\kappa) + \fardist(\nu)$.  Thus $D(K, \nu) \leq D(K,
  \kappa) + d_{\mu\kappa} + d_{\mu\nu} \leq \clsmax(\kappa) +
  \clsdist(\kappa) + \fardist(\nu)$.

\item{\mycase{3.2}} {$D(V_{\far}, \nu) > \fardist(\nu) + \delta$}.
  The case assumption implies that $V_{\far}$ is a proper subset of
  $\wbarfarnb(\nu)$, that is $\wbarfarnb(\nu) \setminus V_{\far}
  \neq\emptyset$.  Let $\hat{y} = \gamma \sum_{\mu\in V_{\smallfar}}
  \bary_{\mu}$.  We can express $\fardist(\nu)$ using $\hat{y}$ as
  follows
%
\begin{equation*}
\fardist(\nu) = D(V_{\far},\nu) \frac{\hat{y}}{\gamma-1} +
    D(\wbarfarnb(\nu)\setminus V_{\far}, \nu) \frac{\gamma-1-\hat{y}}{\gamma-1}.
\end{equation*}
%
Then, using the case condition and simple algebra, we have
%
  \begin{align}
    \clsmax(\nu) &\leq D(\wbarfarnb(\nu) \setminus V_{\far}, \nu) 
			\notag
		\\
		&\leq \fardist(\nu) - \frac{\hat{y}\delta}{\gamma-1-\hat{y}} 
		\leq \fardist(\nu) - \frac{\hat{y}\delta}{1-\hat{y}},
			\label{eqn: case 3, bound on C_cls^max(nu)}
  \end{align}
%
where the last step follows from $1 < \gamma < 2$. 

On the other hand, since $K$, $V_{\cls}$, and $V_{\far}$ form a partition of $\wbarclsnb(\kappa)$,
we have
$\clsdist(\kappa) = (1-\hat{y}) D(K\cup V_{\cls}, \kappa) + \hat{y} D(V_{\far}, \kappa)$.
Then using the definition of $\delta$ we obtain
%
\begin{equation}
    D(K \cup V_{\cls}, \kappa) = \clsdist(\kappa) + \frac{\hat{y}\delta}{1-\hat{y}}.
				\label{eqn: formula for D(V_cls,kappa)}
\end{equation}
%
  Now we are essentially done. If there exists some $\mu \in V_{\cls}$ such
  that $d_{\mu\kappa} \leq \clsdist(\kappa) +
  \hat{y}\delta/(1-\hat{y})$, then	we have
%
  \begin{align*}
    D(K, \nu) &\leq D(K, \kappa) + d_{\mu\kappa} + d_{\mu\nu} \\
    &\leq \clsmax(\kappa) + \clsdist(\kappa) +
    			\frac{\hat{y}\delta}{1-\hat{y}}
    + \clsmax(\nu)\\
    &\leq \clsmax(\kappa) + \clsdist(\kappa) + \fardist(\nu),
  \end{align*}
%
where we used (\ref{eqn: case 3, bound on C_cls^max(nu)}) in the last step.
  Otherwise, from (\ref{eqn: formula for D(V_cls,kappa)}),
we must have $D(K, \kappa) \leq \clsdist(\kappa) +
  \hat{y}\delta/(1-\hat{y})$. Choosing any $\mu \in V_{\cls}$, it follows that
%
  \begin{align*}
    D(K, \nu) &\leq D(K, \kappa) + d_{\mu\kappa} + d_{\mu\nu} \\
    &\leq \clsdist(\kappa) + \frac{\hat{y}\delta}{1-\hat{y}} +
    		\clsmax(\kappa)  + \clsmax(\nu)\\
    &\leq \clsdist(\kappa) + \clsmax(\kappa) + \fardist(\nu),
  \end{align*}
%
again using (\ref{eqn: case 3, bound on C_cls^max(nu)}) in the last step.

\end{description}

This concludes the proof of (\ref{eqn: expected connection
  cost target facility}).  As explained earlier,
Lemma~\ref{lem: EBGS target connection cost} follows.
\end{proof}

Next, we derive some estimates for the expected cost of direct
connections.  The next technical lemma is a generalization of
Lemma~\ref{lem: echs expected C_nu}. In Lemma~\ref{lem: echs expected
  C_nu} we bound the expected distance to the closest open facility in
$\wbarN(\nu)$, conditioned on at least one facility in $\wbarN(\nu)$
being open. The lemma below provides a similar estimate for an
arbitrary set $A$ of facilities in $\wbarN(\nu)$, conditioned on that
at least one facility in set $A$ is open.  Recall that $D(A,\nu) =
\sum_{\mu \in A} d_{\mu\nu} \bary_{\mu} / \sum_{\mu \in A}
\bary_{\mu}$ is the average distance from $\nu$ to a facility in $A$. 

%%%%%%%

\begin{lemma}\label{lem: expected distance in EBGS}
  For any non-empty set $A\subseteq \wbarN(\nu)$, let $\Lambda^\nu_A$ be
  the event that at least one facility in $A$ is opened by Algorithm
  {\EBGS}, and denote by $C_\nu(A)$ the random variable representing
  the distance from $\nu$ to the closest open facility in $A$.  Then
  the expected distance from $\nu$ to the nearest open facility in
  $A$, conditioned on at least one facility in $A$ being opened, is
%
\begin{equation*}
	\Exp[C_\nu(A) \mid \Lambda^\nu_A ] \le D(A,\nu).
\end{equation*}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
  The proof follows the same reasoning as the proof of Lemma~\ref{lem:
    echs expected C_nu}, so we only sketch it here. We start with a
  similar grouping of facilities in $A$: for each primary demand
  $\kappa$, if $\wbarclsnb(\kappa)\cap A\neq\emptyset$ then
  $\wbarclsnb(\kappa)\cap A$ forms a group. Facilities in $A$ that are
  not in a neighborhood of any primary demand form singleton groups.
  We denote these groups $G_1,...,G_k$. It is clear that the groups
  are disjoint because of (PD'.\ref{PD1:disjoint}). Denoting by
  $\bard_s = D(G_s, \nu)$ the average distance from $\nu$ to a group $G_s$, we
  can assume that these groups are ordered so that $\bard_1\le ... \le
  \bard_k$.

  Each group can have at most one facility open and the events
  representing opening of any two facilities that belong to different
  groups are independent. To estimate the distance from $\nu$ to the
  nearest open facility in $A$, we use an alternative
  random process to make connections, that is easier to
  analyze. Instead of connecting $\nu$ to the nearest open facility in
  $A$, we will choose the smallest $s$ for which $G_s$ has an open
  facility and connect $\nu$ to this facility. (Thus we selected an
  open facility with respect to the minimum $\bard_s$, not the actual
  distance from $\nu$ to this facility.)  This can only increase the
  expected connection cost, thus denoting $g_s = \sum_{\mu\in G_s}
  \gamma\bary_\mu$ for all $s=1,\ldots,k$, and letting $\Prob[\Lambda^\nu_A]$
  be the probability that $A$ has at least one facility open, we have
%
\begin{align}
    \Exp[C_\nu(A) \mid \Lambda^\nu_A] &\leq \frac{1}{\Prob[\Lambda^\nu_A]} (\bard_1 g_1 +
    \bard_2 g_2 (1 - g_1) + \ldots + \bard_k  g_k(1 -
    g_1)\ldots(1-g_{k-1}))
    \label{eqn: dist set to nu 1}
    \\
    &\leq \frac{1}{\Prob[\Lambda^\nu_A]} \frac{\sum_{s=1}^k \bard_s
      g_s}{\sum_{s=1}^k  g_s} (1 - \prod_{s=1}^k (1 -  g_s))
    \label{eqn: dist set to nu 2}
    \\
    \notag
    &= \frac{\sum_{s=1}^k \bard_s g_s}{\sum_{s=1}^k g_s} =
    \frac{\sum_{\mu \in A} d_{\mu\nu} \gamma \bary_{\mu}}{\sum_{\mu
        \in A} \gamma \bary_{\mu}}
    \\
    \notag
    &= \frac{\sum_{s=1}^k d_{\mu\nu} \bary_{\mu}}{\sum_{\mu \in A}
      \bary_{\mu}} = D(A, \nu).
\end{align}
%
Inequality (\ref{eqn: dist set to nu 2}) follows from inequality
(\ref{eq:min expected distance}) in~\ref{sec: ECHSinequality}. The rest of the
derivation follows from $\Prob[\Lambda^\nu_A] = 1 - \prod_{s=1}^k (1 -
g_s)$, and the definition of $\bard_s$, $g_s$ and $D(A,\nu)$.
\end{proof}

A consequence of Lemma~\ref{lem: expected distance in EBGS} is the
following corollary which bounds the other two expectations
of $C_\nu$, when at least one facility is opened in $\wbarclsnb(\nu)$,
and when no facility in $\wbarclsnb(\nu)$ opens but a facility in
$\wbarfarnb(\nu)$ is opened.

%%%%%%%%%

\begin{corollary} \label{coro: EBGS close and far distance} 
%
{\rm (a)} $\Exp[C_{\nu} \mid \Lambda_{\cls}^\nu] \leq \clsdist(\nu)$,
and
{\rm (b)} $\Exp[C_{\nu} \mid \Lambda^\nu \wedge \neg \Lambda_{\cls}^\nu]
    			\leq \fardist(\nu)$.
\end{corollary}
%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
When there is an open facility in $\wbarclsnb(\nu)$, the algorithm
  connect $\nu$ to the nearest open facility in
  $\wbarclsnb(\nu)$. When no facility in $\wbarclsnb(\nu)$ opens but
  some facility in $\wbarfarnb(\nu)$ opens, the algorithm connects
  $\nu$ to the nearest open facility in $\wbarfarnb(\nu)$. The rest of
  the proof follows from Lemma~\ref{lem: expected distance in
    EBGS}. By setting the set $A$ in Lemma~\ref{lem: expected distance
    in EBGS} to $\wbarclsnb(\nu)$, we have
%
  \begin{equation*}
    \Exp[C_{\nu} \mid \Lambda_{\cls}^\nu] \leq D(\wbarclsnb(\nu), \nu),
    = \clsdist(\nu),
    \label{eqn: expected connection cost close facility}
  \end{equation*}
% 
proving part (a), and by setting the set $A$ to $\wbarfarnb(\nu)$, we have
%
  \begin{equation*}
    \Exp[C_{\nu}
    \mid \Lambda^\nu \wedge \neg \Lambda_{\cls}^\nu] \leq
    D(\wbarfarnb(\nu), \nu) = \fardist(\nu),
    \label{eqn: expected connection cost far facility}
  \end{equation*}
which proves part (b).
\end{proof}

Given the estimate on the three expected distances when $\nu$ connects
to its close facility in $\wbarclsnb(\nu)$ in (\ref{eqn: expected
  connection cost close facility}), or its far facility in
$\wbarfarnb(\nu)$ in (\ref{eqn: expected connection cost far
  facility}), or its target facility $\phi(\kappa)$ in (\ref{eqn:
  expected connection cost target facility}), the only missing pieces
are estimates on the corresponding probabilities of each event, which
we do in the next lemma. Once done, we shall put all pieces together
and proving the desired inequality on $\Exp[C_{\nu}]$, that is
(\ref{eqn: expectation of C_nu for EBGS}).

The next Lemma bounds the probabilities for events
that no facilities in $\wbarclsnb(\nu)$ and $\wbarN(\nu)$ are
opened by the algorithm.

%%%%%%%%%%%

\begin{lemma}\label{lem: close and far neighbor probability}
{\rm (a)} $\Prob[\neg\Lambda^\nu_{\cls}] \le 1/e$, and
{\rm (b)} $\Prob[\neg\Lambda^\nu] \le 1/e^\gamma$.
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
  (a) To estimate $\Prob[\neg\Lambda^\nu_{\cls}]$, we again consider a
  grouping of facilities in $\wbarclsnb(\nu)$, as in the proof of
  Lemma~\ref{lem: expected distance in EBGS}, according to the primary
  demand's close neighborhood that they fall in, with facilities not
  belonging to such neighborhoods forming their own singleton groups.
  As before, the groups are denoted $G_1, \ldots, G_k$. It is easy to
  see that $\sum_{s=1}^k g_s = \sum_{\mu \in \wbarclsnb(\nu)} \gamma
  \bary_{\mu} = 1$. For any group $G_s$, the probability that a
  facility in this group opens is $\sum_{\mu \in G_s} \gamma
  \bary_{\mu} = g_s$ because in the algorithm at most one facility in
  a group can be chosen and each is chosen with probability $\gamma
  \bary_{\mu}$. Therefore the probability that no facility 
  opens is $\prod_{s=1}^k (1 - g_s)$, which is
  at most $e^{-\sum_{s=1}^k g_s} = 1/e$. Therefore we have
  $\Prob[\neg\Lambda^\nu_A] \leq 1/e$.

(b)
  This proof is similar to the proof of (a). The probability $\Prob[\neg\Lambda^\nu]$ is at most
  $e^{-\sum_{s=1}^k g_s} = 1/e^\gamma$, because we now have
  $\sum_{s=1}^k g_s = \gamma \sum_{\mu \in \wbarN(\nu)} \bary_{\mu} =
  \gamma \cdot 1 = \gamma$.
\end{proof}


We are now ready to bound the overall connection cost of
Algorithm~{\EBGS}, namely inequality (\ref{eqn: expectation of C_nu for EBGS}).

%%%%%%%

\begin{lemma}\label{lem: EBGS nu's connection cost}
The expected connection of $\nu$ is
%
\begin{equation*}
\Exp[C_\nu] \le
  \concost(\nu)\cdot\max\Big\{\frac{1/e+1/e^\gamma}{1-1/\gamma}, 1+\frac{2}{e^\gamma}\Big\}.
\end{equation*}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
  Recall that, to connect $\nu$, the algorithm uses the closest facility in
  $\wbarclsnb(\nu)$ if one is opened; otherwise it will try to connect $\nu$
  to the closest facility in $\wbarfarnb(\nu)$. Failing that, it will
  connect $\nu$ to $\phi(\kappa)$, the sole facility open in the
  neighborhood of $\kappa$, the primary demand $\nu$ was assigned
  to. Given that, we estimate $\Exp[C_\nu]$ as follows:
%
  \begin{align}
    \Exp[C_{\nu}] 
		\;&= \;\Exp[C_{\nu}\mid \Lambda^\nu_{\cls}] \cdot \Prob[\Lambda^\nu_{\cls}]	
				\;+\; \Exp[C_{\nu}\mid \Lambda^\nu\ \wedge\neg \Lambda^\nu_{\cls}] 
				\cdot \Prob[\Lambda^\nu\, \wedge\neg \Lambda^\nu_{\cls}]	
				\notag
		\\
		& \quad\quad\quad
				+ \; \Exp[C_{\nu}\mid \neg \Lambda^\nu] \cdot \Prob[\neg \Lambda^\nu]
				\notag
		\\
		&\leq \; \clsdist(\nu) \cdot \Prob[\Lambda^\nu_{\cls}]
			\;+\; \fardist(\nu)	
				\cdot \Prob[\Lambda^\nu\, \wedge\neg \Lambda^\nu_{\cls}]
                      \label{eqn: apply three expected dist}
						\\
                        &\quad\quad\quad
			+\; [\,\clsdist(\nu) + 2\fardist(\nu)\,] \cdot \Prob[\neg\Lambda^\nu]
		\notag
		\\
                &=\; [\,\clsdist(\nu) + \fardist(\nu)\,]\cdot \Prob[\neg\Lambda^\nu] 
						\;+\; 
							[\,\fardist(\nu)   -\clsdist(\nu)\,]
                                \cdot \Prob[\neg\Lambda^\nu_{\cls}]
                              \;+\;  \clsdist(\nu)
                                                        \notag
		\\
             &\leq\; [\,\clsdist(\nu) + \fardist(\nu)\,] \cdot \frac{1}{e^\gamma}
             \;+\; [\,\fardist(\nu) - \clsdist(\nu)\,] \cdot \frac{1}{e}
             \;+\; \clsdist(\nu)
             \label{eqn: probability estimate}
             \\
             \notag
             &=\; \Big(1 - \frac{1}{e} + \frac{1}{e^\gamma}\Big)\cdot \clsdist(\nu)
 				\;+\; \Big(\frac{1}{e} + \frac{1}{e^\gamma}\Big)\cdot\fardist(\nu).
\end{align}
%
Inequality (\ref{eqn: apply three expected dist}) follows from
Corollary~\ref{coro: EBGS close and far distance} and 
Lemma~\ref{lem: EBGS target connection cost}. 
Inequality (\ref{eqn: probability estimate}) follows from 
Lemma~\ref{lem: close and far neighbor probability} and
$\fardist(\nu) - \clsdist(\nu)\ge 0$.

Now define $\rho =\clsdist(\nu)/\concost(\nu)$. It is easy to
see that $\rho$ is between 0 and 1. Continuing the above
derivation, applying (\ref{eqn:avg dist cls dist far dist}), we get
%
\begin{align*}
\Exp[C_{\nu}]
             \;&\le\; \concost(\nu) 
			\cdot\left((1-\rho)\frac{1/e+1/e^\gamma}{1-1/\gamma} 
				+ \rho (1 + \frac{2}{e^\gamma})\right)
			\\
             &\leq \concost(\nu) 
				\cdot \max\left\{\frac{1/e+1/e^\gamma}{1-1/\gamma}, 1 + \frac{2}{e^\gamma}\right\},
\end{align*}
%
and the proof is now complete.
\end{proof}

With Lemma~\ref{lem: EBGS nu's connection cost} proven, we are now ready to bound our total connection cost.
For any client $j$ we have
%
\begin{align*}
\sum_{\nu\in j} C^{\avg}(\nu)
	&= \sum_{\nu\in j}\sum_{\mu\in\facilityset} d_{\mu\nu}\barx_{\mu\nu} 
	\\
	&= \sum_{i\in\sitesset}d_{ij}\sum_{\mu\in i}\sum_{\nu\in j} \barx_{\mu\nu}
	= \sum_{i\in\sitesset} d_{ij}x_{ij}^\ast = C_j^\ast.
\end{align*}
% 
Summing over all clients $j$ we obtain that the total expected connection cost is
%
\begin{equation*}
	\Exp[ C_{\smallEBGS} ] \le  C^\ast\max\left\{\frac{1/e+1/e^\gamma}{1-1/\gamma}, 1+\frac{2}{e^\gamma}\right\}.
\end{equation*}
%
Recall that the expected facility cost is bounded by $\gamma F^\ast$,
as argued earlier. Hence the total expected cost is bounded by $\max\{\gamma,
\frac{1/e+1/e^\gamma}{1-1/\gamma}, 1+\frac{2}{e^\gamma}\}\cdot
\LP^\ast$. Picking $\gamma=1.575$ we obtain the desired ratio.

%%%%%%

\begin{theorem}\label{thm:ebgs}
  Algorithm~{\EBGS} is a $1.575$-approximation algorithm for \FTFP.
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ch6 PRIMAL-DUAL RESULTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Primal-dual Algorithms} 
\label{ch: primal-dual} 

In this chapter we present the results of primal-dual
algorithms. Unlike the LP-rounding algorithms in
Chapter~\ref{ch: lp-rounding}, primal-dual algorithms do not
require solving the LP explicitly and are computationally
more efficient. Primal-dual algorithms work by making
simultaneous updates to a primal solution, which is
integral, and a dual solution, which may be fractional, and
eventually arriving at a feasible primal solution and a
feasible dual solution. It is then possible to compare the
primal solution's cost to the optimal value, since the
optimal value of the primal is lower bounded by the cost of
any feasible dual solution, assuming the primal problem is a
minimization problem.

We introduce a natural greedy algorithm and employ a
technique called \emph{dual-fitting}~\cite{JainMMSV03},
first introduced by Jain {\etal.}, to analyze the
approximation ratio. We also give an example showing
possible limitation of this analysis.  In Section~\ref{sec:
  greedy}, we review the related work by Jain {\etal}.  In
Section~\ref{sec: greedy_ratio}, we explain how a similar
algorithm can be used to solve {\FTFP} and derive the
approximation ratio for that algorithm, using a general
result from Wolsey~\cite{Wolsey88}~\footnote{The Wolsey's
  result and its applicability to the FTFP problem were
  pointed out by Neal Young.}. Lastly in Section~\ref{sec:
  greedy_example}, we provide an example to illustrate the
difference between {\UFL} and {\FTFP} when the dual-fitting
analysis is used to derive the approximation ratio.

\section{Dual-fitting and Greedy Algorithms}
\label{sec: greedy}

Jain {\etal}~\cite{JainMMSV03} analyzed a greedy algorithm
for {\UFL} that gives a ratio of $1.861$ using
dual-fitting. The algorithm works by repeatedly picking the
most cost-effective star until all clients are connected. A
star consists of a facility and a set of clients. The
cost-effectiveness, or average-cost, is the cost of the star
divided by the number of clients in that star. Clients in
the just-selected star are connected to the facility, and
the opening cost of that facility is then set to zero. The
greedy algorithm can be interpreted as an equivalent
algorithm that grows a dual solution and updates the
correponding primal solution. Each client $j$ is associated
with a dual variable $\alpha_j$, and $\alpha_j$ is fixed to
the average cost of the star when the client $j$ gets
connected. Clearly, the sum of $\alpha_j$ for all clients
$j$ is equal to the cost of the primal solution, which is
the cost to open facilities and the cost to make
connections. The next step is to find a suitable common
factor $\gamma$ to make the dual solution $\{\alpha_j /
\gamma\}$ feasible. For this purpose, Jain {\etal} derived
an upper bound on the objective function value of a series
of linear programs, which are used to capture the hardest
instance for the algorithm. These linear programs are called
the \emph{factor-revealing LP}, and the supremum of their
objective function value is equal to $\gamma$, which is also
the approximation ratio of the algorithm.

The greedy algorithm as stated is very flexible and can be
applied with only minor modifications to solve problems with
fault-tolerant requirements, for example, the {\FTFL}
problem and the {\FTFP} problem. However, it seems rather
difficult to generalize the analysis and we have to settle
for a much worse approximation ratio, except for some
special cases. In the literature, there is no published
result on the approximation ratio of the greedy algorithm
for {\FTFL}, although an $H_n$-approximation
ratio~\footnote{The term $H_n$ is the $n^{th}$ harmonic
  number, $H_n = 1 + 1/2 + 1/3 + \ldots + 1/n \approx \ln
  n$.} is not difficult to obtain. For the uniform demand
case, Swamy and Shmoys~\cite{SwamyS08} showed that the
$1.52$ ratio for {\UFL} can be generalized for {\FTFL} as
well. For the {\FTFP} problem studied in this thesis, the
uniform demand case is trivial, as it is nothing but a
{\UFL} problem. In the next two sections, we have the
following results: for the general demand case, we observe
that a logarithmic ratio comes as a direct consequence of
Wolsey's general result for Set Cover related
problems~\cite{Wolsey88}; we also present an example
illustrating the difficulty in obtaining
$O(1)$-approximation ratio for the {\FTFP} problem when
dual-fitting is used to obtain the approximation ratio.

\section{The Greedy algorithm with $O(\log n)$ Ratio}
\label{sec: greedy_ratio}

\subsection{The Greedy Algorithm}
In this section we show that the greedy algorithm which
repeatedly picking the best star --- the one with the
minimum average cost --- gives an approximation ratio of
$H_n \approx \ln(n)$, where $n=|\clientset|$ is the number
of clients. A star is a site $i$ and a subset of clients
$C'$. The cost of such a star $S$ is $c(S) = f_i +
\sum_{j\in C'} d_{ij}$, and the average cost of $S$ is $c(S)
/ |C'|$. Call a client $j$ fully-connected, or
\emph{exhausted} if $j$ has made $r_j$ connections. Let $U$
be the set of not fully-connected clients. While not all
clients are fully-connected, the algorithm picks a star
$S=(i,C')$, where $C' \subseteq U$, with the minimum average
cost, and opens one facility at the site $i$. Each client in
$C'$ then makes one more connection with the site $i$. The
algorithm terminates when all clients are
fully-connected. To see that the algorithm can be
implemented in polynomial time, we observe that once a star
becomes the best star, it remains the best until one or more
of its member clients become exhausted. To pick the best
star, we notice that although the number of possible stars
are exponential, the best star can be identified by looking
at each site, and considering the nearest $1,2,\ldots,k$
clients for some integer $k$. Thus, we can accomplish
multiple iterations in a single step and the number of steps
is polynomially bounded by $|\sitesset|\cdot|\clientset|$.

\subsection{The Analysis}
We now derive the approximation ratio of the greedy
algorithm just described. It is not difficult to adapt the
dual-fitting analysis for {\UFL} to {\FTFP} by associating
each client $j$ with values
$\alpha_j^1,\ldots,\alpha_j^{r_j}$, using
$\{\alpha_j^{r_j}\}$ as the dual solution, and looking for a
factor $\gamma$ to shrink the dual solution to be feasible,
though we have to settle for a much less satisfying ratio of
$H_n$. Instead, we present this result as a direct
consequence of Wolsey's more general result on Set Cover
related problems.

\paragraph{Wolsey's Result.}
The Wolsey's result shows that the greedy algorithm finds a
solution of approximation ratio $H_d$ for the following
general problem:

\begin{problem}[Wolsey's Problem]
  Given a universe $\calU$ of elements, a cost function $c:
  \calU \mapsto \mathbb{Z}^+$, and a function $f : 2^\calU
  \mapsto \mathbb{N}$ which is submodular~\footnote{A
    function $f$ is submodular if $f(S \cup \{b\}) -
    f(\{b\}) \geq f(S \cup \{a,b\}) - f (S \cup \{a\})$ for
    any $S \subset \calU$ and $a, b \in \calU$.}, and some
  integer $k$, we want to find a subset $\calS$ of $\calU$
  such that $f(\calS)$ is at least $k$, and we want the
  total cost minimized. That is,
\begin{equation*}
  \min_{\calS} \{ \sum_{s\in\calS} c_s \suchthat
  \calS\subseteq \calU, f(\calS) \geq k\}.
\end{equation*}
\end{problem}

The greedy algorithm starts with $\calS = \emptyset$ and
repeatedly chooses an element $s \in \calU$ and adds $s$ to
$\calS$, where $s$ maximizes $(f(\calS \cup \{s\}) -
f(\calS)) / c_s$. The greedy algorithm has an approximation
ratio of $H_d$, where $d$ is $\max\{f(\{s\}) - f(\emptyset)
\suchthat s \in \calU\}$, which is the largest possible
increase in $f$ resulting from adding a single element.

To apply Wolsey's result, we notice that the elements
correspond to the stars in our greedy algorithm, and the
universe $\calU$ is then the collection of all possible
stars. The submodular function $f$ maps a collection of
stars as $\calS$ to an integer that is the sum of
$\min\{p_j, r_j\}$ over all clients $j$, where $p_j$ is the
number of stars in $\calS$ that contain the client $j$, and
$r_j$ is the client $j$'s demand. The integer $k$ in
Wolsey's problem is then set to $\sum_{j} r_j$. Since each
client $j$ can contribute no more than $r_j$ in the function
$f$, having a total of $\sum_j r_j$ guarantees that every
client has its share exactly $r_j$.

For the {\FTFP} problem, the greedy algorithm in the earlier
section works precisely as Wolsey's greedy algorithm would ,
by picking the star with the minimum average cost. Since a
star can contain at most $n=|\clientset|$ clients and hence
could increase $f(\calS)$ by at most $n$, we get a ratio of
$H_n$ of the greedy algorithm for the {\FTFP} problem.

The $H_n$-approximation result is rather weak and is hardly
the best possible approximation ratio for the greedy
algorithm. It is worth mentioning that we do not even have
to use the triangle inequality in deriving the $H_n$
approximation ratio, although we are working on metric
{\FTFP}. On the other hand, similar attempts made by other
researchers to obtain a sub-logarithmic ratio for {\FTFL}
were not successful, as described in Section~\ref{sec:
  greedy}. Although {\FTFP} seems to be easier to
approximate than {\FTFL} when LP-rounding algorithms are
used, it seems the fault-tolerant requirement in both
problems presents a hurdle for primal-dual based
techniques. In the following section, we provide an example
illustrating some difficulty when adapting the dual-fitting
analysis to the {\FTFP} problem.

\section{Limitation of Dual-fitting for FTFP}
\label{sec: greedy_example}
For FTFP, the greedy algorithm that repeatedly picks the
best star until all clients become fully-connected can be
implemented in polynomial time. In Section~\ref{sec:
  greedy_ratio}, we showed that this algorithm is an
$H_n$-approximation where $n=|\mathcal C|$ is the number of
clients. Since the same greedy algorithm is shown to have an
$O(1)$-approximation ratio for UFL~\cite{MahdianMSV01}, a
natural question to ask is whether the greedy algorithm can
be shown to have an $O(1)$ approximation ratio. Here, we
give an example that hints at a negative answer.

We assume the greedy algorithm is analyzed using the
dual-fitting technique, which associates with every client
$j$ with a number $\alpha_j$, interpreted as a dual solution
to the LP~(\ref{eqn:fac_dual}). However, the dual solution
$\{\alpha_j\}$ in general may not be feasible. The
dual-fitting technique aims at finding a smallest possible
number $\gamma$ such that, after the dual solution
$\{\alpha_j\}$ is shrunk (divided) by $\gamma$, all dual
constraints are satisfied. That is
\begin{equation*}
\sum_{j\in \mathcal C} (\alpha_j/\gamma
- d_{ij})_+ \leq f_i \qquad \text{ for all } i\in \mathcal F. 
\end{equation*}
The number $\gamma$ is taken as the approximation ratio.

In the greedy algorithm, a star with minimum average cost is
picked at each iteration and each member client of that star
then gets one more connection. The dual value $\alpha_j$
associated with each client $j$ in the dual-fitting analysis
can be seen as a way to charge the cost to individual
clients. Although for UFL, charging each member client an
equal share works well, the same may not be true for
FTFP. In general, the dual-fitting analysis does have the
freedom of distributing the cost of $f_i$ into member
clients. Nonetheless, we assume that the cost of $f_i$ is
distributed among members only, and not to clients outside
this star, which we call the \emph{local charging}
assumption. Our second assumption is that the proposed dual
solution $\alpha_j$, is taken as the average of individual
$\alpha_j^l$ for each of the $l^{th}$ demand of client $j$,
with $l=1,\ldots,r_j$ --- that is, $\alpha_j =
\sum_{l=1}^{r_j} \alpha_j^l / r_j$. Suppose the $l^{th}$
demand of $j$ is satisfied while $j$ is in a star with a
facility at a site $i$, then $\alpha_j^l = d_{ij} +
f_i^{j,l}$, where $f_i^{j,l}$ is the portion of $f_i$
attributed to $j$ in the analysis. Taking the average
implies the computed $\alpha_j$ values make
$\sum_{j\in\clientset} r_j \alpha_j$ equal to the cost of
the integral solution produced by the greedy algorithm.

%%%%%%%%%%%%%%%%%%% start figure %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
  \centering
  \begin{tikzpicture}[auto,scale=0.8, every node/.style={scale=0.8}]
    \node[draw,rectangle,minimum size=.7cm] (fac) at (-4,0) {};
    \node at (-5,0) {$f_1$};
    
    \node[draw,ellipse,minimum width=4cm,minimum
    height=1.8cm] (client1) at
    (6,0) {$n_1 = k^{k-1}, r_1$};
    \node[draw,ellipse,minimum width=4cm,minimum
    height=1.8cm] (client2) at
    (6,-3) {$n_2 = k^{k-2}, r_2$};
    \node[draw,ellipse,minimum width=4cm,minimum
    height=1.8cm] (client3) at
    (6,-6) {$n_3 = k^{k-3}, r_3$};
    \node[draw,ellipse,minimum width=4cm,minimum
    height=1.8cm] (clientk) at
    (6,-12) {$n_k = 1, r_k$};
    
    \node at (-3,-10) {\large{demands $r_1 \ll r_2 \ll \ldots \ll r_k$}};

    \draw (fac) to node {$d_1=0$} (client1);
    \draw[bend right] (fac) to node {$d_2 = d_1 + f_1 /
      n_2$}  (client2);
    \draw[bend right] (fac) to node {$d_3 = d_2 + f_1 /
      n_3$}  (client3);
    \draw[bend right] (fac) to node {$d_k = d_{k-1} + f_1 / n_k$}  (clientk);
  \end{tikzpicture}
  \caption[$\Omega(\log n / \log\log n)$ example for
  dual-fitting on {\FTFP}]{An example showing the greedy
    algorithm for FTFP, analyzed using dual-fitting, could
    give a ratio of $\Omega(\log n / \log\log n)$, assuming
    that the facility cost can only be charged to clients
    within the star.}
  \label{fig:greedy_lower_bound}
\end{figure}
%%%%%%%%%%%%%%%%% end figure %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now give our example in
Figure~\ref{fig:greedy_lower_bound}. Our example has one
site and $k$ groups of clients. Opening one facility at that
site costs $f_1$. The first group has $n_1$ clients each
with a demand of $r_1$, all at distance $d_1 = 0$ from
$f_1$. The distances from the groups to the site are listed
below:
\begin{align*}
  &d_1 = 0\\
  &d_2 = \frac{f_1}{n_1}\\
  &d_3 = f_1/n_2 + d_2 = f_1/n_2 + f_1/n_1 = f_1 (\frac{1}{n_2} + \frac{1}{n_1})\\
  &\ldots\\
  &d_k = f_1/n_{k-1} + d_{k-1} = f_1 (\frac{1}{n_{k-1}} + \ldots + \frac{1}{n_1})
\end{align*}
For the numbers, we need $r_1 \ll r_2 \ll \ldots \ll r_k$,
and $n_1 = u^{k-1}, n_2 = u^{k-2}, \ldots, n_k = u^0 = 1$
for some number $u$ --- we actually take $u=k$; this may not
be the best possible choice.

Call a star with facility cost zero \emph{trivial}. It is
\emph{non-trivial} if the facility has non-zero cost. Now
the greedy algorithm executes like this: the first
non-trivial star with $r_1$ replica is $(f_1, n_1)$. Then we
have a trivial star of zero cost facility and all $n_2$
clients in group $2$ for $r_1$ replica. The second
non-trivial star with $r_2 - r_1$ replica is $(f_1,
n_2)$. Notice that the $r_1$ replica of trivial stars with
group $2$ satisfies the first $r_1$ demand of the $n_2$
clients in that group. After that, the $n_2$ clients of
group $2$ each has a residual demand of $r_2 - r_1 \approx
r_2$ since $r_2 \gg r_1$, and are then satisfied by the $r_2
- r_1$ replica of the $(f_1, n_2)$ stars. The process
repeats until the $k^{th}$ group finishes with $r_k$ new
facilities.

According to our local charging assumption, we have
$\alpha_1 = f_1$, with $\alpha_1$ defined as the total dual
value of the $n_1$ clients in group $1$, regardless of how
the analysis would distribute the facility cost within that
group. Similarly, $\alpha_2 = f_1 + n_2 d_2$, and so
on. Substituting in the numbers, we have

\begin{align*}
  &\alpha_1 = f_1\\
  &\alpha_2 = f_1 + n_2 d_2 = f_1 + f_1/n_1\cdot n_2 = f_1 (1 + n_2 /
  n_1)\\
  &\alpha_3 = f_1 + n_3 d_3 = f_1 + f_1 (\frac{1}{n_2} +
  \frac{1}{n_1}) n_3 = f_1 (1 + \frac{n_3}{n_2} + \frac{n_3}{n_1})\\
  &\ldots\\
  &\alpha_k = f_1 + n_k d_k = f_1 + f_1 n_k (\frac{1}{n_{k-1}} + \ldots
  \frac{1}{n_1})
\end{align*}
Notice that $r_1 \ll r_2 \ll \ldots \ll r_k$ implies that
$\alpha_j$ is decided by the max among $\alpha_j^l$, so in
the following calculation we ignore the terms involving
trivial stars.

Now going back to the dual constraint --- it requires that
the shrinking factor $\gamma$ needs to satisfy the following
inequality:
\begin{equation}
  \frac{\alpha_1}{\gamma} - d_1 + \frac{\alpha_2}{\gamma} - d_2 +
  \ldots + \frac{\alpha_k}{\gamma} - d_k \leq f_1.
\end{equation}
Substituting in the $\alpha_j$ values derived above, we have
\begin{align*}
  \gamma &\geq (\sum_{j=1}^k \alpha_j) / (f_1 + \sum_{j=1}^k d_j)\\
  &\geq \frac{f_1 + n_1 d_1 + f_1 + n_2 d_2 + f_1 + n_3 d_3 + \ldots +
    f_1 + n_k
    d_k}{f_1 + n_1 d_1 + n_2 d_2 + \ldots + n_k d_k}\\
  &= 1 + (k-1)f_1 / (f_1 + n_1 d_1 + n_2 d_2 + \ldots + n_k d_k)\\
  &= 1 + (k-1)f_1 / \left(f_1 + n_2 f_1 / n_1 + \ldots + n_k f_1
    (\frac{1}{n_{k-1}} + \frac{1}{n_{k-2}} + \ldots +
    \frac{1}{n_1})\right)\\
  &= 1 + (k-1) / \left(1 + n_2 / n_1 + \ldots + n_k
    (\frac{1}{n_{k-1}} + \frac{1}{n_{k-2}} + \ldots +
    \frac{1}{n_1})\right)\\
  &= 1 + (k-1) / \left(1 + 1/u + \ldots + (\frac{1}{u} + \ldots +
    \frac{1}{u^{k-1}})\right)\\
  &= 1 + (k-1) / \left(1 + k/u + (k-1)/u^2 + \ldots +
    1/u^{k-1}\right)\\
  &\geq 1 + (k-1) / \left(1 + k/u + k/u^2 + \ldots +
    k/u^{k-1}\right)\\
  &= 1 + (k-1) / \left(1 + 1 + 1/k + \ldots + 1/k^{k-2}\right)\\
  &\approx k/2
\end{align*}
So for $k$ groups we can force a shrinking factor $\gamma$
as big as $k/2$. Recall that we have the greedy algorithm
being no more than $H_n$-approximation. Is that a
contradiction? No, because we have the number of clients
$n=k^{k-1} + k^{k-1} + \ldots + 1 = k^k$, so $k = O(\log n /
\log\log n)$. Therefore, the example shows that dual-fitting
with local-charging cannot hope to get a ratio better than
$O(\log n / \log\log n)$. Notice this example is similar in
spirit to the $\Omega(\log n/ \log\log n)$ example for
Hochbaum's algorithm for UFL, constructed by Mahdian {\etal}
~\cite{JainMMSV03}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ch7 CONCLUSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion} \label{ch: conclusion} 

In this thesis we have studied the Fault-Tolerant Facility
Placement problem ({\FTFP}), a generalization of the
well-known Uncapacitated Facility Location problem
({\UFL}). We showed that the known LP-rounding algorithms
for {\UFL} can be adapted to {\FTFP} while preserving the
approximation ratio. To accomplish this reduction, we
developed two techniques, namely demand reduction and
adaptive partition, which could be of more general
interest. Our results show that {\FTFP} seems easier in
terms of approximation, compared to {\FTFL}.

We also studied the primal-dual and dual-fitting approaches,
and provided a possible explanation of the difficulty in
obtaining a constant approximation ratio using those
techniques.

We hope our work in this dissertation helps other
researchers interested in the fault-tolerant variants of the
facility location problems to develop more insight into the
difficulty, as well as possible solutions when clients
demand more than one facility and we still need to keep the
total cost under control.

In anticipating future research, we agree with the authors,
Byrka {\etal}~\cite{ByrkaSS10}, who remarked that both
{\UFL} and {\FTFL} are likely to have approximation
algorithms with a ratio matching the $1.463$ lower
bound. From our demand reduction technique, it is almost
sure that {\FTFP} shall have a $1.463$-approximation
algorithm, provided that {\FTFL} can be approximated with a
ratio to meet the lower bound.

\bibliographystyle{plain}
\bibliography{facility}

\appendix
\chapter{Technical Background}

\section{Linear Programming and Integer Programming}
\label{sec: ILP}

In this section we give a short introduction to Linear
Programming and Integer Programming with an emphasis on
their application to the design and analysis of
approximation algorithms for optimization problems.

\subsection{Optimization and Integer Programming}
\label{subsec: Optimization_IP}
Most optimization problems have a natural integer program in
which we use variables to describe the solution that we
seek, and write the constraints imposed by the feasiblity
requirements. The objective function is the cost function of
the solution. Both the feasibility requirements and the cost
function are specified by the problem. For example, in the
Vertex Cover problem, we are given a graph $G=(V,E)$ and we
are to find a subset $W$ of $V$, such that every edge $e\in
E$ has at least one endpoint in $W$; we want such a set $W$
to have minimum size. To formulate this problem as an
integer program, we use $x_v \in \{0,1\}$ to denote whether
a node $v\in V$ is in $W$ or not. The constraint is that for
every edge $e=(u,v)$, we have $x_u + x_v \geq 1$. The
objective is to minimize $\sum_{v\in V} x_v$. The integer
program for Vertex Cover is written as
\begin{empheq}[box=\fbox]{align*}
  \text{minimize } &\sum_{v\in V} x_v &\\
  \text{subject to }& x_u + x_v \geq 1 &\quad &\forall (u,v)
  \in
  E\\
  &x_v \in \{0, 1\} &\quad &\forall v \in V
\end{empheq}
In general an integer program cannot be solved exactly in
polynomial time, as Integer Programming is
{\NP}-hard. However, if we relax the integral constraint and
allow the variables to take fractional values, we then
obtain a Linear Program (LP) and LP is polynomially
solvable, for example, using the ellipsoid method or the
interior point method. Thus we can first solve the LP
optimally, obtaining a fractional optimal solution to the
LP. The value of the fractional optimal solution is then a
lower bound on the value of the integral optimal solution,
assuming a minimization problem. Our next step is then to
round the fractional solution appropriately, so that we
maintain the feasibility while keeping the cost from
increasing too much. The exact rounding procedure is problem
specific and we shall not delve into the topic here. The
rounding relevant to the FTFP problem, the problem studied
in this thesis, is presented in detail in Chapter~\ref{ch:
  lp-rounding}.

\subsection{Linear Programming, Duality and Complementary
  Slackness Conditions}
\label{subsec: LP_duality_CSC}
We now give a brief overview of linear programming;
see~\cite{Chvatal83} for an introductory book on this topic.
A general Linear Program can be written as
\begin{empheq}[box=\fbox]{align}
  \label{eqn:lp_primal}
  \text{minimize } & \sum_{j=1}^n c_j x_j & \\ \notag
  \text{subject to } & \sum_{j=1}^n a_{ij} x_j \geq b_i,
   & \text{for } i = 1, \ldots, m\\ \notag
   & x_j \geq 0 & \text{for } j = 1, \ldots, n
\end{empheq}
For the LP above, we can take its dual as
\begin{empheq}[box=\fbox]{align}
  \label{eqn:lp_dual}
  \text{maximize } & \sum_{i=1}^m b_i y_i &\\ \notag
  \text{subject to } & \sum_{i=1}^m a_{ij} y_i \leq c_j
   & \text{for } j = 1, \ldots, n\\ \notag
   & y_i \geq 0 & \text{for } i = 1,\ldots,m
\end{empheq}
The LP (\ref{eqn:lp_primal}) is called the primal program
and the LP (\ref{eqn:lp_dual}) is called the dual
program. Regarding the objective function value of the two
programs, we have the Weak Duality Theorem:
\begin{theorem}
  \label{thm:weak_duality}
  For every feasible solution $\bfx$ to the primal
  (\ref{eqn:lp_primal}) and $\bfy$ to the dual
  (\ref{eqn:lp_dual}), we have that $\textbf{c}^T \bfx \geq
  \textbf{b}^T \bfy$.
\end{theorem}
The Strong Duality Theorem is that:
\begin{theorem}
  \label{thm:strong_duality}
  If both the primal (\ref{eqn:lp_primal}) and the dual
  (\ref{eqn:lp_dual}) are feasible, then both of them have
  optimal solution $\bfx^\ast$ and $\bfy^\ast$ and their
  objective function values are equal, that is $\textbf{c}^T
  \bfx^\ast = \textbf{b}^T \bfy^\ast$.
\end{theorem}
One way to characterize optimal primal and dual solutions is
the Complementary Slackness Conditions. The complementary
slackness condtions says that:
\begin{theorem}
  \label{thm:complementary_slackness}
  Two feasible solutions $\bfx$ and $\bfy$ are both optimal
  to LP (\ref{eqn:lp_primal}) and (\ref{eqn:lp_dual})
  respectively, if and only if, for every primal variable
  $x_j$, either $x_j = 0$ or the corresponding constraint in
  the dual is tight, that is $\sum_{i=1}^m a_{ij} y_i =
  c_j$; and for every dual variable $y_i$, either $y_i = 0$
  or the corresponding constraint in the primal is tight,
  that is $\sum_{j=1}^n a_{ij} x_j = b_i$.
\end{theorem}
The complementary slackness conditions provide a simple way
to validate the optimality when one is presented with a
primal solution and a dual solution that are claimed to be
optimal. In addition, the complementary slackness conditions
play a crucial role in the design and analysis of
approximation algorithms. For example, suppose we have an
algorithm that computes a feasible integral solution $\bfx$
to the primal program (\ref{eqn:lp_primal}) and a feasible
fractional solution to the dual program
(\ref{eqn:lp_dual}). Moreover, we know that the two
solutions satisfy a relaxed version of the complementary
slackness conditions: for some numbers $\gamma$ and $\rho$,
we have
\begin{empheq}[box=\fbox]{align*}
  \text{either } y_i = 0  \quad \text{or } \quad b_i \leq \sum_{j}
  a_{ij} x_j \leq \gamma\, b_i, \qquad \text{for } i = 1,
  \ldots, m;\\
  \text{either } x_j = 0  \quad \text{or } \quad \rho\, c_j \leq
  \sum_{i}a_{ij}y_i \leq c_j, \qquad \text{for } j = 1,
  \ldots, n.
\end{empheq}
Then the integral solution $\bfx$ has a cost of no more than
$\gamma/\rho$ times the optimal value. In particular, we
have $\sum_{j} c_j x_j \leq \gamma/\rho \sum_{i} b_i y_i$
and the value for a feasible dual solution, namely $\sum_{i}
b_i y_i$, is a lower bound on the optimal value of the
primal program.

To show an application of the complementary slackness
conditions, we look at their use in the design and analysis
of algorithms for the Uncapacitated Facility Location
problem (UFL). Recall that we define the neighborhood $N(j)$
of a client $j$ as the set of facilities with $x_{ij}^\ast >
0$, where $(\bfx^\ast,\bfy^\ast)$ is some fractional optimal
solution to the LP~(\ref{eqn:ufl_primal}) and
$(\bfalpha^\ast, \bfbeta^\ast)$ is some optimal fractional
dual solution to the LP~(\ref{eqn:ufl_dual}). The
complementary slackness conditions give an upper bound of
$\alpha_j^\ast$ on the maximum distance from a facility $i
\in N(j)$ to a client $j$. To see this bound, notice that
the dual constraint says $\alpha_j - \beta_{ij} \leq
d_{ij}$.  If the primal solution has $x_{ij}^\ast > 0$, then
the inequality is actually an equality and we have
$\alpha_j^\ast - \beta_{ij}^\ast = d_{ij}$. Together with
$\beta_{ij}^\ast \geq 0$, we have $\alpha_j^\ast \geq
d_{ij}$ for every facility $i$ such that $x_{ij}^\ast >
0$. By definition, those are facilities in the neighborhood
$N(j)$. Therefore we have $d_{ij} \leq \alpha_j^\ast$ for
every $i\in N(j)$.

A more important application of using relaxed complementary
slackness conditions for the UFL problem is demonstrated by
Jain and Vazirani~\cite{JainV01}. They proposed an algorithm
that outputs an integral solution $(\bfx, \bfy)$ to the
primal program (\ref{eqn:ufl_primal}) and a feasible
(possibly fractional) solution $(\bfalpha,\bfbeta)$ to the
dual program (\ref{eqn:ufl_dual}). Moreover, the two
solutions satisfy the conditions that
\begin{empheq}[box=\fbox]{align*}
  &\text{either } \sum_{j} \beta_{ij} = f_i  \quad \text{ or
} \quad y_i = 0;\\
  &\text{either } (1/3)\, d_{ij} \leq \alpha_j - \beta_{ij}
  \leq d_{ij} \quad \text{ or } \quad x_{ij} = 0.
\end{empheq}
The solution $(\bfx,\bfy)$ then is a $3$-approximation.

\section{Proof of Inequality (\ref{eqn: echs ineq direct
    cost, step 1})}
\label{sec: ECHSinequality}

In Sections~\ref{sec: 1.736-approximation} and \ref{sec:
  1.575-approximation} we use the following inequality
%
\begin{align}
  \label{eq:min expected distance}
  \bard_1 g_1 + \bard_2 g_2 (1-g_1) +
  \ldots &+ \bard_k g_k (1-g_1) (1-g_2) \ldots (1-g_k)\\ \notag
  &\leq \frac{1}{\sum_{s=1}^k g_s} \left(\textstyle\sum_{s=1}^k \bard_s g_s\right)\left(\textstyle\sum_{t=1}^k g_t \textstyle\prod_{z=1}^{t-1} (1-g_z)\right).
\end{align}
%
for $0 < \bard_1\leq \bard_2 \leq \ldots \leq \bard_k$, and
$0 < g_1,...,g_s \le 1$.

\medskip

We give here a new proof of this inequality, much simpler
than the existing proof in \cite{ChudakS04}, and also
simpler than the argument by Sviridenko~\cite{Svi02}.  We
derive this inequality from the following generalized
version of the Chebyshev Sum Inequality:
%
\begin{equation}
  \label{eq:cheby}
  \textstyle{\sum_{i}} p_i \textstyle{\sum_j} p_j a_j b_j \leq \textstyle{\sum_i} p_i a_i \textstyle{\sum_j} p_j b_j,
\end{equation}
%
where each summation runs from $1$ to $l$ and the sequences
$(a_i)$, $(b_i)$ and $(p_i)$ satisfy the following
conditions: $p_i\geq 0, a_i \geq 0, b_i \geq 0$ for all $i$,
$a_1\leq a_2 \leq \ldots \leq a_l$, and $b_1 \geq b_2 \geq
\ldots \geq b_l$.

Given inequality (\ref{eq:cheby}), we can obtain our
inequality (\ref{eq:min expected distance}) by simple
substitution
%
\begin{equation*}
  p_i \leftarrow g_i, a_i \leftarrow \bard_i, b_i \leftarrow
  \Pi_{s=1}^{i-1} (1-g_s),
\end{equation*}
%
for $i = 1,...,k$.

For the sake of completeness, we include the proof of
inequality (\ref{eq:cheby}), due to Hardy, Littlewood and
Polya~\cite{HardyLP88}. The idea is to evaluate the
following sum:
%
\begin{align*}
  S &= \textstyle{\sum_i} p_i \textstyle{\sum_j} p_j a_j b_j - \textstyle{\sum_i} p_i a_i \textstyle{\sum_j} p_j b_j
	\\
  & = \textstyle{\sum_i \sum_j} p_i p_j a_j b_j - \textstyle{\sum_i \sum_j} p_i a_i p_j b_j
	\\
  & = \textstyle{\sum_j \sum_i} p_j p_i a_i b_i - \textstyle{\sum_j \sum _i} p_j a_j p_i b_i
	\\
	&= \half \cdot \textstyle{\sum_i \sum_j} (p_i p_j a_j b_j - p_i a_i p_j b_j + p_j p_i a_i
  							b_i - p_j a_j p_i b_i)
\\
  &= \half \cdot \textstyle{\sum_i \sum_j} p_i p_j (a_i - a_j)(b_i - b_j) \leq 0.
\end{align*}
The last inequality holds because $(a_i-a_j)(b_i-b_j) \leq
0$, since the sequences $(a_i)$ and $(b_i)$ are ordered
oppositely.


\end{document}

% marek Tue Jul  3 10:21:05 PDT 2012
% marek Sun Jul  1 14:57:39 PDT 2012
% lyan Sat Jun 30 2012, 22:01:27
% marek Sat Jun 30 10:08:59 PDT 2012
% lyan Fri Jun 29 19:54:18 PDT 2012
% marek Thu Jun 28 09:21:14 PDT 2012
% lyan Thu Jun 28 00:11:28 PDT 2012
% marek Wed Jun 27 11:24:07 PDT 2012
% lyan Wed Jun 27 2012, 10:08:21
% marek Tue Jun 26 14:48:45 PDT 2012
% lyan Mon Jun 25 2012, 22:23:13
% marek Sun Jun 24 16:46:23 PDT 2012
% marek Wed Jun 20 04:42:40 PDT 2012
% lyan, Sun Jun 17 2012, 09:49:22
% marek Sat Apr  7 16:42:21 PDT 2012
% marek Thu Apr  5 11:39:58 PDT 2012
% marek Wed Apr  4 11:28:20 PDT 2012
% lyan, 04/01/12 10:20 PM
% lyan, Mon Mar 26 2012, 09:10:54
% lyan, Tue Mar 20 2012, 23:28:17
% lyan, 03/18/12 12:28 PM
% marek Sat Mar 17 13:42:32 PDT 2012
% marek, Wed Mar  7 21:28:24 PST 2012
% marek Mon Mar 12 12:08:25 PDT 2012



\end{document}

%% reference
%% http://www.maths.qmul.ac.uk/~fv/books/mw/mwbook.pdf
%%
%% TODO: pdflatex has font issue with \beta k, ligature?

%% 05/15/2013, first draft
