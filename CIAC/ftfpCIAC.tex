% CIAC submission
% lyan 2012-11-03 09:09:22
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tue Aug 28 2012, 09:55:36
% marek Sat Jul 14 13:24:51 PDT 2012
% marek Fri Jul 13 15:51:33 PDT 2012
% lyan Thu Jul 12 2012, 00:06:30
% lyan Wed Jul 04 2012, 22:29:33
%
% marek Fri Oct 26 01:59:39 PDT 2012



\documentclass[11pt]{article}

\usepackage{fullpage,amssymb,amsthm,enumerate}
\usepackage[nosumlimits]{amsmath}
\usepackage[nothing]{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}

\floatname{algorithm}{Pseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\input{macros.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{LP-rounding Algorithms for the Fault-Tolerant\\
 		Facility Placement Problem}

              \author{Li Yan and Marek Chrobak\\
                Computer Science, U of California, Riverside\\
                \{lyan,marek\}@cs.ucr.edu\\
              }

\date{}

\begin{document}
\maketitle

\begin{abstract} 
  The Fault-Tolerant Facility Placement problem (FTFP) is a
  generalization of the classic Uncapacitated Facility Location
  Problem (UFL). In FTFP we are given a set of facility sites and a
  set of clients. Opening a facility at site $i$ costs $f_i$ and
  connecting client $j$ to a facility at site $i$ costs $d_{ij}$. We
  assume that the connection costs (distances) $d_{ij}$ satisfy the
  triangle inequality. Multiple facilities can be opened at any
  site. Each client $j$ has a demand $r_j$, which means that it needs
  to be connected to $r_j$ different facilities (some of which could
  be located on the same site). The goal is to minimize the sum of
  facility opening cost and connection cost. The main result of this
  paper is a $1.575$-approximation algorithm for FTFP, based on
  LP-rounding. The algorithm first reduces the demands to values
  polynomial in the number of sites. Then it uses a technique that we
  call adaptive partitioning, which partitions the instance by
  splitting clients into unit demands and creating a number of (not
  yet opened) facilities at each site. It also partitions the optimal
  fractional solution to produce a fractional solution for this new
  instance.  The partitioned instance satisfies a number of properties
  that allow us to exploit existing LP-rounding methods for UFL to
  round our partitioned solution to an integral solution, preserving
  the approximation ratio.  In particular, our $1.575$-approximation
  algorithm is based on the ideas from the $1.575$-approximation
  algorithm for UFL by Byrka~\etal, with changes necessary to satisfy
  the fault-tolerance requirement.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% INTRODUCTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

In the \emph{Fault-Tolerant Facility Placement} problem
(FTFP), we are given a set $\sitesset$ of \emph{sites} at
which facilities can be built, and a set $\clientset$ of
\emph{clients} with some demands that need to be satisfied
by different facilities. A client $j$ has demand
$r_j$. Building one facility at a site $i$ incurs a cost
$f_i$, and connecting one unit of demand from client $j$ to
a facility at site $i\in\sitesset$ costs $d_{ij}$. Throughout the
paper we assume that the connection costs $d_{ij}$ are
symmetric and satisfy the triangle inequality. In a feasible solution, some
number of facilities, possibly zero, are opened at each site
$i$, and demands from each client are connected to those
open facilities, with the constraint that demands from the
same client have to be connected to different
facilities (possibly on the same site).

It is easy to see that if all $r_j=1$ then FTFP reduces to
the classic Uncapacitated Facility Location problem (UFL).
If we add a constraint that each site can have at most one
facility, then the problem is equivalent to the Fault-Tolerant Facility
Location problem (FTFL). Note that
in FTFL we have $\max_{j\in\clientset}r_j \leq |\sitesset|$, 
while in FTFP the values of $r_j$'s can be much bigger than $|\sitesset|$.

Great progress has been achieved lately in designing
approximation algorithms for UFL.
Shmoys~\etal~\cite{ShmoysTA97} proposed an approach based on
LP-rounding, achieving a ratio of 3.16.  This was then
improved by Chudak~\cite{ChudakS04} to 1.736, and later by
Sviridenko~\cite{Svi02} to 1.582.  Byrka~\cite{ByrkaA10}
gave an improved algorithm with ratio 1.5, by a combination of
LP-rounding with dual-fitting techniques.  Recently, Li~\cite{Li11} refined
the method from \cite{ByrkaA10} to obtain ratio 1.488, which is now
the best known approximation result for UFL. Other
techniques include the primal-dual algorithm by Jain and
Vazirani~\cite{JainV01}, the dual fitting method by
Jain~{\etal}~\cite{JainMMSV03}, and a local search heuristic
by Arya~{\etal}~\cite{AryaGKMMP04}.  On the hardness side,
it is known that it is not possible to approximate UFL in
polynomial time with ratio less than $1.463$, provided that
$\NP\not\subseteq\DTIME(n^{O(\log\log
  n)})$~\cite{GuhaK98}. An observation by Sviridenko
strengthened this assumption to $\PP\neq
\NP$~\cite{vygen05}.

FTFL was first introduced by Jain and
Vazirani~\cite{JainV03} who gave a primal-dual algorithm
with ratio $3\ln(\max_{j\in\clientset}r_j)$.  All
subsequently discovered constant-ratio approximation
algorithms use variations of LP-rounding, including the work
by Guha~{\etal}~\cite{GuhaMM01}, Swamy and
Shmoys~\cite{SwamyS08}, and Byrka~{\etal}~\cite{ByrkaSS10},
who improved the ratio to 1.7245, the best known
approximation ratio for FTFL.

FTFP is a natural generalization of UFL. It was first
studied by Xu and Shen~\cite{XuS09}, who presented an
approximation algorithm with a ratio claimed to be
$1.861$. However their algorithm runs in polynomial time
only if $\max_{j\in\clientset} r_j$ is polynomial in
$O(|\sitesset|\cdot |\clientset|)$ and their analysis of the
approximation ratio is flawed\footnote{Confirmed through
  private communication with the authors.}.  To date, the
best approximation ratio for FTFP is $4$ in~\cite{YanC11}, 
while the only known lower bound is the $1.463$ lower bound for UFL
from~\cite{GuhaK98}, that applies to FTFP.

\smallskip

The main result of this paper is an LP-rounding algorithm for FTFP
with approximation ratio 1.575, matching the best ratio for UFL
achieved via the LP-rounding method \cite{ByrkaGS10} and significantly
improving the bound in~\cite{YanC11}. In Section~\ref{sec: polynomial
  demands} we prove that, for the purpose of LP-based approximations,
we can assume that all demand values are polynomial in the number of
sites. This \emph{demand reduction} trick itself gives us ratio
$1.7245$, since we can then treat an instance of FTFP as an instance
of FTFL and use the algorithm from~\cite{ByrkaSS10}. It also ensures
that our algorithms run in polynomial time. If all demand values $r_j$
are equal, the problem can be solved by simple scaling and applying
LP-rounding algorithms for UFL. This does not affect the approximation
ratio, thus achieving ratio $1.575$ for this special case (see also
\cite{LiaoShen11}). In Section~\ref{sec: adaptive partitioning}, we
demonstrate a technique called \emph{adaptive partitioning}, which
splits clients into unit demands and partitions the optimal fractional
solution into a fractional solution of the split instance. By
exploiting structural properties of the partitioned solution we were
able to extend UFL rounding algorithms
in~\cite{gupta08,ChudakS04,ByrkaGS10}, retaining the approximation
ratio.

Summarizing, we show that the existing LP-rounding
algorithms for UFL can be extended to a much more general
problem FTFP, retaining the approximation ratio. We believe
that, should even better LP-rounding algorithms be developed
for UFL, using our demand reduction and
adaptive partitioning methods, it should be possible to
extend them to FTFP.  In fact, some improvement of the ratio
can be achieved by randomizing the parameter
$\gamma$ used in Section~\ref{sec: 1.575-approximation}, as Li showed in \cite{Li11}
for UFL.  Our ratio of $1.575$ is significantly better
than the best ratio of $1.7245$ for the closely-related FTFL
problem. This suggests that in the fault-tolerant scenario
the capability of creating additional copies of facilities
makes the problem easier from the point of view of
approximation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% LP ForMULATION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The LP Formulation}\label{sec: the lp formulation}

The FTFP problem has a natural Integer Programming (IP)
formulation. Let $y_i$ represent the number of facilities
built at site $i$ and let $x_{ij}$ represent the number of
connections from client $j$ to facilities at site $i$. If we
relax the integrality constraints, we obtain the following LP:

\noindent
\hspace{-0.1in}
\begin{minipage}{3.5in}
\begin{alignat}{3}
  &\textrm{min} &&\;\textstyle{\cost(\bfx,\bfy)} = \textstyle{ \sum_{i\in \sitesset}f_iy_i }
 		&&	+ \textstyle{ \sum_{i\in \sitesset, j\in \clientset}d_{ij}x_{ij} }  \label{eqn:fac_primal} 	
								\\ \notag
  &\textrm{s.t.} && y_i - x_{ij} \geq 0 &&  \forall i\in \sitesset, j\in \clientset 
									\\ \notag
  & &&\textstyle{\sum_{i\in \sitesset} x_{ij} \geq r_j} && \forall j\in \clientset
 									\\ \notag
  & &&\textstyle{ x_{ij} \geq 0, y_i \geq 0} && \forall i\in \sitesset, j\in \clientset 
  									\\ \notag
\end{alignat}
\end{minipage}
\hspace{0.01in}
\begin{minipage}{3in}
\begin{alignat}{3}
  &\textrm{max}&\quad& \textstyle{\sum_{j\in \clientset} r_j\alpha_j}\label{eqn:fac_dual}  
     						\\ \notag
  &\textrm{s.t.} && \textstyle{
    \sum_{j\in \clientset}\beta_{ij} \leq f_i}  &\quad& \forall i \in \sitesset  
							\\ \notag
  & &&\textstyle{\alpha_{j} - \beta_{ij} \leq
    d_{ij}}  && \forall i\in \sitesset, j\in \clientset 
							\\ \notag
  & &&\textstyle{\alpha_j \geq 0,
    \beta_{ij} \geq 0} && \forall i\in \sitesset, j\in \clientset
  							\\ \notag
\end{alignat}
\end{minipage}

In each of our algorithms we will fix some optimal solutions of the
LPs (\ref{eqn:fac_primal}) and (\ref{eqn:fac_dual}) that we will
denote by $(\bfx^\ast, \bfy^\ast)$ and $(\bfalpha^\ast,\bfbeta^\ast)$,
respectively. With $(\bfx^\ast, \bfy^\ast)$ fixed, we can define the
optimal facility cost as $F^\ast=\sum_{i\in\sitesset} f_i y_i^\ast$
and the optimal connection cost as $C^\ast =
\sum_{i\in\sitesset,j\in\clientset} d_{ij}x_{ij}^\ast$.  Then
$\LP^\ast = \cost(\bfx^\ast,\bfy^\ast) = F^\ast+C^\ast$ is the joint
optimal value of (\ref{eqn:fac_primal}) and (\ref{eqn:fac_dual}).  We
can also associate with each client $j$ its fractional connection cost
$C^\ast_j = \sum_{i\in\sitesset} d_{ij}x_{ij}^\ast$.  Clearly, $C^\ast
= \sum_{j\in\clientset} C^\ast_j$.  Throughout the paper we will use
notation $\OPT$ for the optimal integral solution of
(\ref{eqn:fac_primal}).  $\OPT$ is the value we wish to approximate,
but, since $\OPT\ge\LP^\ast$, we can instead use $\LP^\ast$ to
estimate the approximation ratio of our algorithms.

%%%%%%%%%

Define $(\bfx^\ast, \bfy^\ast)$ to be \emph{complete} if
$x_{ij}^\ast>0$ implies that $x_{ij}^\ast=y_i^\ast$ for all
$i,j$. In other words, each connection either uses a site
fully or not at all.  As shown by Chudak and
Shmoys~\cite{ChudakS04}, we can modify the given instance by
adding at most $|\clientset|$ sites to obtain an equivalent
instance that has a complete optimal solution, where
``equivalent" means that the values of $F^\ast$, $C^\ast$ and
$\LP^\ast$ are not affected.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% REDUCTION TO POLYNOMIAL DEMANDS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reduction to Polynomial Demands}
\label{sec: polynomial demands}

This section presents a \emph{demand reduction} trick that
reduces the problem for arbitrary demands to a special case
where demands are bounded by $|\sitesset|$, the number of
sites.  (The formal statement is a little more technical --
see Theorem~\ref{thm: reduction to polynomial}.)  Our
algorithms in the sections that follow process individual
demands of each client one by one, and thus they critically
rely on the demands being bounded polynomially in terms of
$|\sitesset|$ and $|\clientset|$ to keep the overall running time polynomial.

The reduction is based on an optimal fractional solution
$(\bfx^\ast,\bfy^\ast)$ of LP (\ref{eqn:fac_primal}). From the
optimality of this solution, we can also assume that
$\sum_{i\in\sitesset} x^\ast_{ij} = r_j$ for all
$j\in\clientset$.  As explained in Section~\ref{sec: the lp
  formulation}, we can assume that $(\bfx^\ast,\bfy^\ast)$
is complete, that is $x^\ast_{ij} > 0$ implies $x^\ast_{ij}
= y^\ast_i$ for all $i,j$.  We split this solution into two
parts, namely $(\bfx^\ast,\bfy^\ast) = (\hatbfx,\hatbfy)+
(\dotbfx,\dotbfy)$, where
%
$\haty_i \;\assign\; \floor{y_i^\ast}, \quad
			\hatx_{ij} \;\assign\; \floor{x_{ij}^\ast} \quad\textrm{and}\quad
\doty_i \;\assign\; y_i^\ast - \floor{y_i^\ast}, \quad
 	\dotx_{ij} \;\assign\; x_{ij}^\ast -  \floor{x_{ij}^\ast}
$
%
for all $i,j$. Now we construct two
FTFP instances $\hatcalI$ and $\dotcalI$ with the same
parameters as the original instance, except that the demand of each client $j$ is
$\hatr_j = \sum_{i\in\sitesset} \hatx_{ij}$ in instance $\hatcalI$ and
$\dotr_j = \sum_{i\in\sitesset} \dotx_{ij} = r_j - \hatr_j$ in instance $\dotcalI$. 
It is obvious that if we have integral solutions to both $\hatcalI$
and $\dotcalI$ then, when added together, they form an integral
solution to the original instance.  Moreover, we have the
following lemma.

%%%%%%%%%%

\begin{lemma}\label{lem: polynomial demands partition}
{\rm (i)}
  $(\hatbfx, \hatbfy)$ is a feasible integral solution to
  instance $\hatcalI$.

\noindent
{\rm (ii)}
  $(\dotbfx, \dotbfy)$ is a feasible fractional
  solution to instance $\dotcalI$.

\noindent
{\rm (iii)}
$\dotr_j\leq |\sitesset|$ for every client $j$.

\end{lemma}

Notice that our construction relies on the completeness assumption; in
fact, it is easy to give an example where $(\dotbfx, \dotbfy)$ would
not be feasible if we used a non-complete optimal solution
$(\bfx^\ast,\bfy^\ast)$.  Note also that the solutions
$(\hatbfx,\hatbfy)$ and $(\dotbfx, \dotbfy)$ are in fact optimal for
their corresponding instances, for if a better solution to $\hatcalI$
or $\dotcalI$ existed, it could give us a solution to $\calI$ with a
smaller objective value.

%%%%%%%%%%%%%%%

\begin{theorem}\label{thm: reduction to polynomial}
  Suppose that there is a polynomial-time algorithm $\calA$
  that, for any instance of {\FTFP} with maximum demand
  bounded by $|\sitesset|$, computes an integral solution
  that approximates the fractional optimum of this instance
  within factor $\rho\geq 1$.  Then there is a
  $\rho$-approximation algorithm $\calA'$ for {\FTFP}.
\end{theorem}

%%%%%%%%%%%%%%%

\begin{proof}
  Given an {\FTFP} instance with arbitrary demands, Algorithm~$\calA'$ works
as follows: it solves the LP~(\ref{eqn:fac_primal}) to obtain a
  fractional optimal solution $(\bfx^\ast,\bfy^\ast)$, then it constructs
  instances $\hatcalI$ and $\dotcalI$ described above,  applies
  algorithm~$\calA$ to $\dotcalI$, and finally combines (by adding
  the values) the integral solution $(\hatbfx, \hatbfy)$ of
  $\hatcalI$ and the integral solution of $\dotcalI$ produced
  by $\calA$. This clearly produces a feasible integral
  solution for the original instance $\calI$.
The solution produced by $\calA$ has cost at most
$\rho\cdot\cost(\dotbfx,\dotbfy)$, because $(\dotbfx,\dotbfy)$
is feasible for $\dotcalI$. Thus the cost of $\calA'$ is at most
% 
$
 \cost(\hatbfx, \hatbfy) + \rho\cdot\cost(\dotbfx,\dotbfy)
	\le
 \rho(\cost(\hatbfx, \hatbfy) + \cost(\dotbfx,\dotbfy))
		= \rho\cdot\LP^\ast \le \rho\cdot\OPT,
$
%
where the first inequality follows from $\rho\geq 1$. This completes
the proof.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% ADAPTIVE PARTITION  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Adaptive Partitioning}
\label{sec: adaptive partitioning}

In this section we develop our second technique, which we call
\emph{adaptive partitioning}. Given an FTFP instance and an optimal
fractional solution $(\bfx^\ast, \bfy^\ast)$ to
LP~(\ref{eqn:fac_primal}), we split each client $j$ into $r_j$
individual \emph{unit demand points} (or just \emph{demands}), and we
split each site $i$ into no more than $|\sitesset|+2R|\clientset|^2$
\emph{facility points} (or \emph{facilities}), where
$R=\max_{j\in\clientset}r_j$. We denote the demand set by $\demandset$
and the facility set by $\facilityset$, respectively.  We will also
partition $(\bfx^\ast,\bfy^\ast)$ into a fractional solution
$(\barbfx,\barbfy)$ for the split instance.  We will typically use
symbols $\nu$ and $\mu$ to index demands and facilities respectively,
that is $\barbfx = (\barx_{\mu\nu})$ and $\barbfy = (\bary_{\mu})$.
The \emph{neighborhood of a demand} $\nu$ is
$\wbarN(\nu)=\braced{\mu\in\facilityset \suchthat \barx_{\mu\nu}>0}$.
We will use notation $\nu\in j$ to mean that $\nu$ is a demand of
client $j$; similarly, $\mu\in i$ means that facility $\mu$ is on site
$i$. Different demands of the same client (that is, $\nu,\nu'\in j$)
are called \emph{siblings}.  Further, we use the convention that
$f_\mu = f_i$ for $\mu\in i$, $\alpha_\nu^\ast = \alpha_j^\ast$ for
$\nu\in j$ and $d_{\mu\nu} = d_{\mu j} = d_{ij}$ for $\mu\in i$ and
$\nu\in j$.  We define $\concost_{\nu}
=\sum_{\mu\in\wbarN(\nu)}d_{\mu\nu}\barx_{\mu\nu} =
\sum_{\mu\in\facilityset}d_{\mu\nu}\barx_{\mu\nu}$.  One can think of
$\concost_{\nu}$ as the average connection cost of demand $\nu$, if we
chose a connection to facility $\mu$ with probability
$\barx_{\mu\nu}$. In our partitioned fractional solution we guarantee
for every $\nu$ that $\sum_{\mu\in\facilityset} \barx_{\mu\nu}=1$.

Some demands in $\demandset$ will be designated as
\emph{primary demands} and the set of primary demands will
be denoted by $P$. In addition, we will use the overlap
structure between demand neighborhoods to define a mapping
that assigns each demand $\nu\in\demandset$ to some primary
demand $\kappa\in P$. As shown in the rounding algorithms in
later sections, for each primary demand we guarantee exactly
one open facility in its neighborhood, while for a
non-primary demand, there is constant probability that none
of its neighbors open. In this case we estimate its
connection cost by the distance to the facility opened in
its assigned primary demand's neighborhood. For this reason
the connection cost of a primary demand must be ``small''
compared to the non-primary demands assigned to it. We also
need sibling demands assigned to different primary demands to satisfy
the fault-tolerance requirement. Specifically, this
partitioning will be constructed to satisfy a number of
properties that are detailed below.
%
\begin{description}
	
      \renewcommand{\theenumii}{(\alph{enumii})}
      \renewcommand{\labelenumii}{\theenumii}

\item{(PS)} \emph{Partitioned solution}.
Vector $(\barbfx,\barbfy)$ is a partition of $(\bfx^\ast,\bfy^\ast)$, with unit-value
  demands, that is:

	\begin{enumerate}
		%
	\item \label{PS:one} 
          $\sum_{\mu\in\facilityset} \barx_{\mu\nu} = 1$ for each demand $\nu\in\demandset$. 
		%
	\item \label{PS:xij} $\sum_{\mu\in i, \nu\in j} \barx_{\mu\nu}
          = x^\ast_{ij}$ for each site $i\in\sitesset$ and client $j\in\clientset$.
		%
	\item \label{PS:yi}
          $\sum_{\mu\in i} \bary_{\mu} = y^\ast_i$ for each site $i\in\sitesset$.
		%
	\end{enumerate}
		
\item{(CO)} \emph{Completeness.}
	Solution   $(\barbfx,\barbfy)$ is complete, that is $\barx_{\mu\nu}\neq 0$ implies
				$\barx_{\mu\nu} = \bary_{\mu}$, for all $\mu\in\facilityset, \nu\in\demandset$.

\item{(PD)} \emph{Primary demands.}
	Primary demands satisfy the following conditions:

	\begin{enumerate}
		
	\item\label{PD:disjoint}  For any two different primary demands $\kappa,\kappa'\in P$ we have
				$\wbarN(\kappa)\cap \wbarN(\kappa') = \emptyset$.

	\item \label{PD:yi} For each site $i\in\sitesset$, 
		$ \sum_{\mu\in i}\sum_{\kappa\in P}\barx_{\mu\kappa} \leq y_i^\ast$.
		
	\item \label{PD:assign} Each demand $\nu\in\demandset$ is assigned
        to one primary demand $\kappa\in P$ such that

  			\begin{enumerate}
	
				\item \label{PD:assign:overlap} $\wbarN(\nu) \cap \wbarN(\kappa) \neq \emptyset$, and
				%
				\item \label{PD:assign:cost} $\concost_{\nu}+\alpha_{\nu}^\ast \geq
        			\concost_{\kappa}+\alpha_{\kappa}^\ast$.

			\end{enumerate}

	\end{enumerate}
	
\item{(SI)} \emph{Siblings}. For any pair $\nu,\nu'$ of different siblings we have
  \begin{enumerate}

	\item \label{SI:siblings disjoint}
		  $\wbarN(\nu)\cap \wbarN(\nu') = \emptyset$.
		
	\item \label{SI:primary disjoint} If $\nu$ is assigned to a primary demand $\kappa$ then
 		$\wbarN(\nu')\cap \wbarN(\kappa) = \emptyset$. In particular, by Property~PD(\ref{PD:assign:overlap}),
		this implies that different sibling demands are assigned to different primary demands.

	\end{enumerate}
	
\end{description}

As we shall demonstrate in later sections, these properties allow us
to extend known UFL rounding algorithms to obtain an integral solution
to our FTFP problem with a matching approximation ratio. Our
partitioning is ``adaptive" in the sense that it is constructed one
demand at a time, and the connection values for the demands of a
client depend on the choice of earlier demands, of this or other
clients, and their connection values. We would like to point out that
the adaptive partitioning process for the $1.575$-approximation
algorithm (Section~\ref{sec: 1.575-approximation}) is more subtle than
the $3$-apprximation (Section~\ref{sec: 3-approximation}) and the
$1.736$-approximation algorithms (Section~\ref{sec:
  1.736-approximation}), due to the introduction of close and far
neighborhood.

%%%%%%%%%%%%%%%%

\paragraph{Implementation of Adaptive Partitioning.}
We now describe an algorithm for partitioning the instance
and the fractional solution so that the properties (PS),
(CO), (PD), and (SI) are satisfied.  Recall that
$\facilityset$ and $\demandset$, respectively, denote the
sets of facilities and demands that will be created in this
stage, and $(\barbfx,\barbfy)$ is the partitioned solution
to be computed. 

The adaptive partitioning algorithm consists of two phases:
Phase 1 is called the partition phase and Phase 2 is called
the augmenting phase. Phase 1 is done in iterations, where
in each iteration we find the ``best'' client $j$ and create a
new demand $\nu$ out of it. This demand either becomes a
primary demand itself, or it is assigned to some existing
primary demand. We call a client $j$ \emph{exhausted} when
all its $r_j$ demands have been created and assigned to some
primary demands. Phase 1 completes when all clients are
exhausted. In Phase 2 we ensure that every demand has a
total connection values equal to $1$, that is condition (PS.\ref{PS:one}).

For each site $i$ we will initially create one ``big" facility $\mu$
with initial value $\bary_\mu = y^\ast_i$.  While we partition the
instance, creating new demands and connections, this facility may end
up being split into more facilities to preserve completeness of the
fractional solution. Also, we will gradually decrease the fractional
connection vector for each client $j$, to account for the demands
already created for $j$ and their connection values.  These decreased
connection values will be stored in an auxiliary vector
$\tildebfx$. The intuition is that $\tildebfx$ represents the part of
$\bfx^\ast$ that still has not been allocated to existing demands and
future demands can use $\tildebfx$ for their connections. For
technical reasons, $\tildebfx$ will be indexed by facilities (rather
than sites) and clients, that is $\tildebfx = (\tildex_{\mu j})$.  At
the beginning, we set $\tildex_{\mu j}\assign x_{ij}^\ast$ for each
$j\in\clientset$, where $\mu\in i$ is the single facility created
initially at site $i$.  At each step, whenever we create a new demand
$\nu$ for a client $j$, we will define its values $\barx_{\mu\nu}$ and
appropriately reduce the values $\tildex_{\mu j}$, for all facilities
$\mu$. We will deal with two types of neighborhoods, with respect to
$\tildebfx$ and $\barbfx$, that is $\wtildeN(j)=\{\mu\in\facilityset
\suchthat\tildex_{\mu j} > 0\}$ for $j\in\clientset$ and
$\wbarN(\nu)=\{\mu\in\facilityset \suchthat \barx_{\mu\nu} >0\}$ for
$\nu\in\demandset$.  During this process, the following properties
will hold for every facility $\mu$ after every iteration: (c1) For
each demand $\nu$ either $\barx_{\mu\nu}=0$ or
$\barx_{\mu\nu}=\bary_{\mu}$; and (c2) For each client $j$, either
$\tildex_{\mu j}=0$ or $\tildex_{\mu j}=\bary_{\mu}$. (c1) is the same
condition as (CO), yet we repeat it here as (c1) needs to hold after
every iteration, while (CO) only applies to the final partitioned
fractional solution $(\barbfx, \barbfy)$;

A full description of the algorithm is given in
Pseudocode~\ref{alg:lpr2}.  Initially, the set $U$ of non-exhausted
clients contains all clients, the set $\demandset$ of demands is
empty, the set $\facilityset$ of facilities consists of one facility
$\mu$ on each site $i$ with $\bary_\mu = y^\ast_i$, and the set $P$ of
primary demands is empty.  In one iteration of the while loop, for
each client $j$ we compute a quantity called $\tcc(j)$ (tentative
connection cost), that represents the average distance from $j$ to the
set $\wtildeN_1(j)$ of the nearest facilities $\mu$ whose total
connection value to $j$ (the sum of $\tildex_{\mu j}$'s) equals $1$.
This set is computed by Procedure $\NearestUnitChunk()$, which adds
facilities to $\wtildeN_1(j)$ in order of nondecreasing distance,
until the total connection value is exactly $1$. This may require
splitting the last added facility and adjusting the connection values
so that conditions (c1) and (c2) are preserved. The next step is to
pick a client $p$ with minimum $\tcc(p)+\alpha_p^\ast$ and create a
demand $\nu$ for $p$. If $\wtildeN_1(p)$ overlaps the neighborhood of
some existing primary demand $\kappa$ (if there are multiple such
$\kappa$'s, pick any of them), we assign $\nu$ to $\kappa$, and $\nu$
acquires all the connection values $\tildex_{\mu p}$ between client
$p$ and facility $\mu$ in $\wtildeN(p)\cap \wbarN(\kappa)$. Note that
although we check for overlap with $\wtildeN_1(p)$, we then move all
facilities in the intersection with $\wtildeN(p)$, a bigger set, into
$\wbarN(\nu)$.  The other case is when $\wtildeN_1(p)$ is disjoint
from the neighborhoods of all existing primary demands. Then, $\nu$
becomes itself a primary demand and we assign $\nu$ to itself. It also
inherits the connection values to all facilities $\mu\in\wtildeN_1(p)$
from $p$ (recall that $\tildex_{\mu p} = \bary_{\mu}$), with all other
$\barx_{\mu\nu}$ values set to $0$. For non-primary demands we still
need to adjust the $\barx_{\mu\nu}$ values so that $\connsum(\nu)
\stackrel{\mathrm{def}}{=} \sum_{\mu\in\facilityset}\barx_{\mu \nu}$,
is equal to $1$. This is accomplished by Procedure $\AugmentToUnit()$
that allocates to $\nu\in j$ some of the remaining connection values
$\tildex_{\mu j}$ of client $j$. $\AugmentToUnit()$ will repeatedly
pick any facility $\eta$ with $\tildex_{\eta j} >0$.  If
$\tildex_{\eta j} \leq 1-\connsum(\nu)$, then the connection value
$\tildex_{\eta j}$ is reassigned to $\nu$.  Otherwise, $\tildex_{\eta
  j} > 1-\connsum(\nu)$, in which case we split $\eta$ so that
connecting $\nu$ to one of the created copies of $\eta$ will make
$\connsum(\nu)$ equal $1$, and we'll be done.

%%%%%%%%%%%

\begin{algorithm}[ht]
  \caption{Algorithm: Adaptive Partitioning}
  \label{alg:lpr2}
  \begin{algorithmic}[1]
    \Require $\sitesset$, $\clientset$, $(\bfx^\ast,\bfy^\ast)$
    \Ensure  $\facilityset$,  $\demandset$, $(\barbfx, \barbfy)$ 
    \Comment Unspecified $\barx_{\mu \nu}$'s and $\tildex_{\mu j}$'s are assumed to be $0$

    \State $\tildebfr \assign \bfr, U\assign \clientset, \facilityset\assign \emptyset,
    \demandset\assign \emptyset, P\assign \emptyset$
    \Comment{Phase 1}

    \For{each site $i\in\sitesset$} 
    \State create a facility $\mu$ at $i$ and add $\mu$ to $\facilityset$,
    $\bary_\mu \assign y_i^\ast$ and $\tildex_{\mu j}\assign
    x_{ij}^\ast$ for each $j\in\clientset$ 
    \EndFor

    \While{$U\neq \emptyset$}
    \For{each $j\in U$}
    \State $\wtildeN_1(j) \assign {\NearestUnitChunk}(j, \facilityset, \tildebfx, \barbfx, \barbfy)$ \Comment see Pseudocode~\ref{alg:helper}
    \State $\tcc(j)\assign \sum_{\mu\in \wtildeN_1(j)} d_{{\mu}j}\cdot \tildex_{\mu j}$
    \EndFor
 
    \State $p \assign {\argmin}_{j\in U}\{ \tcc(j)+\alpha_j^\ast \}$
    \State create a new demand $\nu$ for client $p$

    \If{$\wtildeN_1 (p)\cap \wbarN(\kappa) \neq \emptyset$
      for some primary demand $\kappa\in P$}
    \State assign $\nu$ to $\kappa$,
    $\barx_{\mu \nu}\assign \tildex_{\mu p}$ and $\tildex_{\mu p}\assign 0$ for each $\mu \in \wtildeN(p) \cap \wbarN(\kappa)$
    \Else 
    \State make $\nu$ primary, $P \assign P \cup \{\nu\}$,
    set $\barx_{\mu\nu} \assign \tildex_{\mu p}$ and $\tildex_{\mu p}\assign 0$ for each $\mu\in \wtildeN_1(p)$

    \EndIf
    \State $\demandset\assign \demandset\cup \{\nu\},
    \tilder_p \assign \tilder_p -1$
	\State \textbf{if} {$\tilder_p=0$} \textbf{then} $U\assign U \setminus \{p\}$
    \EndWhile

    \For{each client $j\in\clientset$} \Comment{Phase 2}
    \For{each demand $\nu\in j$}    \Comment{each client $j$ has $r_j$ demands}
    \State \textbf{if} $\sum_{\mu\in \wbarN(\nu)}\barx_{\mu\nu}<1$
    \textbf{then} $\AugmentToUnit(\nu, j, \facilityset, \tildebfx, \barbfx, \barbfy)$ \Comment see Pseudocode~\ref{alg:helper}
    \EndFor
    \EndFor
    \Comment{Pseudocode~\ref{alg:helper} in appendix}
  \end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%


Notice that we start with $|\sitesset|$ facilities and in
each iteration of the while loop in Line~5 each client causes at most one split.
 We have a total of no more than $R|\clientset|$ iterations as in
each iteration we create one demand. (Recall that $R =
\max_jr_j$.) In Phase 2 we do an augment step for each
demand $\nu$ and this creates no more than $R|\clientset|$
new facilities.  So the total number of facilities we
created will be at most $|\sitesset|+ R|\clientset|^2 +
R|\clientset| \leq |\sitesset| + 2R|\clientset|^2$, which is
polynomial in $|\sitesset|+|\clientset|$ due to our earlier
bound on $R$.

%%%%%%

\medskip

\emparagraph{Correctness.}  We now show that all the
required properties (PS), (CO), (PD) and (SI) are satisfied
by the above construction.

Properties~(PS) and (CO) follow directly from the
algorithm. (CO) is implied by the completeness condition
(c1) that the algorithm maintains after each
iteration. Condition~(PS.\ref{PS:one}) is a result of
calling Procedure~$\AugmentToUnit()$ in Line~21. To see that
(PS.\ref{PS:xij}) holds, note that
at each step the algorithm maintains the
invariant that, for every $i\in\sitesset$ and
$j\in\clientset$, we have $\sum_{\mu\in i}\sum_{\nu \in j}
\barx_{\mu \nu} + \sum_{\mu\in i} \tildex_{\mu j} =
x_{ij}^\ast$. In the end, we will create $r_j$ demands for
each client $j$, with each demand $\nu\in j$ satisfying
(PS.\ref{PS:one}), and thus $\sum_{\nu\in
  j}\sum_{\mu\in\facilityset}\barx_{\mu\nu}=r_j$.  This
implies that $\tildex_{\mu j}=0$ for every facility
$\mu\in\facilityset$, and PS(\ref{PS:xij}) follows.
PS(\ref{PS:yi}) holds because every time we split a
facility $\mu$ into $\mu'$ and $\mu''$, the sum of
$\bary_{\mu'}$ and $\bary_{\mu''}$ is equal to the old value of
$\bary_{\mu}$.

Now we deal with properties in group (PD).  First,
(PD.\ref{PD:disjoint}) follows directly from the algorithm,
Pseudocode~\ref{alg:lpr2} (Lines 14--16), since every
primary demand has its neighborhood fixed when created, and
that neighborhood is disjoint from those of the existing primary
demands.

Property (PD.\ref{PD:yi}) follows from (PD.\ref{PD:disjoint}), (CO) and
(PS.\ref{PS:yi}). In more detail, it can be justified as
follows. By (PD.\ref{PD:disjoint}), for each $\mu\in i$ there
is at most one $\kappa\in P$ with $\barx_{\mu\kappa} > 0$
and we have $\barx_{\mu\kappa} = \bary_{\mu}$ due do (CO).
Let $K\subseteq i$ be the set of those $\mu$'s for which
such $\kappa\in P$ exists, and denote this $\kappa$ by
$\kappa_\mu$. Then, using conditions (CO) and
(PS.\ref{PS:yi}), we have $ \sum_{\mu\in i}\sum_{\kappa\in
  P}\barx_{\mu\kappa} = \sum_{\mu\in K}\barx_{\mu\kappa_\mu}
= \sum_{\mu\in K}\bary_{\mu} \leq \sum_{\mu\in i}
\bary_{\mu} = y_i^\ast$.

Property (PD.\ref{PD:assign:overlap}) follows from the way the algorithm
assigns primary demands.  When demand $\nu$ of
client $p$ is assigned to a primary demand $\kappa$ in
Lines~11--13 of Pseudocode~\ref{alg:lpr2}, we move all
facilities in $\wtildeN(p)\cap \wbarN(\kappa)$ (the
intersection is nonempty) into $\wbarN(\nu)$, and we never
remove a facility from $\wbarN(\nu)$.  We postpone the proof 
for (PD.\ref{PD:assign:cost}) to Lemma~\ref{lem: PD:assign:cost holds}.

Finally we argue that the properties in group (SI)
hold. (SI.\ref{SI:siblings disjoint}) is easy, since for any client
$j$, each facility $\mu$ is added to the neighborhood of at most one
demand $\nu\in j$, by setting $\barx_{\mu\nu}$ to $\bary_\mu$, while
other siblings $\nu'$ of $\nu$ have $\barx_{\mu\nu'}=0$. Note that
right after a demand $\nu\in p$ is created, its neighborhood is
disjoint from the neighborhood of $p$, that is $\wbarN(\nu)\cap
\wtildeN(p) = \emptyset$, by Lines~11--13 of the algorithm. Thus all
demands of $p$ created later will have neighborhoods disjoint from the
set $\wbarN(\nu)$ before the augmenting phase 2. Furthermore,
Procedure~$\AugmentToUnit()$ preserves this property, because when it
adds a facility to $\wbarN(\nu)$ then it removes it from
$\wtildeN(p)$, and in case of splitting, one resulting facility is
added to $\wbarN(\nu)$ and the other to $\wtildeN(p)$. Property
(SI.\ref{SI:primary disjoint}) is shown below in Lemma~\ref{lem:
  property SI:primary disjoint holds}.

It remains to show Properties~(PD.\ref{PD:assign:cost}) and
(SI.\ref{SI:primary disjoint}). We show them in the lemmas
below, thus completing the description of our adaptive
partition process.

%%%%%%%

\begin{lemma}\label{lem: property SI:primary disjoint holds}
  Property~(SI.\ref{SI:primary disjoint}) holds after the
  Adaptive Partitioning stage.
\end{lemma}

We need one more lemma before proving our last property
(PD.\ref{PD:assign:cost}).  For a client $j$ and a demand
$\nu$, we use notation $\tcc_{\nu}(j)$ for the value of
$\tcc(j)$ at the time when $\nu$ was created. (It is not
necessary that $\nu\in j$ but we assume that $j$ is not
exhausted at that time.)


\begin{lemma}\label{lem: tcc optimal}
  Let $\eta$ and $\nu$ be two demands, with $\eta$ created
  not later than $\nu$, and let $j\in\clientset$ be a client
  that is not exhausted when $\nu$ is created. Then we have
\begin{description}
	\item{(a)} $\tcc_\eta(j) \le \tcc_{\nu}(j)$, and 
	\item{(b)} if $\nu\in j$ then $\tcc_\eta(j) \le \concost_{\nu}$.
\end{description}
\end{lemma}

%%%%%%%

\begin{lemma}\label{lem: PD:assign:cost holds}
Property~(PD.\ref{PD:assign:cost}) holds after the Adaptive Partitioning stage.
\end{lemma}

\begin{proof}
Suppose that demand $\nu\in j$ is assigned to some primary demand $\kappa\in p$.
Then
%
\begin{eqnarray*}
 \concost_{\kappa} + \alpha_{\kappa}^\ast \;=\; \tcc_\kappa(p) + \alpha^\ast_p
 					\;\le\; \tcc_\kappa(j) + \alpha^\ast_j   
					\;\le\; \concost_{\nu} + \alpha^\ast_\nu.
\end{eqnarray*}
%
We now justify this derivation. By definition we have
$\alpha_{\kappa}^\ast = \alpha^\ast_p$.  Further, by the
algorithm, if $\kappa$ is a primary demand of client $p$,
then $\concost_{\kappa}$ is equal to $\tcc(p)$ computed when
$\kappa$ is created, which is exactly $\tcc_\kappa(p)$. Thus
the first equation is true. The first inequality follows
from the choice of $p$ in Line~9 in
Pseudocode~\ref{alg:lpr2}. The last inequality holds
because $\alpha^\ast_j = \alpha^\ast_\nu$ (due to $\nu\in
j$), and because $\tcc_\kappa(j) \le \concost_{\nu}$, which
follows from Lemma~\ref{lem: tcc optimal}.
\end{proof}

We have thus proved that all properties (PS), (CO), (PD) and (SI) hold
for our partitioned fractional solution $(\barbfx,\barbfy)$. In the
following sections we show how to use these properties to round the
fractional solution to an approximate integral solution. For the
$3$-approximation algorithm (Section~\ref{sec: 3-approximation}) and
the $1.736$-approximation algorithm (Section~\ref{sec:
  1.736-approximation}), the first phase of the algorithm is exactly
the same partition process as described above. However, the
$1.575$-approximation algorithm (Section~\ref{sec:
  1.575-approximation}) demands a more sophisticated partitioning
process as the interplay between close and far neighborhood of sibling
demands result in more delicate properties that our partitioned
fractional solution must satisfy.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% 3-APPROXIMATION ALGORITHM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\input{three_approx.tex}
\section{Algorithm~{\EGUP} with Ratio $3$}
\label{sec: 3-approximation}

With the partitioned FTFP instance and its associated fractional
solution in place, we now begin to introduce our rounding algorithms.
The algorithm we describe in this section achieves ratio $3$. Although
this is still quite far from our best ratio $1.575$ that we derive
later, we include this algorithm in the paper to illustrate, in a
relatively simple setting, how the properties of our partitioned
fractional solution are used in rounding it to an integral solution
with cost not too far away from an optimal solution.  The rounding
approach we use here is an extension of the corresponding method for
UFL described in~\cite{gupta08}.

\paragraph{Algorithm~{\EGUP.}}
At a high level, we would open exactly one facility for each
primary demand $\kappa$, and each non-primary demand is
connected to the facility opened for the primary demand it
was assigned to.

More precisely, we apply a rounding process, guided by the
fractional values $(\bary_{\mu})$ and $(\barx_{\mu\nu})$,
that produces an integral solution. This integral solution
is obtained by choosing a subset of facilities in
$\facilityset$ to open, and for each demand in $\demandset$,
specifying an open facility that this demand will be
connected to.  For each primary demand $\kappa\in P$, we
want to open one facility $\phi(\kappa) \in
\wbarN(\kappa)$. To this end, we use randomization: for each
$\mu\in\wbarN(\kappa)$, we choose $\phi(\kappa) = \mu$ with
probability $\barx_{\mu\kappa}$, ensuring that exactly one
$\mu \in \wbarN(\kappa)$ is chosen. Note that
$\sum_{\mu\in\wbarN(\kappa)}\barx_{\mu\kappa}=1$, so this
distribution is well-defined.  We open this facility
$\phi(\kappa)$ and connect to $\phi(\kappa)$ all demands
that are assigned to $\kappa$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%******** Do we need the derandomization for conference version? (Li)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%In our description above, the algorithm is presented as a
%%randomized algorithm. It can be de-randomized using the
%%method of conditional expectations, which is commonly used
%%in approximation algorithms for facility location problems
%%and standard enough that presenting it here would be
%%redundant. Readers less familiar with this field are
%%recommended to consult \cite{ChudakS04}, where the method of
%%conditional expectations is applied in a context very
%%similar to ours.

%%%%%%%%%

\paragraph{Analysis.}
We now bound the expected facility cost and connection cost
by establishing the two lemmas below.

%%%%%

\begin{lemma}\label{lemma:3fac}
The expectation of facility cost $F_{\smallEGUP}$ of our solution is
  at most $F^\ast$.
\end{lemma}

%%%%%%%

\begin{lemma}\label{lemma:3dist}
The expectation of connection cost $C_{\smallEGUP}$ of our solution
is at most  $C^\ast+2\cdot\LP^\ast$.
\end{lemma}


\begin{theorem}
Algorithm~{\EGUP} is a $3$-approximation algorithm.
\end{theorem}

\begin{proof}
  By Property~(SI.\ref{SI:primary disjoint}), different
  demands from the same client are assigned to different
  primary demands, and by (PD.\ref{PD:disjoint}) each primary
  demand opens a different facility. This ensures that our
  solution is feasible, namely each client $j$ is connected
  to $r_j$ different facilities (some possibly located on
  the same site).  As for the total cost,
  Lemma~\ref{lemma:3fac} and Lemma~\ref{lemma:3dist} imply
  that the total cost is at most
  $F^\ast+C^\ast+2\cdot\LP^\ast = 3\cdot\LP^\ast \leq
  3\cdot\OPT$.
\end{proof}

%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% 1.736-APPROXIMATION ALGORITHM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\input{ECHS.tex}
\section{Algorithm~{\ECHS} with Ratio $1.736$}\label{sec: 1.736-approximation}

In this section we improve the approximation ratio to $1+2/e \approx
1.736$. The improvement comes from a slightly modified rounding
process and refined analysis.  Note that the facility opening cost of
Algorithm~{\EGUP} does not exceed that of the fractional optimum
solution, while the connection cost is quite far from the optimum,
since we connect a non-primary demand to a facility in its assigned
primary demand's neighborhood and then estimate the distance using the
triangle inequality. To improve the estimate of the connection cost,
the basic idea, following the approach of Chudak and
Shmoys~\cite{ChudakS04}, is to connect a non-primary demand to its
nearest neighbor when one is available and only use the facility that
its primary demand opens when none of its neighbor is open.

%%%%%%%%%%

\paragraph{Algorithm~{\ECHS}.}
As before,
the algorithm starts by solving the linear program and applying the
adaptive partitioning algorithm  described in 
Section~\ref{sec: adaptive partitioning} to obtain a partitioned
solution $(\barbfx, \barbfy)$. Then we apply the rounding
process to compute an integral solution (see Pseudocode~\ref{alg:lpr3}).  
For convenience, we will use
the term \emph{facility cluster} for the neighborhood of a
primary demand. Facilities that do not belong to these
clusters are said to be \emph{non-clustered}.  We start, as before, by
opening exactly one facility $\phi(\kappa)$ in the facility
cluster of each primary demand $\kappa$ (Line 2).  For any
non-primary demand $\nu$ assigned to $\kappa$, we refer to
$\phi(\kappa)$ as the \emph{target} facility of $\nu$.  In
Algorithm~{\EGUP}, $\nu$ was connected to $\phi(\kappa)$,
but in Algorithm~{\ECHS} we may be able to find an open
facility in $\nu$'s neighborhood and connect $\nu$ to this
facility. 

%%Specifically, the two changes in the
%%algorithm are as follows:
%%%
%%\begin{description}
%%	\item{(1)} Each non-clustered facility $\mu$ is opened,
%%independently, with probability $\bary_{\mu}$ (Lines
%%4--5). Notice that if $\bary_\mu>0$ then,
%%due to completeness of the partitioned
%%fractional solution, we have $\bary_{\mu}= \barx_{\mu\nu}$
%%for some demand $\nu$. This implies that $\bary_{\mu}\leq 1$,
%%because $\barx_{\mu\nu}\le 1$, by (PS.\ref{PS:one}).
%%%
%%	\item{(2)} When connecting demands to facilities, a primary demand
%%$\kappa$ is connected to the only facility $\phi(\kappa)$
%%opened in its neighborhood, as before (Line 3).  For a
%%non-primary demand $\nu$, if its neighborhood $\wbarN(\nu)$ has an open
%%facility, we connect $\nu$ to the closest open facility in $\wbarN(\nu)$
%%(Line 8). Otherwise, we connect $\nu$ to
%%its target facility (Line 10).
%%%
%%\end{description}

%%%%%%%%%%%%%

\begin{algorithm}
  \caption{Algorithm~{\ECHS}:
    Constructing Integral Solution}
  \label{alg:lpr3}
  \begin{algorithmic}[1]
    \For{each $\kappa\in P$} 
    \State choose one $\phi(\kappa)\in \wbarN(\kappa)$,
    with each $\mu\in\wbarN(\kappa)$ chosen as $\phi(\kappa)$
    with probability $\bary_\mu$ (note {$\barx_{\mu\kappa} =
      \bary_{\kappa}$ for all $\mu \in \wbarN(\kappa)$})
    \State open $\phi(\kappa)$ and connect $\kappa$ to $\phi(\kappa)$
    \EndFor
    \For{each $\mu\in\facilityset - \bigcup_{\kappa\in P}\wbarN(\kappa)$} 
    \State open $\mu$ with probability $\bary_\mu$ (independently)
    \EndFor
    \For{each non-primary demand $\nu\in\demandset$}
    \If{any facility in $\wbarN(\nu)$ is open}
    \State{connect $\nu$ to the nearest open facility in $\wbarN(\nu)$}
    \Else
    \State connect $\nu$ to $\phi(\kappa)$ where $\kappa$ is $\nu$'s
     primary demand
    \EndIf
    \EndFor
  \end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%

\paragraph{Analysis.}
We shall first argue that the integral solution thus constructed is
feasible, and then we bound the total cost of the solution. Regarding
feasibility, the only constraint that is not explicitly enforced by
the algorithm is the fault-tolerance requirement; namely that each
client $j$ is connected to $r_j$ different facilities. Let $\nu$ and
$\nu'$ be two different sibling demands of client $j$ and let their
assigned primary demands be $\kappa$ and $\kappa'$ respectively. Due
to (SI.\ref{SI:primary disjoint}) we know $\kappa \neq \kappa'$. From
(SI.\ref{SI:siblings disjoint}) we have $\wbarN(\nu) \cap \wbarN(\nu')
= \emptyset$. From (SI.\ref{SI:primary disjoint}), we have
$\wbarN(\nu) \cap \wbarN(\kappa') = \emptyset$ and $\wbarN(\nu') \cap
\wbarN(\kappa) = \emptyset$. From (PD.\ref{PD:disjoint}) we have
$\wbarN(\kappa)\cap \wbarN(\kappa') = \emptyset$. It follows that
$(\wbarN(\nu) \cup \wbarN(\kappa)) \cap (\wbarN(\nu') \cup
\wbarN(\kappa')) = \emptyset$. Since the algorithm connects $\nu$ to
some facility in $\wbarN(\nu) \cup \wbarN(\kappa)$ and $\nu'$ to some
facility in $\wbarN(\nu') \cup \wbarN(\kappa')$, $\nu$ and $\nu'$ will
be connected to different facilities. This integral solution can be
shown to have expected facility cost bounded by $F^\ast$ and
connection cost bounded by $C^\ast + (2/e)\cdot \LP^\ast$ (see
Appendix~\ref{app: 1.736 analysis}). As a result the expected total
cost is bounded by $(1 + 2/e)\cdot \LP^\ast$. Summarizing, we obtain
the main result of this section.
\begin{theorem}\label{thm:1736}
  Algorithm~{\ECHS} is a $(1+2/e)$-approximation algorithm for \FTFP.
\end{theorem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Byrka 2010 1.575
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\input{EBGS.tex}
%% NEW VERSION

\section{Algorithm~{\EBGS} with Ratio $1.575$}\label{sec: 1.575-approximation}

In this section we give our main result, a $1.575$-approximation
algorithm for $\FTFP$, where $1.575$ is the value of $\min_{\gamma\geq
  1}\max\{\gamma, 1+2/e^\gamma, \frac{1/e+1/e^\gamma}{1-1/\gamma}\}$,
rounded to three decimal digits. This matches the ratio of the best
known LP-rounding algorithm for UFL by
Byrka~{\etal}~\cite{ByrkaGS10}. Recall that in Section~\ref{sec:
  1.736-approximation} we showed how to compute an integral solution
with facility cost bounded by $F^\ast$ and connection cost bounded by
$C^\ast + 2/e\cdot\LP^\ast$. A natural idea is to balance these two
costs, by reducing the connection cost, at the expense of slightly
increasing the facility cost.

Our approach can be thought of as a combination of the ideas
in~\cite{ByrkaGS10} with the techniques of demand reduction and
adaptive partitioning that we introduced earlier. However, our
adaptive partitioning technique needs to be carefully modified because
now we will be using a more intricate neighborhood structure, with the
neighborhood of each demand divided into two parts, the close and far
neighborhood, and with some conditions on which pairs of neighborhoods
need to overlap and which need to be disjoint. The final rounding
stage that construct an integral solution is a relatively
straightforward generalization of the rounding method in
\cite{ByrkaGS10}.

We begin by describing properties that our partitioned fractional
solution $(\barbfx,\barbfy)$ needs to satisfy. The neighborhood
$\wbarN(\nu)$ of each demand $\nu$ will be divided into two disjoint
parts.  The first part, called the \emph{close neighborhood}
$\wbarclsnb(\nu)$, contains the facilities in $\wbarN(\nu)$ nearest to
$\nu$ with the total connection value equal $1/\gamma$. The second
part, called the \emph{far neighborhood} $\wbarfarnb(\nu)$, contains
the remaining facilities in $\wbarN(\nu)$. The formal definitions of
these sets are given below in Property~(NB).  The respective average
connection costs from $\nu$ for these sets are defined by
$\clsdist(\nu)=\gamma\sum_{\mu\in\wbarclsnb(\nu)}
d_{\mu\nu}\barx_{\mu\nu}$ and
$\fardist(\nu)=\frac{\gamma}{\gamma-1}\sum_{\mu\in\wbarfarnb(\nu)}
d_{\mu\nu}\barx_{\mu\nu}$. We will also use notation
$\clsmax(\nu)=\max_{\mu\in\wbarclsnb(\nu)} d_{\mu\nu}$ for the maximum
distance from $\nu$ to its close neighborhood.

Our partitioned solution $(\barbfx,\barbfy)$ must satisfy the same
partitioning and completeness properties as before, namely properties
(PS) and (CO) in Section~\ref{sec: adaptive partitioning}.  In
addition, it must satisfy new neighborhood property (NB) and modified
properties (PD') and (SI'), listed below.

\begin{description}
	
      \renewcommand{\theenumii}{(\alph{enumii})}
      \renewcommand{\labelenumii}{\theenumii}

\item{(NB)} \label{NB}
	For each demand $\nu$, its neighborhood is divided into \emph{close} and
	\emph{far} neighborhood, that is $\wbarN(\nu) = \wbarclsnb(\nu) \cup \wbarfarnb(\nu)$, where
	%
	\begin{itemize}
	\item $\wbarclsnb(\nu) \cap \wbarfarnb(\nu) = \emptyset$,
	\item $\sum_{\mu\in\wbarclsnb(\nu)} \barx_{\mu\nu} =1/\gamma$, and 
	\item if $\mu\in \wbarclsnb(\nu)$ and $\mu'\in \wbarfarnb(\nu)$ 
				then $d_{\mu\nu}\le d_{\mu'\nu}$.   
	\end{itemize}
	%
	Note that the second condition, together with (PS.\ref{PS:one}), implies
	that $\sum_{\mu\in\wbarfarnb(\nu)} \barx_{\mu\nu} =1-1/\gamma$.

\item{(PD')} \emph{Primary demands.}
	Primary demands satisfy the following conditions:

	\begin{enumerate}
		
	\item\label{PD1:disjoint}  For any two different primary demands $\kappa,\kappa'\in P$ we have
				$\wbarclsnb(\kappa)\cap \wbarclsnb(\kappa') = \emptyset$.

	\item \label{PD1:yi} For each site $i\in\sitesset$, 
		$ \sum_{\kappa\in P}\sum_{\mu\in i\cap\wbarclsnb(\kappa)}\barx_{\mu\kappa} \leq y_i^\ast$.
		
	\item \label{PD1:assign} Each demand $\nu\in\demandset$ is assigned
        to one primary demand $\kappa\in P$ such that

  			\begin{enumerate}
	
				\item \label{PD1:assign:overlap} $\wbarclsnb(\nu) \cap \wbarclsnb(\kappa) \neq \emptyset$, and
				%
				\item \label{PD1:assign:cost}
          $\clsdist(\nu)+\clsmax(\nu) \geq
          \clsdist(\kappa)+\clsmax(\kappa)$.
          %
			\end{enumerate}

	\end{enumerate}
	
\item{(SI')} \emph{Siblings}. For any pair $\nu,\nu'$ of different siblings we have
  \begin{enumerate}

	\item \label{SI1:siblings disjoint}
		  $\wbarN(\nu)\cap \wbarN(\nu') = \emptyset$.
		
	\item \label{SI1:primary disjoint} If $\nu$ is assigned to a primary demand $\kappa$ then
 		$\wbarN(\nu')\cap \wbarclsnb(\kappa) = \emptyset$. In particular, by Property~(PD.\ref{PD1:assign:overlap}),
		this implies that different sibling demands are assigned to different primary demands.

	\end{enumerate}
	
\end{description}

%%%%%%%%%%%%%%%%%

\paragraph{Modified adaptive partitioning.}
To obtain a fractional solution with the above properties,
we employ a modified adaptive partitioning algorithm. As
in Section~\ref{sec: adaptive partitioning}, we have two phases.
In Phase~1 we split clients into demands and create facilities on
sites, while in Phase~2 we augment each demand's
connection values so that its total value is $1$.

Phase~1 runs in iterations. Consider any client $j$.  As before,
$\wtildeN(j)$ is the neighborhood of $j$ with respect to the yet
unpartitioned solution, namely the set of facilities $\mu$ such that
$\tildex_{\mu j}>0$. Order the facilities in this set as
$\wtildeN(j) = \braced{\mu_1,...,\mu_q}$ in order of non-decreasing
distance from $j$, that is
$d_{\mu_1 j} \leq d_{\mu_2 j} \leq \ldots \leq d_{\mu_q j}$, where
$q = |\wtildeN(j)|$. Without loss of generality, there is an index
$l$ for which $\sum_{s=1}^l \tildex_{s j} = 1/\gamma$, since we can
always split one facility to have this property. Then we define
$\wtildeN_{\gamma}(j) = \braced{\mu_1,...,\mu_l}$. We also use notation
%
\begin{equation*}
\tcc_\gamma(j) =  D(\wtildeN_\gamma(j), j) = \sum_{\mu\in\wtildeN_{\gamma}(j)} d_{\mu j} \tildex_{\mu j}
			\quad\textrm{ and }\quad
 \dmax_\gamma(j) = \max_{\mu \in \wtildeN_{\gamma}(j)} d_{\mu j}. 
\end{equation*}
%

In each iteration, we find a not yet exhausted client $p$ that minimizes the
value of $\tcc_\gamma(p) + \dmax_\gamma(p)$. Now we have two cases:

\begin{description}

\item{\mycase{1}:} $\wtildeN_{\gamma}(p) \cap
  \wbarclsnb(\kappa)\neq\emptyset$, for some existing primary demand
  $\kappa$.  In this case we assign $\nu$ to $\kappa$. As before, if
  there are multiple such $\kappa$, we pick any of them. We also fix
  $\barx_{\mu \nu} \assign \tildex_{\mu p}, \tildex_{\mu p}\assign 0$
  for each $\mu \in \wtildeN(p)\cap \wbarclsnb(\kappa)$. As before,
  although we check for overlap between $\wtildeN_{\gamma}(p)$ and
  $\wbarclsnb(\kappa)$, the facilities we actually move into
  $\wbarN(\nu)$ include all facilities in the intersection of
  $\wtildeN(p)$, a bigger set, with $\wbarclsnb(\kappa)$. We would
  like to point out that $\wbarN(\nu)$ is not finalized at this time
  as we will add more facilities to it in the augment phase. As a
  result $\wbarclsnb(\nu)$ is not fixed either, as we could
  potentially add facilities closer to $\nu$ than facilities already
  in $\wbarN(\nu)$. Recall that by definition $\wbarclsnb(\nu)$
  consists of the facilities that closest to $\nu$ once $\wbarN(\nu)$
  is fixed with total connection value of $1$.

\item{\mycase{2}:} $\wtildeN_{\gamma}(p) \cap \wbarclsnb(\kappa) =
  \emptyset$, for all existing primary demands $\kappa$.  In this case
  we make $\nu$ a primary demand. We then fix $\barx_{\mu \nu}\assign
  \tildex_{\mu p}$ for $\mu \in \wtildeN_{\gamma}(p)$ and set the
  corresponding $\tildex_{\mu p}$ to $0$.  Note that the total
  connection value in $\wbarclsnb(\nu)$ is now exactly $1/\gamma$.
  The set $\wtildeN_{\gamma}(p)$ turns out to coincide with
  $\wbarclsnb(\nu)$ as the facilities in $\wtildeN(p) \setminus
  \wtildeN_{\gamma}(p)$ are all farther away than any facilitity in
  $\wtildeN_{\gamma}(p)$. In the augmenting phase, Phase 2, we have
  available only facilities in some subset of $\wtildeN(p) \setminus
  \wtildeN_{\gamma}(p)$. Thus $\wbarclsnb(\nu)$ is defined when $\nu$
  is created.
\end{description}

Once all clients are exhausted, that is, each client $j$ has $r_j$
demands created, Phase~1 concludes. We then do Phase~2, the augmenting
phase.  For each demand $\nu$ of client $j$ with total connection
value less than $1$, we use our $\AugmentToUnit()$ procedure to add
additional facilities from $\wtildeN(j)$ to $\nu$'s neighborhood to
make its total connection value equal $1$, as before a facility is
removed from $\wtildeN(j)$ once added to a demand's neighborhood. We
do facility split if necessary to make $\wbarN(\nu)$ have total
connection value of 1.  This completes the description of the
partitioning algorithm.

\smallskip

We now argue that the fractional solution $(\barbfx,\barbfy)$
satisfies all the stated properties. Properties~(PS), (CO), (NB)
(PD'.\ref{PD1:disjoint}) and (SI'.\ref{SI1:siblings disjoint}) are
directly enforced by the adaptive partitioning algorithm. The proofs
for other properties (PD'.\ref{PD1:yi}), (PD'.\ref{PD1:assign:cost})
and (SI'.\ref{SI1:primary disjoint}) are similar to those in
Section~\ref{sec: adaptive partitioning}, with the exception of
(PD.\ref{PD:assign:overlap}), which we justify below.

The argument for (PD.\ref{PD:assign:overlap}) is a bit subtle, because
of possible complications arising in the augmenting phase, Phase 2.
This phase does not change close neighborhoods of primary demands, as
each primary demand already contains all the nearest facilities with
total connection value $1/\gamma$.  For non-primary demands, however,
$\wbarN(\nu)$, for $\nu\in j$, takes all facilities in
$\wbarclsnb(\kappa)\cap \wtildeN(j)$, which might be close to $\kappa$
but far from $j$.  It seems that facilities added in the augment step
might actually be closer to $\nu$ than some of the facilities already
in $\wbarN(\nu)$. As a result, facilities added in the augmenting
phase, Phase 2, might appear in $\wbarclsnb(\nu)$, yet they are not in
$\wbarclsnb(\kappa)$, the close neighborhood of the primary demand
$\kappa$ that $\nu$ is assigned to.  Nevertheless, we show that
Property~(PD.\ref{PD:assign:overlap}) holds.

Consider an iteration when we create a demand $\nu\in p$
and assign it to $\kappa$. Then the set
$B(p)=\wtildeN_{\gamma}(p)\cap \wbarclsnb(\kappa)$ is not empty.
We claim that
$B(p)$ must be a subset of $\wbarclsnb(\nu)$ after $\wbarN(\nu)$ is
finalized with a total connection value of $1$. To see this, first
observe that $B(p)$ is a subset of $\wbarN(\nu)$, which in turn is a
subset of $\wtildeN(p)$, after taking into account the facility
split. Here $\wtildeN(p)$ refers to the neighborhood of client $p$
just before $\nu$ was created. For an arbitrary set of facilities
$A$ define $\dmax(A, \nu)$ as the minimum distance $\tau$ such
that $\sum_{\mu\in A \suchthat d_{\mu\nu} \leq \tau}\;\bary_{\mu} \geq
1/\gamma$.
Adding additional facilities into $A$ cannot make
$\dmax(A, \nu)$ larger, so it follows that $\dmax(\wbarclsnb(\nu), \nu)
\geq \dmax(\wtildeN(p), \nu)$, because $\wbarclsnb(\nu)$ is a subset of
$\wtildeN(p)$. Since we have $d_{\mu \nu} = d_{\mu p}$ by definition,
it is easy to see that every $\mu \in B(p)$ satisfies $d_{\mu \nu}
\leq \dmax(\wtildeN(p), \nu) \leq \dmax(\wbarclsnb(\nu), \nu)$ and
hence they all belong to $\wbarclsnb(\nu)$. We need to be a bit more
careful here when we have a tie in $d_{\mu\nu}$ but we can assume ties
are always broken in favor of facilities in $B(p)$ when defining
$\wbarclsnb(\nu)$. Finally, since $B(p)\neq\emptyset$, we have that the
close neighborhood of a demand $\nu$ and its primary demand $\kappa$
must overlap.

%%%%%%%%

\paragraph{Algorithm~{\EBGS}.}
The complete algorithm starts with solving the linear program and
computing the partitioning described earlier in this section.
Given the partitioned fractional solution $(\barbfx,
\barbfy)$ with the desired properties, we then start opening
facilities and making connections to obtain an integral
solution. As before, we open exactly one facility in each
cluster (the close neighborhood of a primary demand), but
now each facility $\mu$ is chosen with probability
$\gamma\bary_{\mu}$. The non-clusterd facilities $\mu$,
those that do not belong to $\wbarN_{\cls}(\kappa)$ for any
primary demand $\kappa$, are opened independently with
probability $\gamma\bary_{\mu}$ each. 

Next, we connect demands to facilities.
Each primary demand $\kappa$ will connect
to the only facility $\phi(\kappa)$ open in its cluster
$\wbarclsnb(\kappa)$.  For each non-primary demand $\nu$, if
there is an open facility in $\wbarN(\nu)$ then we connect
$\nu$ to the nearest such facility. Otherwise, we connect
$\nu$ to its \emph{target facility} $\phi(\kappa)$, where $\kappa$ is the primary
demand that $\nu$ is assigned to. 

%%%%%%%%%%%

\paragraph{Analysis.}
The feasibility of our integral solution follows from
Properties~(SI.\ref{SI1:siblings disjoint}), (SI.\ref{SI1:primary
  disjoint}), and (PD.\ref{PD1:disjoint}), as these properties together
ensure that each facility is accessible to at most one demand among
sibling demands of the same client, regardless whether a demand
connects to its neighbor or its target facility.

The expected facility cost of our algorithm is bounded by $\gamma
F^\ast$, using essentially the same argument as in the previous
section (with the the factor $\gamma$ accounting for using
probabilities $\gamma \bary_{\mu}$ instead of $\bary_{\mu}$). The
expected connection cost can be bounded by $C^\ast
\max\{\frac{1/e+1/e^\gamma}{1-1/\gamma}, 1+\frac{2}{e^\gamma}\}$ (see
Appendix~\ref{app:EBGS}).  Hence the total cost is bounded by
$\max\{\gamma, \frac{1/e+1/e^\gamma}{1-1/\gamma},
1+\frac{2}{e^\gamma}\}\cdot \LP^\ast$. Picking $\gamma=1.575$ we
obtain the desired ratio.

\begin{theorem}\label{thm:ebgs}
  Algorithm~{\EBGS} is a $1.575$-approximation algorithm for \FTFP.
\end{theorem}





%%%%%%%%%%%%%%%%%%%%%%

\section{Final Comments}

In this paper we show a sequence of LP-rounding approximation algorithms
for FTFP, with the best algorithm achieving  ratio $1.575$. The two techniques we introduced,
namely the demand reduction and adaptive partitioning, are very flexible. Should any new
LP-rounding algorithms be discovered for UFL, we believe that with our approach they can be
adapted to FTFP as well, preserving the approximation ratio. In fact, by randomizing the
scaling parameter $\gamma$ from Section~\ref{sec: 1.575-approximation},
following the approach by Li~\cite{Li11}, we
could further improve the ratio to below $1.575$. This is not enough, however, to
match the $1.488$ bound for UFL in~\cite{Li11}, because matching this bound
also requires appropriately extending dual-fitting algorithms~\cite{MahdianYZ06}
to FTFP, which we have so far been unable to do.

One of the main open problems in this area is whether FTFL can be approximated with the
same ratio as UFL, and our work was partly motivated by this question. The techniques we
introduced are not directly applicable to FTFL, mainly because our partitioning
approach involves facility splitting that could result in several sibling demands being served
by facilities on the same site. Nonetheless, we hope that further refinements of 
our construction might get around this issue and
lead to new algorithms for FTFL with improved ratios.

\vfill
\eject

\bibliographystyle{plain}
\bibliography{facility}
%\printbibliography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\section{Proof of Lemmas}
\begin{proof}[Proof of Lemma~\ref{lem: polynomial demands partition}]
(i) For feasibility, we need to verify that the constraints of LP~(\ref{eqn:fac_primal})
are satisfied. Directly from the definition, we have $\hatr_j = \sum_{i\in\sitesset} \hatx_{ij}$.
For any $i$ and $j$, by the feasibility of $(\bfx^\ast,\bfy^\ast)$ we have
$\hatx_{ij} = \floor{x_{ij}^\ast} \le \floor{y^\ast_i} = \haty_i$.

(ii) From the definition, we have  $\dotr_j = \sum_{i\in\sitesset} \dotx_{ij}$.
It remains to show that $\doty_i \geq \dotx_{ij}$ for all $i,j$. 
If $x_{ij}^\ast=0$, then $\dotx_{ij}=0$ and we are done. 
Otherwise, by completeness, we have $x_{ij}^\ast=y_i^\ast$. 
Then  $\doty_i = y_i^\ast - \floor{y_i^\ast} = x_{ij}^\ast - \floor{x_{ij}^\ast} =\dotx_{ij}$. 

(iii) From the definition of $\dotx_{ij}$ we have
  $\dotx_{ij} < 1$.  Then the bound follows from the definition of $\dotr_j$.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem: property SI:primary disjoint
    holds}]
  Let $\nu_1,\ldots,\nu_{r_j}$ be the demands of a client
  $j\in\clientset$, listed in the order of creation, and, for each
  $q=1,2,\ldots,r_j$, denote by $\kappa_q$ the primary demand that
  $\nu_q$ is assigned to. After the completion of Phase~1 of
  Pseudocode~\ref{alg:lpr2} (Lines 5--18), we have
  $\wbarN(\nu_s)\subseteq \wbarN(\kappa_s)$ for  $s=1,\ldots,r_j$. 
Since any two primary demands have disjoint
  neighborhoods, we have $\wbarN(\nu_s) \cap \wbarN(\kappa_q) =
  \emptyset$ for any $s\neq q$, that is
	Property~(SI.\ref{SI:primary disjoint}) holds right after Phase~1.

        After Phase~1 all neighborhoods $\wbarN(\kappa_s),
        s=1,\ldots,r_j$ have already been fixed and they do not change
        in Phase~2.  None of the facilities in $\wtildeN(j)$ appear in
        any of $\wbarN(\kappa_s)$ for $s=1,\ldots,r_j$, by the way we
        allocate facilities in Lines~13 and 16.  Therefore during the
        augmentation process in Phase~2, when we add facilities from
        $\wtildeN(j)$ to $\wbarN(\nu)$, for some $\nu\in j$
        (Line~19--21 of Pseudocode~\ref{alg:lpr2}), all the required
        disjointness conditions will be preserved.
\end{proof}

%%%%%%%

\begin{proof}[Proof of Lemma~\ref{lem: tcc optimal}]
  We focus first on the time when demand $\eta$ is about to be created,
  right after the call to $\NearestUnitChunk()$ in
  Pseudocode~\ref{alg:lpr2}, Line~7.  Let $\wtildeN(j) =
  \{\mu_1,...,\mu_q\}$ with all facilities $\mu_s$ ordered
  according to nondecreasing distance from $j$.  Consider
  the following linear program:
%
\begin{alignat*}{1}
	\textrm{minimize} \quad & \sum_s d_{\mu_s j}z_s
			\\
	\textrm{subject to} \quad & \sum_s z_s  \ge 1
			\\
 	0 &\le z_s \le \tildex_{\mu_s j} \quad \textrm{for all}\ s
\end{alignat*}
%
  This is a fractional
  minimum knapsack covering problem (with knapsack size equal $1$) and its optimal fractional
  solution is the greedy solution, whose value is exactly
  $\tcc_\eta(j)$.  

On the other hand, we claim that
  $\tcc_\nu(j)$ can be thought of as the value of some feasible
  solution to this linear program, and that the same is true for $\concost_{\nu}$ if $\nu\in j$.
  Indeed, each of these
  quantities involves some later values $\tildex_{\mu j}$,
  where $\mu$ could be one of the facilities $\mu_s$ or a
  new facility obtained from splitting. For each $s$,
  however, the sum of all values $\tildex_{\mu j}$,
  over the facilities $\mu$ that were split from $\mu_s$, cannot exceed
 the value $\tildex_{\mu_s j}$ at the time when
  $\eta$ was created, because splitting facilities preserves this sum and
 creating new demands for $j$ can only decrease it.
Therefore both quantities
  $\tcc_\nu(j)$ and $\concost_{\nu}$ (for $\nu\in j$) correspond to some
  choice of the $z_s$ variables (adding up to $1$), and the
  lemma follows.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:3fac}]
  By Property~(PD.\ref{PD:disjoint}), the neighborhoods of
  primary demands are disjoint. Also, for any primary demand
  $\kappa\in P$, the probability that a facility
  $\mu\in\wbarN(\kappa)$ is chosen as the open facility
  $\phi(\kappa)$ is $\barx_{\mu\kappa}$. Hence the expected
  total facility cost is
%
\begin{equation*}
    \Exp[F_{\smallEGUP}]
	= \textstyle{\sum_{\kappa\in P}\sum_{\mu\in\wbarN(\kappa)}} f_{\mu} \barx_{\mu\kappa}
	= \textstyle{\sum_{\kappa\in P}\sum_{\mu\in\facilityset}} f_{\mu} \barx_{\mu\kappa} 
	= \textstyle{\sum_{i\in\sitesset}} f_i \textstyle{\sum_{\mu\in i}\sum_{\kappa\in P}} \barx_{\mu\kappa} 
	\leq \textstyle{\sum_{i\in\sitesset}} f_i y_i^\ast 
	= F^\ast,
\end{equation*}
%
where the inequality follows from Property~(PD.\ref{PD:yi}).
\end{proof}

%%%%%%%%%%
\begin{proof}[Proof of Lemma~\ref{lemma:3dist}]
  For a primary demand $\kappa$, its expected connection cost is
  $C_{\kappa}^{\avg}$ because we choose facility $\mu$ with
  probability $\barx_{\mu\kappa}$.

  Consider a non-primary demand $\nu$ assigned to a primary demand
  $\kappa\in P$. Let $\mu$ be any facility in $\wbarN(\nu) \cap
  \wbarN(\kappa)$.  Since $\mu$ is in both $\wbarN(\nu)$ and
  $\wbarN(\kappa)$, we have $d_{\mu\nu} \leq \alpha_{\nu}^\ast$ and
  $d_{\mu\kappa} \leq \alpha_{\kappa}^\ast$ (This follows from the
  complementary slackness conditions since
  $\alpha_{\nu}^\ast=\beta_{\mu\nu}^\ast + d_{\mu\nu}$ for each
  $\mu\in \wbarN(\nu)$.). Thus, applying the triangle inequality, for
  any fixed choice of facility $\phi(\kappa)$ we have
%
\begin{equation*}
    d_{\phi(\kappa)\nu} \leq d_{\phi(\kappa)\kappa}+d_{\mu\kappa}+d_{\mu\nu}
    \leq d_{\phi(\kappa)\kappa} + \alpha_{\kappa}^\ast + \alpha_{\nu}^\ast.
\end{equation*}
%
Therefore the expected distance from $\nu$ to its facility $\phi(\kappa)$ is 
%
\begin{equation*}
  \Exp[  d_{\phi(\kappa)\nu}   ] \le \concost_{\kappa} + \alpha_{\kappa}^\ast + \alpha_{\nu}^\ast 
  \leq \concost_{\nu} + \alpha_{\nu}^\ast + \alpha_{\nu}^\ast
   = \concost_{\nu} + 2\alpha_{\nu}^\ast,
\end{equation*}
%
  where the second inequality follows from Property~(PD.\ref{PD:assign:cost}).  
From the definition of $\concost_{\nu}$ and Property~(PS.\ref{PS:xij}), for any $j\in \clientset$ 
we have
%
\begin{equation*}
\sum_{\nu\in j} \concost_{\nu} = \sum_{\nu\in j}\sum_{\mu\in\facilityset}d_{\mu\nu}\barx_{\mu\nu}
 			= \sum_{i\in\sitesset} d_{ij}\sum_{\nu\in j}\sum_{\mu\in i}\barx_{\mu\nu}
			= \sum_{i\in\sitesset} d_{ij}x^\ast_{ij} 
			= C^\ast_j.
\end{equation*}
% 
Thus, summing over all demands, the expected total connection cost is
%
\begin{equation*}
    \Exp[C_{\smallEGUP}] \le 
			\textstyle{\sum_{j\in\clientset} \sum_{\nu\in j}} (\concost_{\nu} + 2\alpha_{\nu}^\ast) 
    	= \textstyle{\sum_{j\in\clientset}} (C_j^\ast + 2r_j\alpha_j^\ast)
 		= C^\ast + 2\cdot\LP^\ast,
\end{equation*}
%
completing the proof of the lemma.
\end{proof}

%%%%%%%%

\section{Helper Function for Adaptive Partitioning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% subroutine: NearestUnitChunk and AugmentToUnit
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[ht]
  \caption{Helper functions used in Pseudocode~\ref{alg:lpr2}}
  \label{alg:helper}
  \begin{algorithmic}[1]
    \Function{\NearestUnitChunk}{$j, \facilityset, \tildebfx, \barbfx,\barbfy$}		
						\Comment upon return, $\sum_{\mu\in\wtildeN_1(j)} \tildex_{\mu j} = 1$
    \State Let $\wtildeN(j) = \{\mu_1,...,\mu_{q}\}$ where $d_{\mu_1 j} \leq d_{\mu_2 j} \leq \ldots \leq d_{\mu_{q j}}$
    \State Let $l$ be such that $\sum_{k=1}^{l} \bary_{\mu_k} \geq 1$ and $\sum_{k=1}^{l -1} \bary_{\mu_{k}} < 1$
    \State Create a new facility $\sigma$ at the same site as $\mu_l$ and add it to $\facilityset$
			\Comment split $\mu_l$
    \State Set $\bary_{\sigma}\assign \sum_{k=1}^{l} \bary_{\mu_{k}}-1$
					and $\bary_{\mu_l} \assign \bary_{\mu_l} - \bary_{\sigma}$
    \State For each $\nu\in\demandset$ with $\barx_{\mu_{l}\nu}>0$
 			set $\barx_{\mu_{l}\nu} \assign \bary_{\mu_l}$ and $\barx_{\sigma \nu} \assign \bary_{\sigma}$
    \State For each $j'\in\clientset$ with $\tildex_{\mu_{l} j'}>0$ (including $j$)
			set $\tildex_{\mu_l j'} \assign \bary_{\mu_l}$ and $\tildex_{\sigma j'} \assign \bary_\sigma$
	\State (All other new connection values are set to $0$)
    \State \Return $\wtildeN_1(j) = \{\mu_{1},\ldots,\mu_{l-1}, \mu_{l}\}$    				
    \EndFunction

    \Function{\AugmentToUnit}{$\nu, j, \facilityset, \tildebfx, \barbfx, \barbfy$}
    					\Comment $\nu$ is a demand of client $j$
    \While{$\sum_{\mu\in \facilityset} \barx_{\mu\nu} <1$}
    					\Comment upon return, $\sum_{\mu\in\wbarN(\nu)} \barx_{\mu\nu} = 1$
    \State Let $\eta$ be any facility such that $\tildex_{\eta j} > 0$
    \If{$1-\sum_{\mu\in \facilityset} \barx_{\mu\nu} \geq \tildex_{\eta j}$}
    \State $\barx_{\eta\nu} \assign \tildex_{\eta j}, \tildex_{\eta j} \assign 0$
    \Else
    \State Create a new facility $\sigma$ at the same site as $\eta$ and add it to $\facilityset$
    					\Comment split $\eta$
    \State Let $\bary_\sigma \assign 1-\sum_{\mu\in \facilityset} \barx_{\mu\nu}, \bary_{\eta} \assign \bary_{\eta} - \bary_{\sigma}$
    \State Set $\barx_{\sigma\nu}\assign \bary_{\sigma},\; \barx_{\eta \nu} \assign  0,\; \tildex_{\eta j} \assign \bary_{\eta}, \; \tildex_{\sigma j} \assign 0$
    \State For each $\nu' \neq \nu$ with $\barx_{\eta \nu'}>0$ set $\barx_{\eta \nu'} \assign \bary_{\eta},\; \barx_{\sigma \nu'} \assign \bary_{\sigma}$
    \State For each $j' \neq j$ with $\tildex_{\eta j'}>0$ set $\tildex_{\eta j'} \assign \bary_{\eta}, \tildex_{\sigma j'} \assign \bary_{\sigma}$
	\State  (All other new connection values are set to $0$)
    \EndIf
    \EndWhile
    \EndFunction
  \end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%
\section{Approximation Ratio Analysis of Algorithm ECHS}
\label{app: 1.736 analysis}
In this section we show that the expected facility cost is bounded by
$F^\ast$ and the expected connection cost is bounded by $C^\ast +
(2/e) \cdot \LP^\ast$. As a result the expected total cost is bounded
by $(1 + 2/e)\cdot \LP^\ast$.

By (PD.\ref{PD:disjoint}), every facility may appear in at most one
primary demand's neighborhood, and the facilities open in Line~4--5 of
Pseudocode~\ref{alg:lpr3} do not appear in any primary demand's
neighborhood. Therefore, by linearity of expectation, the expected
facility cost of Algorithm~{\ECHS} is
%
\begin{equation*}
\Exp[F_{\smallECHS}] 
	= \sum_{\mu\in\facilityset} f_\mu \bary_{\mu} 
	= \sum_{i\in\sitesset} f_i\sum_{\mu\in i} \bary_{\mu} 
	= \sum_{i\in\sitesset} f_i y_i^\ast = F^\ast,
\end{equation*}
%
where the third equality follows from (PS.\ref{PS:yi}).

\smallskip

To bound the connection cost, we adapt an argument of Chudak
and Shmoys~\cite{ChudakS04}. Consider a demand $\nu$. This
demand can either get connected directly to some facility in
$\wbarN(\nu)$ or indirectly to its target facility $\phi(\kappa)\in
\wbarN(\kappa)$, where $\kappa$ is the primary demand to
which $\nu$ is assigned.

We now estimate the expected cost $d_{\phi(\kappa)\nu}$ of the indirect
connection. Let $\Lambda_\nu$ denote the event that none of the
facilities in $\wbarN(\nu)$ is opened. Then
%
\begin{equation*}
	\Exp[ d_{\phi(\kappa)\nu} | \Lambda_\nu] 
			= D(\wbarN(\kappa) \setminus \wbarN(\nu), \nu),
\end{equation*}
%
where $D(A,\sigma) \stackrel{\mathrm{def}}{=}\sum_{\mu\in A}
d_{\mu\sigma}\bary_{\mu}/\sum_{\mu\in A} \bary_{\mu}$, for
any set $A$ of facilities and a demand $\sigma$.
Note that $\concost_\nu = D(\wbarN(\nu),\nu)$,
and that $\Lambda_\nu$ implies that $\wbarN(\kappa) \setminus \wbarN(\nu)\neq\emptyset$,
since $\wbarN(\kappa)$ contains at least one open facility, namely $\phi(\kappa)$.

%%%%%%%%%%

\begin{lemma}
  \label{lem:echu indirect}
  Let $\nu$ be a demand assigned to a primary demand $\kappa$, and
assume that $\wbarN(\kappa) \setminus \wbarN(\nu)\neq\emptyset$.
Then $\Exp[ d_{\phi(\kappa)\nu} | \Lambda_\nu]  \leq
  		\concost_\nu+2\alpha_{\nu}^\ast$.
\end{lemma}

\begin{proof}
  By the discussion above, we need to show that $D(\wbarN(\kappa)
  \setminus \wbarN(\nu), \nu) \leq \concost(\nu) +
  2\alpha_{\nu}^\ast$. There are two cases to consider.

\begin{description}
%	
\item{\mycase{1}}
	 There exists some $\mu'\in \wbarN(\kappa) \cap
  \wbarN(\nu)$ such that $d_{\mu' \kappa} \leq \concost_\kappa$.
In this case, for every $\mu\in \wbarN(\kappa)\setminus \wbarN(\nu)$, we have
%
\begin{equation*}
d_{\mu \nu} \leq d_{\mu \kappa} + d_{\mu' \kappa} + d_{\mu' \nu}  
 	\le  \alpha^\ast_\kappa + \concost_\kappa + \alpha^\ast_{\nu}
  \leq \concost_\nu + 2\alpha_{\nu}^\ast,
\end{equation*}
%
using the triangle inequality, complementary slackness, and (PD.\ref{PD:assign:cost}).
By summing over all $\mu\in \wbarN(\kappa) \setminus \wbarN(\nu)$, it
follows that $D(\wbarN(\kappa) \setminus \wbarN(\nu), \nu) \leq
\concost_\nu + 2\alpha_{\nu}^\ast$.

\item{\mycase{2}}
 Every $\mu'\in \wbarN(\kappa)\cap \wbarN(\nu)$
has $d_{\mu'\kappa} > \concost_\kappa$. Then we
have $D(\wbarN(\kappa) \setminus \wbarN(\nu),\kappa)\leq
D(\wbarN(\kappa),\kappa)=\concost_{\kappa}$. Therefore,
choosing an arbitrary $\mu'\in \wbarN(\kappa)\cap \wbarN(\nu)$,
we obtain
%
\begin{equation*}
  D(\wbarN(\kappa) \setminus \wbarN(\nu), \nu) 
	\leq  D(\wbarN(\kappa) \setminus \wbarN(\nu), \kappa) 
			+ d_{\mu' \kappa} + d_{\mu' \nu} 
	\leq  \concost_{\kappa} +
  \alpha_{\kappa}^\ast + \alpha_{\nu}^\ast
	\leq \concost_\nu + 2\alpha_{\nu}^\ast,
\end{equation*}
%
where we again use the triangle inequality,
complementary slackness, and  (PD.\ref{PD:assign:cost}).
%
\end{description}
%
Since the lemma holds in both cases, the proof is now complete.
\end{proof}

We now continue our estimation of the connection cost. We note first
that for a primary demand $\kappa$, its expected connection cost
is $C^\avg_{\kappa} = \sum_{\mu\in\wbarN(\kappa)}
d_{\mu\kappa}\barx_{\mu\kappa}$ as in the previous section.

Next, we consider a non-primary demand $\nu$.  Denote by $C_\nu$ the
random variable representing the connection cost for $\nu$. We first
deal with the case when $\wbarN(\kappa)\setminus \wbarN(\nu) =
\emptyset$, which is the same as $\wbarN(\kappa) \subseteq
\wbarN(\nu)$. This actually implies that $\wbarN(\kappa) =
\wbarN(\nu)$ because Property (CO) implies that $\barx_{\mu\nu} =
\bary_{\mu} = \barx_{\mu\kappa}$ for every $\mu \in \wbarN(\kappa)$
and we have $\sum_{\mu\in\wbarN(\kappa)} \barx_{\mu\nu} = 1$ due to
(PS.\ref{PS:one}). Thus $\barx_{\mu\nu} = \bary_{\mu} =
\barx_{\mu\kappa}$ for $\mu \in \wbarN(\kappa)$ and $\barx_{\mu\nu} =
0$ for $\mu\in \facilityset \setminus \wbarN(\kappa)$. As a result we
have $\Exp[C_{\nu}] = \concost_{\nu}$.

Now we bound the expected connection cost of $\nu$ when
$\wbarN(\kappa)\setminus \wbarN(\nu)\neq\emptyset$.  Let $\wbarN(\nu)
= \braced{\mu_1,\ldots,\mu_l}$ and let $d_s = d_{\mu_s\nu}$ and $y_s =
\bary_{\mu_s}$ for $s = 1,\ldots,l$. By reordering, we can assume that
$d_1 \le d_2 \le \ldots \le d_l$.  By Pseudocode~\ref{alg:lpr3}, the
connection cost is no more than that obtained through the random
process that opens each $\mu_s$ independently with probability $y_{s}$
(note that $\barx_{\mu_s \nu} = y_{s}$ because $\barx_{\mu_s \nu} > 0$
and by (CO)), and connects $\nu$ to the nearest such open facility, if
any of them opens; otherwise $\nu$ is connected indirectly to its
target facility $\phi(\kappa)$. The intuition is that we only use a
facility $\mu_s$ if none of $\mu_1,\ldots,\mu_{s-1}$ is open. We know
that $\mu_s$ opens (unconditionally) with probability $y_{s}$. The
only way that some of $\mu_1,\ldots,\mu_{s-1}$ can affect that
probability is if they belong to $\wbarN(\kappa)$ for some primary
demand $\kappa$ and it also happens to be that $\mu_s \in
\wbarN(\kappa)$ as well. However in this case, the condition that they
are closed actually implies that the conditional probability of
$\mu_s$ being open is larger than $y_{s}$. (For a detailed proof,
see~\cite{ChudakS04}.)

Putting it all together, we estimate the (unconditional) expected 
connection cost of $\nu$ as follows:
%
\begin{align*}
  \Exp[C_\nu] &\leq 
	\sum_{r=1}^l d_ry_r\prod_{s=1}^{r-1}(1-y_s)
		+  \Exp[ d_{\phi(\kappa)\nu} |\Lambda_\nu]\cdot \textstyle{\prod_{s=1}^{l}} (1-y_{s})
		\\
  &\leq \Big(1 - \prod_{s=1}^l (1-y_s)\Big) \cdot \sum_{r=1}^l d_r y_r
	+  \Exp[ d_{\phi(\kappa)\nu} |\Lambda_\nu ]\cdot \textstyle{\prod_{s=1}^{l}} (1-y_{s})
	\\
  &\leq (1-\frac{1}{e}) {\textstyle\sum_{r=1}^l} d_ry_r 
	+ \frac{1}{e} \Exp[ d_{\phi(\kappa)\nu} |\Lambda_\nu ]
   \leq (1-\frac{1}{e}) \concost_{\nu} 
	+	\frac{1}{e}	(\concost_{\nu} + 2\alpha_{\nu}^\ast) = \concost_{\nu} + \frac{2}{e}\alpha_{\nu}^\ast,
\end{align*}
%
where the second inequality is shown in the appendix
(see also \cite{ChudakS04}) and the last inequality follows from
Lemma~\ref{lem:echu indirect}. 

Summing over all demands of a client $j$, we bound
the expected connection cost of client $j$:
%
\begin{equation*}
  \Exp[C_j] \leq {\textstyle\sum_{\nu\in j} (\concost_{\nu} + \frac{2}{e}\alpha_{\nu}^\ast) }
  = \textstyle{ C_j^\ast + \frac{2}{e}r_j\alpha_j^\ast}.
\end{equation*}
%
Finally, summing over all clients $j$, we obtain our bound on
the expected connection cost,
%
\begin{equation*}
	 \Exp[ C_{\smallECHS}] \le C^\ast +
\frac{2}{e}\LP^\ast.
\end{equation*}
% 
Therefore, we have established that
our algorithm constructs a feasible integral solution with
an overall expected cost 
%
\begin{equation*}
  \label{eq:chudakall}
	 \Exp[ F_{\smallECHS} + C_{\smallECHS}]
	\le
  	F^\ast + C^\ast + \frac{2}{e}\cdot \LP^\ast = (1+2/e)\cdot \LP^\ast
  \leq (1+2/e)\cdot \OPT.
\end{equation*}
%

\section{Connection Cost of Algorithm~{\EBGS}}
\label{app:EBGS}
We now bound the connection
cost. Properties~(PD.\ref{PD1:assign:overlap}) and
(PD.\ref{PD1:assign:cost}) allow us to bound the expected
distance from a demand $\nu$ to its target facility by
$\clsdist(\nu)+\clsmax(\nu)+\fardist(\nu)$, in the event
that none of $\nu$'s neighbors opens, using a similar
argument as Lemma 2.2 in~\cite{ByrkaGS10}~\footnote{The full
  proof of the lemma appears in~\cite{ByrkaA10} as
  Lemma~3.3.}. We are then able to estimate the expected
connection cost for demand $\nu$ using an argument similar
to~\cite{ByrkaGS10}: with probability
no less than $1-1/e$, $\nu$ has some facility open in its
close neighborhood, with probability no less than
$1-1/e^\gamma$, $\nu$ has some facility open in its overall
neighborhood, and with probability no more than
$1/e^\gamma$, $\nu$ will connect to its target facility.
This gives us the bound
%
\begin{align*}
  \Exp[C_{\nu}] &\leq \clsdist(\nu)(1-1/e) +
  \fardist(\nu)(1/e-1/e^\gamma) + (\clsdist(\nu)+\clsmax(\nu)+\fardist(\nu))1/e^\gamma \\
  &\leq \clsdist(\nu)(1-1/e) +
  \fardist(\nu)(1/e-1/e^\gamma) + (\clsdist(\nu)+2\fardist(\nu))1/e^\gamma\\
  &\leq
  \concost(\nu)((1-\rho_{\nu})(\frac{1/e+1/e^\gamma}{1-1/\gamma})
  + \rho_{\nu}(1+2/e^\gamma)) \\
  &\leq \concost(\nu) \cdot
  \max\Big\{\frac{1/e+1/e^\gamma}{1-1/\gamma},
  1+\frac{2}{e^\gamma}\Big\},
\end{align*}
%
where $\rho_{\nu}=\clsdist(\nu)/\concost(\nu)$. It is easy to see that
$\rho_{\nu}$ is between 0 and 1.  Since $\sum_{\nu\in j} C^{\avg}(\nu)
= \sum_{\nu\in j}\sum_{\mu\in\facilityset} d_{\mu\nu}\barx_{\mu\nu} =
\sum_{i\in\sitesset} d_{ij}x_{ij}^\ast = C_j^\ast$, summing over all
clients $j$ we have total connection cost bounded by $C^\ast
\max\{\frac{1/e+1/e^\gamma}{1-1/\gamma}, 1+\frac{2}{e^\gamma}\}$.


\section{An Elementary Proof of an Inequality in Section~\ref{sec:
    1.736-approximation}}
In the $1+2/e=1.736$-approximation in Section~\ref{sec: 1.736-approximation}
we need to show the following inequality
%
\begin{equation}
  \label{eq:dist}
\sum_{r=1}^l d_ry_r\prod_{s=1}^{r-1}(1-y_s)
  \;\leq\;  \Big(1 - \prod_{s=1}^l (1-y_s)\Big) \cdot \sum_{r=1}^l d_r y_r
\end{equation}
%
for $d_1\leq d_2 \leq \ldots \leq d_l$ and $\sum_{s=1}^l y_s = 1, y_s \geq 0$.

In this section we give a new proof of this inequality, much
simpler than the existing proof in \cite{ChudakS04}, and also simpler than
the argument by Sviridenko~\cite{Svi02}.  
We derive this inequality from the following generalized version of the Chebyshev Sum
Inequality:
%
\begin{equation}
  \label{eq:cheby}
  \sum_{i} p_i \sum_j p_j a_j b_j \leq \sum_i p_i a_i \sum_j p_j b_j,
\end{equation}
%
where each summation below runs from $1$ to $l$ and the sequences 
$(a_i)$, $(b_i)$ and $(p_i)$ satisfy the following conditions:
$p_i\geq 0, a_i \geq 0, b_i \geq 0$ for all $i$, $a_1\leq a_2 \leq
\ldots \leq a_l$, and $b_1 \geq b_2 \geq \ldots \geq b_l$.

Given inequality (\ref{eq:cheby}), we can obtain our inequality
(\ref{eq:dist}) by simple substitution
%
\begin{equation*}
  p_i \leftarrow y_i, a_i \leftarrow d_i, b_i \leftarrow
  \Pi_{s=1}^{i-1} (1-y_s)
\end{equation*}

For the sake of completeness, we include the proof of inequality (\ref{eq:cheby}), 
due to Hardy, Littlewood and Polya~\cite{HardyLP88}. The idea is to evaluate the 
following sum:
%
\begin{align*}
  S &= \sum_i p_i \sum_j p_j a_j b_j - \sum_i p_i a_i \sum_j p_j b_j
	\\
  & = \sum_i \sum_j p_i p_j a_j b_j - \sum_i \sum_j p_i a_i p_j b_j
	\\
  & = \sum_j \sum_i p_j p_i a_i b_i - \sum_j \sum _i p_j a_j p_i b_i
	\\
	&= \half \cdot \sum_i \sum_j (p_i p_j a_j b_j - p_i a_i p_j b_j + p_j p_i a_i
  							b_i - p_j a_j p_i b_i)
\\
  &= \half \cdot \sum_i \sum_j p_i p_j (a_i - a_j)(b_i - b_j) \leq 0.
\end{align*}
The last inequality holds because $(a_i-a_j)(b_i-b_j) \leq 0$, since the sequences
$(a_i)$ and $(b_i)$ are ordered oppositely.

\end{document}
